{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL-Pipeline zum Import von Sensor-Daten\n",
    "\n",
    "## Aufgabenstellung\n",
    "Firma: Peter Döhle Schiffahrts-KG\n",
    "\n",
    "Daten: Signaldaten von 12+ Schiffen\n",
    "- Treibstoff Emissionen, Tankfüllstand, Geschwindigkeit, Gewicht,...\n",
    "- im 15 min / 1h Takt via Rest API verfügbar\n",
    "\n",
    "ursprüngliches Ziel: Skalierbare ETL-Strecke für den poc von Microsoft Fabric\n",
    "\n",
    "geändertes Ziel: Vergleich verschiedener ETL-Ansätze anhand festgelegter Kriterien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-Endpoints\n",
    "\n",
    "Response: JSON-Format\n",
    "- /fleet : Alle Schiffe inkl. technischer Informationen\n",
    "- /fleet/{imo}/signals: Auflistung der für das Schiff verfügbaren Sensordaten inkl. Erklärung\n",
    "- /fleet/{imo}/timeseries?{optionaleParameter}: Zeitreihe der Sensordaten des Schiffes in 5-min Abständen\n",
    "\n",
    "Zielbild: \n",
    "Zeitreihe mit verständlichen Signalnamen pivotisiert und vollständig historisiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herausforderungen\n",
    "\n",
    "- Teils starke Verschachtelung\n",
    "- Schlechte Verbindung des Schiffes \n",
    "    - Fehlende Daten als NULL gespeichert\n",
    "- Fehlende Daten teils nachträglich überschrieben \n",
    "    - überlappendes Laden notwendig\n",
    "- Veränderung der Anzahl der Signale \n",
    "- Pivotierung führt zu sehr vielen Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from typing import Dict, Tuple, List, Union\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Konfiguriert das Logging\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"./logs/pipeline.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    base_url: str = \"https://api.hoppe-sts.com/\"\n",
    "    raw_path: str = \"./data/raw_data\"\n",
    "    transformed_path: str = \"./data/transformed_data\"\n",
    "    gaps_path: str = \"./data/gaps_data\"  ############# Neuer Pfad für Null-Wert-Lücken\n",
    "    batch_size: int = 1000\n",
    "    max_workers: int = 8  # Erhöhte Worker für bessere Parallelisierung\n",
    "    days_to_keep: int = 90  # Daten werden für 90 Tage aufbewahrt\n",
    "    history_days: int = 5  # Letzten 5 Tage für Historie laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class API_Client:\n",
    "\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.logger = logging.getLogger('API Client')\n",
    "\n",
    "    def get_data(self, relative_url, max_retries=3, backoff_factor=2):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                request_url = f\"{self.base_url}{relative_url}\"\n",
    "                response = requests.request(\"GET\", request_url, headers={\"Authorization\": f\"ApiKey {self.api_key}\"})\n",
    "                self.logger.info(f\"Request for {relative_url} successful\")\n",
    "                return response, response.json()\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                self.logger.error(f\"SSL-Zertifikatsfehler: {str(e)}\")\n",
    "                return None, None\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = backoff_factor ** attempt\n",
    "                    self.logger.warning(f\"Timeout bei API-Anfrage: {str(e)}. Retry {attempt+1}/{max_retries} nach {wait_time}s\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                self.logger.error(f\"Timeout bei API-Anfrage nach {max_retries} Versuchen: {str(e)}\")\n",
    "                return None, None\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = backoff_factor ** attempt\n",
    "                    self.logger.warning(f\"Verbindungsfehler: {str(e)}. Retry {attempt+1}/{max_retries} nach {wait_time}s\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                self.logger.error(f\"Verbindungsfehler nach {max_retries} Versuchen: {str(e)}\")\n",
    "                return None, None\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.error(f\"API request failed: {str(e)}\")\n",
    "                if hasattr(e, 'response'):\n",
    "                    return e.response, None\n",
    "                return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Storage:\n",
    "    \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger('Data Storage')\n",
    "        \n",
    "    # Schreiben von Files in lokale Ordner    \n",
    "    def write_file(self, data: Union[List, Dict, pl.DataFrame], filename: str, path: str, postfix: str) -> None:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        full_path = f\"{path}/{filename}.{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            # Schreibt json files\n",
    "            if postfix == 'json':\n",
    "                with open(full_path, 'w') as f:\n",
    "                    json.dump(data, f)\n",
    "                self.logger.info(f\"Writting to {filename}.json file successfully\")\n",
    "\n",
    "            # Schreibt parquet files\n",
    "            elif postfix == 'parquet':\n",
    "\n",
    "                # Check auf richtiges Input-Format\n",
    "                if not isinstance(data, pl.DataFrame):\n",
    "                    if isinstance(data, list) or isinstance(data, dict):\n",
    "                        data = pl.DataFrame(data)\n",
    "                    else:\n",
    "                        raise ValueError(\"Data must be DataFrame, List, or Dict for parquet format\")\n",
    "                    \n",
    "                data.write_parquet(full_path, compression=\"snappy\")\n",
    "                self.logger.info(f\"Writting to {filename}.parquet file successfully\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {postfix}\")\n",
    "                \n",
    "            self.logger.info(f\"Data saved to {full_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to write file {full_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def read_file(self, filename: str, path: str, postfix: str) -> pl.DataFrame:\n",
    "        full_path = f\"{path}/{filename}.{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(full_path):\n",
    "                self.logger.info(f\"File {full_path} does not exist\")\n",
    "                return pl.DataFrame()  # Leeres DataFrame zurückgeben\n",
    "            \n",
    "            else:\n",
    "                if postfix == 'json':\n",
    "                    with open(full_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        data = pl.DataFrame(data)\n",
    "                    self.logger.info(f\"Reading from {filename}.json file successfully\")\n",
    "                    return data\n",
    "\n",
    "                elif postfix == 'parquet':\n",
    "                    data = pl.read_parquet(full_path)\n",
    "                    self.logger.info(f\"Reading from {filename}.parquet file successfully\")\n",
    "                    return data\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported format: {postfix}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to read file {full_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        \n",
    "    def find_timeseries_files(self, base_path: str, max_days: int = None, pattern: str = \"Timeseries_*.parquet\") -> defaultdict:\n",
    "        base_dir = Path(base_path)\n",
    "        if not base_dir.exists() or not base_dir.is_dir():\n",
    "            self.logger.error(f\"Directory {base_path} does not exist or is no directory\")\n",
    "            return defaultdict(list)\n",
    "\n",
    "        try:\n",
    "            # Optimiertes Dateisystem-Scannen mit einmaliger Tiefensuche\n",
    "            files_by_imo = defaultdict(list)\n",
    "            \n",
    "            # Metadaten für Logging\n",
    "            days_found = set()\n",
    "            months_found = set()\n",
    "            years_found = set()\n",
    "            \n",
    "            # Schnellere Dateisuche mit glob statt rekursivem Durchsuchen\n",
    "            year_dirs = sorted([d for d in base_dir.iterdir() if d.is_dir()], key=lambda x: x.name, reverse=True)\n",
    "            \n",
    "            for year_dir in year_dirs:\n",
    "                years_found.add(year_dir.name)\n",
    "                \n",
    "                month_dirs = sorted([d for d in year_dir.iterdir() if d.is_dir()], key=lambda x: x.name, reverse=True)\n",
    "                for month_dir in month_dirs:\n",
    "                    months_found.add(f\"{year_dir.name}/{month_dir.name}\")\n",
    "                    \n",
    "                    day_dirs = sorted([d for d in month_dir.iterdir() if d.is_dir()], key=lambda x: x.name, reverse=True)\n",
    "                    \n",
    "                    # Begrenzung auf max_days\n",
    "                    if max_days is not None and len(days_found) >= max_days:\n",
    "                        break\n",
    "                        \n",
    "                    for day_dir in day_dirs:\n",
    "                        # Begrenzung auf max_days\n",
    "                        if max_days is not None and len(days_found) >= max_days:\n",
    "                            break\n",
    "                            \n",
    "                        days_found.add(f\"{year_dir.name}/{month_dir.name}/{day_dir.name}\")\n",
    "                        \n",
    "                        # Direktes Sammeln aller Dateien für diesen Tag mit glob\n",
    "                        for file in day_dir.glob(pattern):\n",
    "                            imo = file.stem.split(\"_\")[1]  # Extrahiert <imo> aus \"Timeseries_<imo>.parquet\"\n",
    "                            files_by_imo[imo].append(file)\n",
    "            \n",
    "            self.logger.info(f\"{sum(len(files) for files in files_by_imo.values())} files found: \"\n",
    "                            f\"{len(files_by_imo)} different ships, {len(days_found)} days, \"\n",
    "                            f\"{len(months_found)} months, {len(years_found)} years\")\n",
    "            \n",
    "            return files_by_imo  # Dictionary mit Listen von Dateien nach IMO\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get historical Data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def find_timeseries_summaries(self, base_path: str,  pattern:str = \"*.parquet\") -> list:\n",
    "        base_dir = Path(base_path)\n",
    "        if not base_dir.exists() or not base_dir.is_dir():\n",
    "            self.logger.error(f\"Directory {base_path} does not exist or is no directory\")\n",
    "            return defaultdict(list)\n",
    "\n",
    "        # Alles Files mit dem pattern finden, pattern kann Datum begrenzen z.B. 2025*.parquet = alle Dateien aus 2025\n",
    "        try:\n",
    "            \n",
    "            files =  []\n",
    "            files_found = 0\n",
    "\n",
    "            for file in base_dir.rglob(pattern):\n",
    "                files.append(file)\n",
    "                files_found +=1\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get historical Data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        self.logger.info(f\"{files_found} summary files found\")\n",
    "    \n",
    "        return files\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Processor:\n",
    "    #logger = logging.getLogger(\"Data_Processor\") ???\n",
    "\n",
    "    @staticmethod\n",
    "    def get_imo_numbers(data: List[dict]) -> List[str]:\n",
    "        return [ship['imo'] for ship in data if ship.get('active', True)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_shipdata(shipdata: pl.DataFrame, run_timestamp: str) -> Tuple[pl.DataFrame, Dict[str, pl.DataFrame]]:\n",
    "        shipdata = shipdata.unnest(\"data\")\n",
    "        \n",
    "        # Verschachtelte Tabellen extrahiren\n",
    "        tables = {}\n",
    "        for column, dtype in shipdata.collect_schema().items():\n",
    "            if dtype == pl.List(pl.Struct):\n",
    "                tables[column] = (\n",
    "                    shipdata.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .unnest(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    \n",
    "                )\n",
    "            elif dtype == pl.List:\n",
    "                tables[column] = (\n",
    "                    shipdata.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    \n",
    "                )\n",
    "\n",
    "        # Schiffsdaten ohne Verschachtelung extrahieren\n",
    "        shipdata = shipdata.select(\n",
    "            pl.exclude([col for col, dtype in shipdata.collect_schema().items() if dtype == pl.List])\n",
    "        ).with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        )\n",
    "\n",
    "        return shipdata, tables\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_signals(signals: pl.DataFrame, run_timestamp: str) -> pl.DataFrame:\n",
    "        if len(signals) == 0:\n",
    "            return signals\n",
    "\n",
    "        # Initiale Transformation    \n",
    "        signals = (\n",
    "            signals.unnest(\"signals\")\n",
    "            .unpivot(index=\"imo\", variable_name=\"signal\")\n",
    "            .unnest(\"value\")\n",
    "        )\n",
    "\n",
    "        # Verbleibende Verschachtelungen plätten\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Struct:\n",
    "                signals = signals.unnest(column)\n",
    "\n",
    "        # Null-Werte\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Null:\n",
    "                signals = signals.with_columns(pl.col(column).cast(pl.String))\n",
    "        \n",
    "        # Das Lade-Datum hinzufügen\n",
    "        signals = signals.with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        )\n",
    "                \n",
    "        return signals\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def transform_timeseries(timeseries: pl.DataFrame, imo: str, run_timestamp: str) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \n",
    "        if len(timeseries) == 0:\n",
    "            # Hier müssen wir sicherstellen, dass beide zurückgegebenen DataFrames dieselbe Struktur haben\n",
    "            empty_df = pl.DataFrame({\n",
    "                \"signal\": [], \n",
    "                \"signal_timestamp\": [], \n",
    "                \"signal_value\": [], \n",
    "                \"imo\": [], \n",
    "                \"loaddate\": []\n",
    "            })\n",
    "            return empty_df, empty_df.clone()\n",
    "        \n",
    "        # Optimierte Transformation\n",
    "        transformed = (\n",
    "            timeseries.drop(\"timestamp\")\n",
    "            .unpivot(\n",
    "                index=[],\n",
    "                variable_name=\"signal\"\n",
    "            )\n",
    "            .unnest(\"value\")\n",
    "            .unpivot(\n",
    "                index=[\"signal\"],\n",
    "                variable_name=\"signal_timestamp\",\n",
    "                value_name=\"signal_value\"\n",
    "            )\n",
    "            .with_columns([\n",
    "                pl.lit(imo).alias(\"imo\"),\n",
    "                pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Lücken (NULL-Werte) identifizieren\n",
    "        gaps = (\n",
    "            transformed\n",
    "            .filter(pl.col(\"signal_value\").is_null())\n",
    "            .select(\"imo\", \"signal\", \"signal_timestamp\", \"loaddate\")\n",
    "            .with_columns([\n",
    "                pl.col(\"signal_timestamp\").alias(\"gap_start\")\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # NULL-Werte aus dem Hauptdatensatz entfernen\n",
    "        data = transformed.filter(pl.col(\"signal_value\").is_not_null())\n",
    "        \n",
    "        return data, gaps\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_gaps(gaps_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \n",
    "        if len(gaps_df) == 0:\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "        # Stelle sicher, dass gap_start ein Datetime-Objekt ist\n",
    "        if gaps_df[\"gap_start\"].dtype != pl.Datetime:\n",
    "            gaps_df = gaps_df.with_columns(\n",
    "                pl.col(\"gap_start\").cast(pl.Datetime)\n",
    "            )\n",
    "            \n",
    "        result = []\n",
    "        \n",
    "        # Gruppieren nach IMO und Signal, sortiere nach Zeitstempel\n",
    "        for (imo, signal), group in gaps_df.sort(\"gap_start\").group_by([\"imo\", \"signal\"]):\n",
    "            group_df = group.sort(\"gap_start\")\n",
    "            \n",
    "            if len(group_df) <= 1:\n",
    "                # Bei nur einem Eintrag\n",
    "                result.append({\n",
    "                    \"imo\": imo,\n",
    "                    \"signal\": signal,\n",
    "                    \"gap_start\": group_df[\"gap_start\"][0],\n",
    "                    \"gap_end\": group_df[\"gap_start\"][0],\n",
    "                    \"loaddate\": group_df[\"loaddate\"][0]\n",
    "                })\n",
    "                continue\n",
    "                \n",
    "            current_start = group_df[\"gap_start\"][0]\n",
    "            prev_time = current_start\n",
    "            \n",
    "            for i in range(1, len(group_df)):\n",
    "                curr_time = group_df[\"gap_start\"][i]\n",
    "                \n",
    "                # Prüfe, ob mehr als 5 Minuten zwischen zwei Einträgen liegen\n",
    "                max_sec = 5*60\n",
    "                # Hier kommt es zum Fehler, wenn prev_time und curr_time Strings sind\n",
    "                time_diff_seconds = (curr_time - prev_time).total_seconds()\n",
    "                \n",
    "                if time_diff_seconds > max_sec:\n",
    "                    result.append({\n",
    "                        \"imo\": imo,\n",
    "                        \"signal\": signal,\n",
    "                        \"gap_start\": current_start,\n",
    "                        \"gap_end\": prev_time,\n",
    "                        \"loaddate\": group_df[\"loaddate\"][i]\n",
    "                    })\n",
    "                    current_start = curr_time  # Starte neue Lücke\n",
    "                \n",
    "                prev_time = curr_time  # Aktualisiere den vorherigen Zeitstempel\n",
    "            \n",
    "            # Letzte Lücke hinzufügen\n",
    "            result.append({\n",
    "                \"imo\": imo,\n",
    "                \"signal\": signal,\n",
    "                \"gap_start\": current_start,\n",
    "                \"gap_end\": prev_time,\n",
    "                \"loaddate\": group_df[\"loaddate\"][-1]\n",
    "            })\n",
    "        \n",
    "        return pl.DataFrame(result)\n",
    "    \n",
    "    @staticmethod\n",
    "    def enrich_timeseries_with_friendly_names(timeseries_df: pl.DataFrame, signals_df: pl.DataFrame) -> pl.DataFrame:\n",
    "\n",
    "        if len(timeseries_df) == 0 or len(signals_df) == 0:\n",
    "            return timeseries_df\n",
    "            \n",
    "        # Extrahiere Signal-Mapping (signal -> friendly_name)\n",
    "        signal_mapping = (\n",
    "            signals_df\n",
    "            .filter(pl.col(\"friendly_name\").is_not_null())\n",
    "            .select([\"signal\", \"friendly_name\"])\n",
    "            .unique()\n",
    "        )\n",
    "        \n",
    "        # Join mit Timeseries-Daten\n",
    "        return timeseries_df.join(\n",
    "            signal_mapping,\n",
    "            on=\"signal\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_daily_timeseries_summary(hist_df: pl.DataFrame, daily_df: pl.DataFrame, current_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \n",
    "        combined_df = pl.concat([hist_df, daily_df, current_df])\n",
    "        # summary_df = combined_df.unique(subset=[\"imo\", \"signal_timestamp\", \"friendly_name\"], keep=\"first\").filter(pl.col(\"tag\")==\"new\"| pl.col(\"tag\")==\"today\")\n",
    "        summary_df = combined_df.unique(subset=[\"imo\", \"signal_timestamp\", \"friendly_name\"], keep=\"first\").filter(pl.col(\"tag\").is_in([\"new\", \"today\"]))\n",
    "\n",
    "        return summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idee: Eine Datei, in der alle neuen einträge des Tages gespeichert sind im verzeichnis data/daily_summary/jahrmonattag.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoppe: Pipeline-Ablauf\n",
    "\n",
    "### Voraussetzungen\n",
    "- IMO-Liste erstellen\n",
    "- Signals-Liste anlegen\n",
    "\n",
    "### ShipData Pipeline\n",
    "*Läuft maximal einmal täglich*\n",
    "\n",
    "1. ShipData API abrufen\n",
    "2. JSON-Daten speichern\n",
    "3. IMO-Nummern extrahieren und unter `data/latest/` speichern\n",
    "4. Relevante Daten aus JSON extrahieren\n",
    "5. Daten im PARQUET-Format speichern\n",
    "\n",
    "### Signals Pipeline\n",
    "*Läuft maximal einmal täglich*\n",
    "\n",
    "1. ShipData API abrufen\n",
    "2. IMO-Nummern aus gespeicherter Datei laden\n",
    "3. Für jedes Schiff (parallelisierte Verarbeitung):\n",
    "   1. Signals API für das spezifische Schiff abrufen\n",
    "   2. JSON-Daten speichern\n",
    "   3. Relevante Daten aus JSON extrahieren\n",
    "   4. Daten im PARQUET-Format speichern\n",
    "   5. Die `signal_mapping`-Datei in `data/latest/` aktualisieren (enthält imo, signal_id & friendly_name)\n",
    "\n",
    "### Timeseries Pipeline\n",
    "*Läuft mehrmals täglich (ca. 1x pro Stunde)*\n",
    "\n",
    "1. ShipData API abrufen\n",
    "2. IMO-Nummern aus gespeicherter Datei laden\n",
    "3. `signal_mapping` Datei laden\n",
    "4. Aktuelle Daily Summary in `daily_df` laden und mit Tag \"today\" versehen\n",
    "5. Falls noch keine Daily Summary Datei vorhanden ist (`daily_df` leer):\n",
    "   - `ref_data` Datei aktualisieren, sodass nur die letzten X Tage enthalten sind\n",
    "   - Ältesten Tag entfernen & letzten Tag hinzufügen\n",
    "6. Reference Data in `hist_df` laden und mit Tag \"old\" versehen\n",
    "7. Leeren DataFrame `current_df` erstellen\n",
    "8. Für jedes Schiff (parallelisierte Verarbeitung):\n",
    "   1. Timeseries API abrufen (mit `to_date = run_timestamp` um Abweichungen durch Abfragezeit zu vermeiden)\n",
    "   2. JSON-Daten speichern\n",
    "   3. Relevante Daten aus JSON extrahieren\n",
    "   4. Friendly-Name aus `signal_mapping` hinzufügen\n",
    "   5. Daten im PARQUET-Format speichern\n",
    "   6. Daten zum Gesamt-DataFrame `current_df` hinzufügen\n",
    "9. Daily Summary erstellen/ergänzen:\n",
    "   - `hist_df`, `daily_df` und `current_df` zusammenführen\n",
    "   - Nur eindeutige Einträge behalten, die zuerst im DataFrame erscheinen und den Tag \"new\" oder \"today\" haben\n",
    "   - Ergebnis im `summary_df` speichern\n",
    "10. Daily Summary (`summary_df`) als PARQUET speichern:\n",
    "    - Falls noch keine Datei zum aktuellen Tag existiert, neue Datei anlegen\n",
    "    - Ansonsten vorhandene Datei überschreiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, config: Config, api_key: str, verify_ssl: bool = True):\n",
    "        self.config = config\n",
    "        self.api_client = API_Client(config.base_url, api_key)\n",
    "        self.processor = Data_Processor()\n",
    "        self.storage = Data_Storage(config)\n",
    "        self.logger = logging.getLogger('Pipeline')\n",
    "    \n",
    "    def process_shipdata(self, run_timestamp: str):\n",
    "        try:\n",
    "\n",
    "            self.logger.info(\"Processing ship data\")\n",
    "\n",
    "            # Get Shipdata\n",
    "            response, shipdata = self.api_client.get_data(\"fleet\")\n",
    "\n",
    "            # Get & Store IMO Numbers\n",
    "            imo_numbers = self.processor.get_imo_numbers(shipdata)\n",
    "            self.storage.write_file(\n",
    "                imo_numbers,\n",
    "                'imos',\n",
    "                f\"./data/latest\",\n",
    "                'parquet'\n",
    "            )\n",
    "\n",
    "            self.logger.info(f\"Found {len(imo_numbers)} active ships\")\n",
    "\n",
    "            if not shipdata:\n",
    "                self.logger.error(f\"No ship data received - {response}\")\n",
    "                raise ValueError(f\"No ship data received - {response}\")\n",
    "            \n",
    "            # Store raw ship data\n",
    "            self.storage.write_file(\n",
    "                    shipdata,\n",
    "                    'ShipData',\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "            \n",
    "            # Transform and store ship data\n",
    "            ships_df = pl.DataFrame(shipdata)\n",
    "            if ships_df.columns == ['detail']:\n",
    "                self.logger.info(\"Request Error: see json file for details\")\n",
    "            \n",
    "            else:\n",
    "                ships_transformed, tables = self.processor.transform_shipdata(ships_df, run_timestamp)\n",
    "            \n",
    "                self.storage.write_file(\n",
    "                        ships_transformed,\n",
    "                        'ShipData',\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                    \n",
    "                # Process nested tables\n",
    "                for name, table in tables.items():\n",
    "                        self.storage.write_file(\n",
    "                            table,\n",
    "                            f\"ShipData_{name}\",\n",
    "                            f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process ship data: {str(e)}\")\n",
    "\n",
    "\n",
    "    def process_signals(self, run_timestamp: str, imo_numbers:List[str], current_signals_df: pl.DataFrame):\n",
    "        try:\n",
    "            \n",
    "            for imo in imo_numbers:\n",
    "\n",
    "                self.logger.info(f\"Processing signals data for {imo}\")\n",
    "\n",
    "                response, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "\n",
    "                if not signals:\n",
    "                    self.logger.error(f\"No signals data received - {response}\")\n",
    "                    ValueError(f\"No signals data received - {response}\")\n",
    "\n",
    "                # Store raw signals data\n",
    "                self.storage.write_file(\n",
    "                        signals,\n",
    "                        f'Signals_{imo}',\n",
    "                        f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                        'json'\n",
    "                    )\n",
    "                \n",
    "                # Transform and store signals data\n",
    "                signals_df = pl.DataFrame(signals)\n",
    "                if signals_df.columns == ['detail']:\n",
    "                    self.logger.info(\"Request Error: see json file for details\")\n",
    "                else:\n",
    "                    signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "                    self.storage.write_file(\n",
    "                            signals_transformed,\n",
    "                            f'Signals_{imo}',\n",
    "                            f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                    self.logger.info(f\"Signals data processed for {imo}\")\n",
    "                \n",
    "                    # Update Signals Mapping\n",
    "\n",
    "                    new_signal_mapping = signals_transformed.select([\"imo\", \"signal\", \"friendly_name\"]).unique()\n",
    "\n",
    "                    if current_signals_df is None:\n",
    "                        self.storage.write_file(\n",
    "                            new_signal_mapping,\n",
    "                            'signal_mapping',\n",
    "                            f\"./data/latest\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                        self.logger.info(f\"Signal Mapping updated for the first time\")\n",
    "                    else:        \n",
    "                        # Add new Signals to existing Signal Mapping\n",
    "                        \n",
    "                        updated_signals = pl.concat([current_signals_df, new_signal_mapping]).unique(subset=[\"imo\", \"signal\", \"friendly_name\"], keep=\"first\")\n",
    "                        \n",
    "                        self.storage.write_file(\n",
    "                                updated_signals,\n",
    "                                \"signal_mapping\",\n",
    "                                f\"./data/latest\",\n",
    "                                \"parquet\")\n",
    "                        self.logger.info(f\"Signal Mapping updated\")    \n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process signals data: {str(e)}\")\n",
    "\n",
    "    def process_timeseries(self, run_timestamp: str, imo_numbers: List[str], signal_mapping: pl.DataFrame, current_df: pl.DataFrame, run_start) -> pl.DataFrame:\n",
    "        try:\n",
    "            self.logger.info(f\"Starting parallel processing of timeseries data for {len(imo_numbers)} ships\")\n",
    "            result_df = current_df\n",
    "            \n",
    "            def process_per_imo(imo):\n",
    "                try:\n",
    "                    self.logger.info(f\"Processing timeseries data for {imo}\")\n",
    "                    response, timeseries = self.api_client.get_data(f\"fleet/{imo}/timeseries\")\n",
    "                    \n",
    "                    # Store raw timeseries data\n",
    "                    self.storage.write_file(\n",
    "                        timeseries,\n",
    "                        f'Timeseries_{imo}',\n",
    "                        f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                        'json'\n",
    "                    )\n",
    "                    \n",
    "                    # Transform and store timeseries data\n",
    "                    timeseries_df = pl.DataFrame(timeseries)\n",
    "                    \n",
    "                    if timeseries_df.columns == ['detail']:\n",
    "                        self.logger.info(f\"Request Error for {imo}: see json file for details\")\n",
    "                        return pl.DataFrame()\n",
    "                    \n",
    "                    if not timeseries:\n",
    "                        self.logger.error(f\"No timeseries data received for {imo} - {response}\")\n",
    "                        timeseries_transformed = pl.DataFrame({\n",
    "                            \"signal\": [], \"signal_timestamp\": [], \"signal_value\": [], \n",
    "                            \"imo\": [], \"loaddate\": [], \"friendly_name\": []\n",
    "                        })\n",
    "                        gaps = pl.DataFrame()\n",
    "                    else:\n",
    "                        timeseries_transformed, gaps = self.processor.transform_timeseries(timeseries_df, imo, run_timestamp)\n",
    "                    \n",
    "                    # Enrich with friendly names\n",
    "                    timeseries_transformed = self.processor.enrich_timeseries_with_friendly_names(\n",
    "                        timeseries_transformed, signal_mapping\n",
    "                    )\n",
    "                    \n",
    "                    self.storage.write_file(\n",
    "                        timeseries_transformed,\n",
    "                        f\"Timeseries_{imo}\",\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                    \n",
    "                    # Process gaps\n",
    "                    gaps_df = self.processor.process_gaps(pl.DataFrame(gaps))\n",
    "                    self.storage.write_file(\n",
    "                        gaps_df,\n",
    "                        f\"Gaps_{imo}\",\n",
    "                        f\"{self.config.gaps_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                    \n",
    "                    return timeseries_transformed\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Failed to process timeseries for {imo}: {str(e)}\")\n",
    "                    return pl.DataFrame()\n",
    "            \n",
    "            # Process parallelized with ThreadPoolExecutor\n",
    "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                results = list(executor.map(process_per_imo, imo_numbers))\n",
    "            \n",
    "            # Alle Ergebnisse kombinieren\n",
    "            valid_results = [df for df in results if not df.is_empty()]\n",
    "            if valid_results:\n",
    "                result_df = pl.concat([result_df] + valid_results)\n",
    "                \n",
    "            return result_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process timeseries data: {str(e)}\")\n",
    "            return current_df\n",
    "        \n",
    "    def process_signals_in_batches(self, run_timestamp: str, imo_numbers: List[str], current_signals_df: pl.DataFrame):\n",
    "        try:\n",
    "            batch_size = min(self.config.batch_size, len(imo_numbers))\n",
    "            total_batches = (len(imo_numbers) + batch_size - 1) // batch_size\n",
    "            \n",
    "            self.logger.info(f\"Processing signals in {total_batches} batches with batch size {batch_size}\")\n",
    "            \n",
    "            updated_signals = current_signals_df if current_signals_df is not None else pl.DataFrame()\n",
    "            \n",
    "            for batch_idx in range(total_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, len(imo_numbers))\n",
    "                batch_imos = imo_numbers[start_idx:end_idx]\n",
    "                \n",
    "                self.logger.info(f\"Processing batch {batch_idx+1}/{total_batches} with {len(batch_imos)} ships\")\n",
    "                \n",
    "                with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                    batch_results = list(executor.map(\n",
    "                        lambda imo: self._process_single_ship_signals(imo, run_timestamp),\n",
    "                        batch_imos\n",
    "                    ))\n",
    "                \n",
    "                # Kombiniere alle Batch-Ergebnisse\n",
    "                valid_results = [df for df in batch_results if not df.is_empty()]\n",
    "                if valid_results:\n",
    "                    batch_signals = pl.concat(valid_results)\n",
    "                    \n",
    "                    # Update signal mapping\n",
    "                    new_signal_mapping = batch_signals.select([\"imo\", \"signal\", \"friendly_name\"]).unique()\n",
    "                    \n",
    "                    if updated_signals.is_empty():\n",
    "                        updated_signals = new_signal_mapping\n",
    "                    else:\n",
    "                        updated_signals = pl.concat([updated_signals, new_signal_mapping]).unique(\n",
    "                            subset=[\"imo\", \"signal\", \"friendly_name\"], \n",
    "                            keep=\"first\"\n",
    "                        )\n",
    "                    \n",
    "            # Speichere das aktualisierte Signal-Mapping\n",
    "            if not updated_signals.is_empty():\n",
    "                self.storage.write_file(\n",
    "                    updated_signals,\n",
    "                    \"signal_mapping\",\n",
    "                    f\"./data/latest\",\n",
    "                    \"parquet\"\n",
    "                )\n",
    "                self.logger.info(f\"Signal Mapping updated with {len(updated_signals)} records\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process signals in batches: {str(e)}\")\n",
    "            \n",
    "    def _process_single_ship_signals(self, imo: str, run_timestamp: str) -> pl.DataFrame:\n",
    "        \"\"\"Helper-Methode zur Verarbeitung von Signals für ein einzelnes Schiff\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Processing signals data for {imo}\")\n",
    "            \n",
    "            response, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "            \n",
    "            if not signals:\n",
    "                self.logger.error(f\"No signals data received for {imo} - {response}\")\n",
    "                return pl.DataFrame()\n",
    "                \n",
    "            # Store raw signals data\n",
    "            self.storage.write_file(\n",
    "                signals,\n",
    "                f'Signals_{imo}',\n",
    "                f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                'json'\n",
    "            )\n",
    "            \n",
    "            # Transform and store signals data\n",
    "            signals_df = pl.DataFrame(signals)\n",
    "            if signals_df.columns == ['detail']:\n",
    "                self.logger.info(f\"Request Error for {imo}: see json file for details\")\n",
    "                return pl.DataFrame()\n",
    "                \n",
    "            signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "            \n",
    "            self.storage.write_file(\n",
    "                signals_transformed,\n",
    "                f'Signals_{imo}',\n",
    "                f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                'parquet'\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Signals data processed for {imo}\")\n",
    "            return signals_transformed\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process signals for {imo}: {str(e)}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "\n",
    "    def run(self, mode: str = \"all\"):\n",
    "\n",
    "        try: \n",
    "\n",
    "            run_start = datetime.now()\n",
    "            run_end = None\n",
    "            run_timestamp = run_start.strftime('%Y/%m/%d/%H/%M')\n",
    "            summary_filename = f\"{run_start.strftime('%Y%m%d')}\"\n",
    "            self.logger.info(f\"Starting pipeline run at {run_start}\")\n",
    "            cutoff_date_str = (run_start - timedelta(days=self.config.history_days)).strftime('%Y/%m/%d')\n",
    "\n",
    "            # Initialize directories\n",
    "            os.makedirs(f\"{self.config.raw_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.config.transformed_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.config.gaps_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"./data/latest\", exist_ok=True)\n",
    "            os.makedirs(f\"./data/daily_summary\", exist_ok=True)\n",
    "            os.makedirs(f\"./logs\", exist_ok=True)\n",
    "\n",
    "            empty_ts_schema = {\"signal\": pl.String, \"signal_timestamp\": pl.String, \"signal_value\": pl.Float64, \"imo\": pl.String, \"loaddate\": pl.String, \"friendly_name\": pl.String}\n",
    "            empty_ts_schema_tags = {\"signal\": pl.String, \"signal_timestamp\": pl.String, \"signal_value\": pl.Float64, \"imo\": pl.String, \"loaddate\": pl.String, \"friendly_name\": pl.String, \"tag\": pl.String}\n",
    "            current_signals_df = self.storage.read_file(\"signal_mapping\", \"./data/latest\", \"parquet\")\n",
    "\n",
    "            # Read daily and historical data\n",
    "            daily_df = self.storage.read_file(summary_filename, \"./data/daily_summary\", \"parquet\")\n",
    "            hist_df = self.storage.read_file(\"ref_data\", \"./data/latest\", \"parquet\")\n",
    "            if hist_df.is_empty(): hist_df = pl.DataFrame(schema=empty_ts_schema_tags)\n",
    "            else: # Filter, wo die ersten 10 Zeichen (YYYY/MM/DD) größer als cutoff sind\n",
    "                hist_df = hist_df.filter(pl.col(\"loaddate\").str.slice(0, 10) > cutoff_date_str)\n",
    "\n",
    "            if daily_df.is_empty(): # means it is a new day\n",
    "\n",
    "                daily_df = pl.DataFrame(schema=empty_ts_schema_tags)\n",
    "                \n",
    "                last_day = (run_start - timedelta(days=1)).strftime('%Y%m%d')\n",
    "                last_day_df = self.storage.read_file(last_day, \"./data/daily_summary\", \"parquet\")\n",
    "                \n",
    "                if last_day_df.is_empty(): \n",
    "                    last_day_df = pl.DataFrame(schema=empty_ts_schema_tags)\n",
    "                else:\n",
    "                    last_day_df = last_day_df.with_columns(pl.col(\"hist\").alias(\"tag\"))\n",
    "\n",
    "                hist_df = pl.concat([hist_df, last_day_df])\n",
    "\n",
    "                self.storage.write_file(hist_df, \n",
    "                                \"ref_data\", \n",
    "                                \"./data/latest\", \n",
    "                                \"parquet\")\n",
    "                \n",
    "            else: daily_df = daily_df.with_columns(pl.col(\"today\").alias(\"tag\"))\n",
    "\n",
    "            current_df = pl.DataFrame(schema=empty_ts_schema)\n",
    "\n",
    "            self.logger.info(f\"Loaded {hist_df.shape[0]} historical records and {daily_df.shape[0]} daily records\")\n",
    "            \n",
    "            self.logger.info(f\"Processing data for mode: {mode}\")\n",
    "\n",
    "            if mode in [\"all\", \"fleet\"]:\n",
    "\n",
    "                # Process Shipdata\n",
    "                self.process_shipdata(run_timestamp)\n",
    "\n",
    "                # Get IMO Numbers and turn them into list\n",
    "                imo_numbers = self.storage.read_file(\n",
    "                    'imos',\n",
    "                    f\"./data/latest\",\n",
    "                    'parquet'\n",
    "                )            \n",
    "                imo_numbers = imo_numbers.to_series(0).to_list()\n",
    "\n",
    "                self.logger.info(f\"Processing signals for {len(imo_numbers)} ships\")\n",
    "\n",
    "                # Process Signals\n",
    "                self.process_signals_in_batches(run_timestamp, imo_numbers, current_signals_df)\n",
    "\n",
    "                self.logger.info(\"All Signals processed\")\n",
    "\n",
    "\n",
    "            if mode in [\"all\", \"timeseries\"]:\n",
    "\n",
    "                self.logger.info(\"Start to Process Timeseries Data\")\n",
    "                # Get IMO Numbers from file in data\n",
    "                imo_numbers = self.storage.read_file(\n",
    "                    'imos',\n",
    "                    f\"./data/latest\",\n",
    "                    'parquet'\n",
    "                )\n",
    "                imo_numbers = imo_numbers.to_series(0).to_list()\n",
    "\n",
    "                # Get Signal-Mapping\n",
    "                signal_mapping = self.storage.read_file(\n",
    "                    'signal_mapping',\n",
    "                    f\"./data/latest\",\n",
    "                    'parquet'\n",
    "                )                \n",
    "                \n",
    "                # Process Timeseries\n",
    "                current_df = self.process_timeseries(run_timestamp, imo_numbers, signal_mapping, current_df, run_start)\n",
    "                current_df = current_df.with_columns(pl.lit(\"new\").alias(\"tag\"))\n",
    "\n",
    "                # Save Delta\n",
    "                summary_df = self.processor.update_daily_timeseries_summary(hist_df, daily_df, current_df)\n",
    "                self.storage.write_file(\n",
    "                    summary_df.select([\"imo\", \"signal_timestamp\", \"signal\", \"signal_value\", \"friendly_name\", \"loaddate\"]),\n",
    "                    summary_filename,\n",
    "                    f\"./data/daily_summary\",\n",
    "                    \"parquet\"\n",
    "                )\n",
    "            \n",
    "            run_end = datetime.now()\n",
    "        \n",
    "        except Exception as e:\n",
    "            run_end = datetime.now()\n",
    "            self.logger.error(f\"Pipeline run failed at {run_end}: {str(e)}\") \n",
    "            raise\n",
    "\n",
    "        \n",
    "\n",
    "        return run_start, run_end\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Run Hoppe Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logging()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if environment variables are set\n",
    "api_key = os.getenv('HOPPE_API_KEY')\n",
    "if not api_key:\n",
    "    logger.error(\"HOPPE_API_KEY environment variable not set\")\n",
    "    raise ValueError(\"HOPPE_API_KEY environment variable not set\")\n",
    "\n",
    "# Configure pipeline\n",
    "config = Config(\n",
    "    base_url=os.getenv('HOPPE_BASE_URL', \"https://api.hoppe-sts.com/\"),\n",
    "    raw_path=os.getenv('RAW_PATH', \"./data/raw_data\"),\n",
    "    transformed_path=os.getenv('TRANSFORMED_PATH', \"./data/transformed_data\"),\n",
    "    gaps_path=os.getenv('GAPS_PATH', \"./data/gaps_data\"),\n",
    "    batch_size = int(os.getenv('BATCH_SIZE', \"1000\")),\n",
    "    max_workers=int(os.getenv('MAX_WORKERS', \"4\")),\n",
    "    days_to_keep=int(os.getenv('DAYS_TO_KEEP', \"90\")),\n",
    "    history_days=int(os.getenv('HISTORY_DAYS', \"5\"))\n",
    ")\n",
    "\n",
    "mode = \"timeseries\" # \"all\" or \"timeseries\" or \"fleet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run pipeline\n",
    "try:\n",
    "    pipeline = Pipeline(config, api_key)\n",
    "    run_start, run_end = pipeline.run(mode)  \n",
    "    logger.info(f\"Pipeline run completed at {run_end}: total runtime {run_end - run_start}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Pipeline run failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoppe-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
