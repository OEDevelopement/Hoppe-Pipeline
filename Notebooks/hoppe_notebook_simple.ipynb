{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from typing import Dict, Optional, Tuple, List, Union\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptordner erstellen\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Unterordner erstellen\n",
    "for sub_dir in ['raw_data', 'transformed_data', 'gaps_data', 'logs']:\n",
    "    os.makedirs(os.path.join('../data', sub_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Konfiguriert das Logging\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"./logs/pipeline.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    base_url: str = \"https://api.hoppe-sts.com/\"\n",
    "    raw_path: str = \"./data/raw_data\"\n",
    "    transformed_path: str = \"./data/transformed_data\"\n",
    "    gaps_path: str = \"./data/gaps_data\"  # Neuer Pfad für Null-Wert-Lücken\n",
    "    batch_size: int = 1000\n",
    "    max_workers: int = 8  # Erhöhte Worker für bessere Parallelisierung\n",
    "    days_to_keep: int = 90  # Daten werden für 90 Tage aufbewahrt\n",
    "    history_days: int = 5  # Letzten 5 Tage für Historie laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class API_Client:\n",
    "\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.logger = logging.getLogger('API Client')\n",
    "\n",
    "    def get_data(self, relative_url):\n",
    "        \n",
    "        try:\n",
    "            request_url = f\"{self.base_url}{relative_url}\"\n",
    "            response = requests.request(\"GET\", request_url, headers={\"Authorization\": f\"ApiKey {self.api_key}\"})\n",
    "            self.logger.info(f\"Request for {relative_url} successful\")\n",
    "            return response, response.json()\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            self.logger.error(f\"SSL-Zertifikatsfehler: {str(e)}\")\n",
    "            return None, None\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            self.logger.error(f\"Timeout bei API-Anfrage: {str(e)}\")\n",
    "            return None, None\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            self.logger.error(f\"Verbindungsfehler: {str(e)}\")\n",
    "            return None, None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"API request failed: {str(e)}\")\n",
    "            if hasattr(e, 'response'):\n",
    "                return e.response, None\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Storage:\n",
    "    \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger('Data Storage')\n",
    "        \n",
    "    # Schreiben von Files in lokale Ordner    \n",
    "    def write_file(self, data: Union[List, Dict, pl.DataFrame], filename: str, path: str, postfix: str) -> None:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        full_path = f\"{path}/{filename}.{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            # Schreibt json files\n",
    "            if postfix == 'json':\n",
    "                with open(full_path, 'w') as f:\n",
    "                    json.dump(data, f)\n",
    "                self.logger.info(f\"Writting to {filename}.json file successfully\")\n",
    "\n",
    "            # Schreibt parquet files\n",
    "            elif postfix == 'parquet':\n",
    "\n",
    "                # Check auf richtiges Input-Format\n",
    "                if not isinstance(data, pl.DataFrame):\n",
    "                    if isinstance(data, list) or isinstance(data, dict):\n",
    "                        data = pl.DataFrame(data)\n",
    "                    else:\n",
    "                        raise ValueError(\"Data must be DataFrame, List, or Dict for parquet format\")\n",
    "                    \n",
    "                data.write_parquet(full_path, compression=\"snappy\")\n",
    "                self.logger.info(f\"Writting to {filename}.parquet file successfully\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {postfix}\")\n",
    "                \n",
    "            self.logger.info(f\"Data saved to {full_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to write file {full_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def read_file(self, filename: str, path: str, postfix: str) -> pl.DataFrame:\n",
    "        full_path = f\"{path}/{filename}.{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(full_path):\n",
    "                self.logger.info(f\"File {full_path} does not exist\")\n",
    "                return pl.DataFrame()  # Leeres DataFrame zurückgeben\n",
    "            \n",
    "            else:\n",
    "                if postfix == 'json':\n",
    "                    with open(full_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        data = pl.DataFrame(data)\n",
    "                    self.logger.info(f\"Reading from {filename}.json file successfully\")\n",
    "                    return data\n",
    "\n",
    "                elif postfix == 'parquet':\n",
    "                    data = pl.read_parquet(full_path)\n",
    "                    self.logger.info(f\"Reading from {filename}.parquet file successfully\")\n",
    "                    return data\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported format: {postfix}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to read file {full_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        \n",
    "    def find_timeseries_files(self, base_path: str, max_days: int = None, pattern:str = \"Timeseries_*.parquet\") -> defaultdict:\n",
    "        base_dir = Path(base_path)\n",
    "        if not base_dir.exists() or not base_dir.is_dir():\n",
    "            self.logger.error(f\"Directory {base_path} does not exist or is no directory\")\n",
    "            return defaultdict(list)\n",
    "    \n",
    "        try:\n",
    "            days_found = 0\n",
    "            months_found = 0\n",
    "            years_found = 0\n",
    "            imos_found = 0\n",
    "            files_found = 0\n",
    "\n",
    "            # Dictionary für Partitionierung nach imo\n",
    "            files_by_imo = defaultdict(list)\n",
    "            \n",
    "            # Alle Jahre-Ordner sortiert durchsuchen (neuste zuerst)\n",
    "            for year_dir in sorted(base_dir.iterdir(), key=lambda x: x.name, reverse=True):\n",
    "                if not year_dir.is_dir():\n",
    "                    continue\n",
    "                \n",
    "                # Alle Monate-Ordner sortiert durchsuchen (neuste zuerst)\n",
    "                for month_dir in sorted(year_dir.iterdir(), key=lambda x: x.name, reverse=True):\n",
    "                    if not month_dir.is_dir():\n",
    "                        continue\n",
    "                    \n",
    "                    # Alle Tage-Ordner sortiert durchsuchen (neuste zuerst)\n",
    "                    days_processed = 0\n",
    "                    for day_dir in sorted(month_dir.iterdir(), key=lambda x: x.name, reverse=True):\n",
    "                        if not day_dir.is_dir():\n",
    "                            continue\n",
    "                        \n",
    "                        # Begrenzung auf max_days\n",
    "                        if max_days is not None and days_processed >= max_days:\n",
    "                            return files_by_imo  # Sofort zurückgeben, sobald Grenze erreicht\n",
    "                        \n",
    "                        # Alle Dateien für diesen Tag sammeln\n",
    "                        for file in day_dir.rglob(pattern):\n",
    "                            imo = file.stem.split(\"_\")[1]  # Extrahiert <imo> aus \"Timeseries_<imo>.parquet\"\n",
    "                            files_by_imo[imo].append(file)\n",
    "                            files_found +=1\n",
    "                        \n",
    "                        days_processed += 1\n",
    "                        days_found += 1\n",
    "                    \n",
    "                    months_found += 1\n",
    "\n",
    "                years_found += 1\n",
    "\n",
    "            imos_found = len(files_by_imo)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get historical Data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        self.logger.info(f\"{files_found} files found: {imos_found} different ships, {days_found} days, {months_found} months, {years_found} years\")\n",
    "        return files_by_imo  # Dictionary mit Listen von Dateien nach IMO\n",
    "    \n",
    "    def find_timeseries_summaries(self, base_path: str,  pattern:str = \"*.parquet\") -> list:\n",
    "        base_dir = Path(base_path)\n",
    "        if not base_dir.exists() or not base_dir.is_dir():\n",
    "            self.logger.error(f\"Directory {base_path} does not exist or is no directory\")\n",
    "            return defaultdict(list)\n",
    "\n",
    "        # Alles Files mit dem pattern finden, pattern kann Datum begrenzen z.B. 2025*.parquet = alle Dateien aus 2025\n",
    "        try:\n",
    "            \n",
    "            files =  []\n",
    "            files_found = 0\n",
    "\n",
    "            for file in base_dir.rglob(pattern):\n",
    "                files.append(file)\n",
    "                files_found +=1\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get historical Data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        self.logger.info(f\"{files_found} summary files found\")\n",
    "    \n",
    "        return files\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Processor:\n",
    "    #logger = logging.getLogger(\"Data_Processor\") ???\n",
    "\n",
    "    @staticmethod\n",
    "    def get_imo_numbers(data: List[dict]) -> List[str]:\n",
    "        return [ship['imo'] for ship in data if ship.get('active', True)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_shipdata(shipdata: pl.DataFrame, run_timestamp: str) -> Tuple[pl.DataFrame, Dict[str, pl.DataFrame]]:\n",
    "        shipdata = shipdata.unnest(\"data\")\n",
    "        \n",
    "        # Verschachtelte Tabellen extrahiren\n",
    "        tables = {}\n",
    "        for column, dtype in shipdata.collect_schema().items():\n",
    "            if dtype == pl.List(pl.Struct):\n",
    "                tables[column] = (\n",
    "                    shipdata.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .unnest(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    \n",
    "                )\n",
    "            elif dtype == pl.List:\n",
    "                tables[column] = (\n",
    "                    shipdata.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    \n",
    "                )\n",
    "\n",
    "        # Schiffsdaten ohne Verschachtelung extrahieren\n",
    "        shipdata = shipdata.select(\n",
    "            pl.exclude([col for col, dtype in shipdata.collect_schema().items() if dtype == pl.List])\n",
    "        ).with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        )\n",
    "\n",
    "        return shipdata, tables\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_signals(signals: pl.DataFrame, run_timestamp: str) -> pl.DataFrame:\n",
    "        if len(signals) == 0:\n",
    "            return signals\n",
    "\n",
    "        # Initiale Transformation    \n",
    "        signals = (\n",
    "            signals.unnest(\"signals\")\n",
    "            .unpivot(index=\"imo\", variable_name=\"signal\")\n",
    "            .unnest(\"value\")\n",
    "        )\n",
    "\n",
    "        # Verbleibende Verschachtelungen plätten\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Struct:\n",
    "                signals = signals.unnest(column)\n",
    "\n",
    "        # Null-Werte\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Null:\n",
    "                signals = signals.with_columns(pl.col(column).cast(pl.String))\n",
    "        \n",
    "        # Das Lade-Datum hinzufügen\n",
    "        signals = signals.with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        )\n",
    "                \n",
    "        return signals\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_timeseries(timeseries: pl.DataFrame, imo: str, run_timestamp: str) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \n",
    "        if len(timeseries) == 0:\n",
    "            return timeseries, pl.DataFrame()\n",
    "        \n",
    "        # Initiale Transformation\n",
    "        transformed = (\n",
    "            timeseries.drop(\"timestamp\")\n",
    "            .unpivot(variable_name=\"signal\")\n",
    "            .unnest(\"value\")\n",
    "            .unpivot(\n",
    "                index=\"signal\",\n",
    "                variable_name=\"signal_timestamp\",\n",
    "                value_name=\"signal_value\",\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.lit(imo).alias(\"imo\"),\n",
    "                pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Lücken (NULL-Werte) identifizieren\n",
    "        gaps = (\n",
    "            transformed\n",
    "            .filter(pl.col(\"signal_value\").is_null())\n",
    "            .select([\"imo\", \"signal\", \"signal_timestamp\", \"loaddate\"])\n",
    "            .with_columns(\n",
    "                pl.col(\"signal_timestamp\").alias(\"gap_start\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # NULL-Werte aus dem Hauptdatensatz entfernen\n",
    "        data = transformed.filter(pl.col(\"signal_value\").is_not_null())\n",
    "        \n",
    "        return data, gaps\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_gaps(gaps_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \n",
    "        if len(gaps_df) == 0:\n",
    "            return pl.DataFrame()\n",
    "            \n",
    "        # Gruppiere nach IMO und Signal, sortiere nach Zeitstempel\n",
    "        result = []\n",
    "\n",
    "        # Gruppieren nach IMO und Signal\n",
    "        for (imo, signal), group in gaps_df.group_by([\"imo\", \"signal\"]):\n",
    "            group = group.sort(\"gap_start\")\n",
    "\n",
    "            current_start = group[\"gap_start\"][0]\n",
    "            prev_time = current_start\n",
    "\n",
    "            for row in group[1:]:  # Iteriere ab zweitem Eintrag\n",
    "                curr_time = row[\"gap_start\"]\n",
    "\n",
    "                # Prüfe, ob mehr als 5 Minuten zwischen zwei Einträgen liegen\n",
    "                max_sec = 5*60\n",
    "                if (curr_time - prev_time).total_seconds() > max_sec:\n",
    "                    result.append({\n",
    "                        \"imo\": imo,\n",
    "                        \"signal\": signal,\n",
    "                        \"gap_start\": current_start,\n",
    "                        \"gap_end\": prev_time,\n",
    "                        \"loaddate\": row[\"loaddate\"]\n",
    "                    })\n",
    "                    current_start = curr_time  # Starte neue Lücke\n",
    "\n",
    "                prev_time = curr_time  # Aktualisiere den vorherigen Zeitstempel\n",
    "            \n",
    "            # Letzte Lücke hinzufügen\n",
    "            result.append({\n",
    "                \"imo\": imo,\n",
    "                \"signal\": signal,\n",
    "                \"gap_start\": current_start,\n",
    "                \"gap_end\": prev_time,\n",
    "                \"loaddate\": group[\"loaddate\"][-1]\n",
    "                    })\n",
    "\n",
    "        \n",
    "        return pl.DataFrame(result)\n",
    "    \n",
    "    @staticmethod\n",
    "    def enrich_timeseries_with_friendly_names(timeseries_df: pl.DataFrame, signals_df: pl.DataFrame) -> pl.DataFrame:\n",
    "\n",
    "        if len(timeseries_df) == 0 or len(signals_df) == 0:\n",
    "            return timeseries_df\n",
    "            \n",
    "        # Extrahiere Signal-Mapping (signal -> friendly_name)\n",
    "        signal_mapping = (\n",
    "            signals_df\n",
    "            .filter(pl.col(\"friendly_name\").is_not_null())\n",
    "            .select([\"signal\", \"friendly_name\"])\n",
    "            .unique()\n",
    "        )\n",
    "        \n",
    "        # Join mit Timeseries-Daten\n",
    "        return timeseries_df.join(\n",
    "            signal_mapping,\n",
    "            on=\"signal\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_daily_timeseries_summary(hist_df: pl.DataFrame, daily_df: pl.DataFrame, current_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \n",
    "        \n",
    "        combined_df = pl.concat([hist_df, daily_df, current_df])\n",
    "        summary_df = combined_df.unique(subset=[\"imo\", \"timestamp\", \"friendly_name\"], keep=\"first\").filter(pl.col(\"tag\")==\"new\"| pl.col(\"tag\")==\"today\")\n",
    "\n",
    "        return summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Datei, in der alle neuen einträge des Tages gespeichert sind im verzeichnis data/daily_summary/jahrmonattag.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abläufe Pipeline:\n",
    "IMO Liste und Signals Liste anlegen?\n",
    "ShipData\n",
    "    - max. einmal täglich\n",
    "    1. API\n",
    "    2. JSON speichern\n",
    "    3. IMO Nummern extrahieren und unter data/latest/imos speichern\n",
    "    4. Daten extrahiren\n",
    "    5. PARQUET speichern\n",
    "\n",
    "Signals \n",
    "    - max. einmal täglich\n",
    "    1. API ShipData\n",
    "    2. IMO Nummern laden\n",
    "    3. Pro Schiff (parallelisiert)\n",
    "        1. API Signals\n",
    "        2. JSON speichern\n",
    "        3. Daten extrahiren\n",
    "        4. PARQUET speichern\n",
    "        5. update der signal_mapping datei in data/latest (enthält imo, signal_id & friendly_name)\n",
    "\n",
    "\n",
    "Timeseries \n",
    "    - mehrfach täglich (ca. 1x/h)\n",
    "    1. API ShipData\n",
    "    2. IMO Nummern laden\n",
    "    3. Signal_mapping laden\n",
    "    4. aktuelle Daily Summary in daily_df laden & mit dem tag today versehen\n",
    "    5. Wenn noch keine Daily Summary Datei vorhanden, also daily_df leer ist, dann ref_data Datei aktualisieren, sodass nur die letzten x Tage darin vorkommen (ältesten Tag raus & letzten Tag rein)\n",
    "    6. Reference Data in hist_df laden & mit dem tag old versehen \n",
    "    7. current_df anlegen\n",
    "    8. Pro Schiff (parallelisiert)\n",
    "        1. API Timeseries (to_date = run_timestamp einfügen damit keine Abweichungen durch Abfragezeit)\n",
    "        2. JSON speichern\n",
    "        3. Daten extrahiren\n",
    "        4. Friendly_name anfügen (aus signal_mapping)\n",
    "        5. PARQUET speichern\n",
    "        6. einem gesamt Dataframe current_df hinzufügen\n",
    "    9. Daily Summary erstellen / ergänzen\n",
    "        - hist_df & daily_df & current_df zusammenfügen\n",
    "        - nur behalten was unique ist, zu erst im df steht & den tag new oder today hat \n",
    "        - im summary_df speichern\n",
    "    10. Daily summary (summary_df) als PARQUET speichern\n",
    "        - wenn noch keine datei zum aktuellen Tag, dann neu anlegen, sonst nur überschreiben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, config: Config, api_key: str, verify_ssl: bool = True):\n",
    "        self.config = config\n",
    "        self.api_client = API_Client(config.base_url, api_key)\n",
    "        self.processor = Data_Processor()\n",
    "        self.storage = Data_Storage(config)\n",
    "        self.logger = logging.getLogger('Pipeline')\n",
    "    \n",
    "    def process_shipdata(self, run_timestamp: str):\n",
    "        try:\n",
    "\n",
    "            self.logger.info(\"Processing ship data\")\n",
    "\n",
    "            # Get Shipdata\n",
    "            response, shipdata = self.api_client.get_data(\"fleet\")\n",
    "\n",
    "            # Get & Store IMO Numbers\n",
    "            imo_numbers = self.processor.get_imo_numbers(shipdata)\n",
    "            self.storage.write_file(\n",
    "                imo_numbers,\n",
    "                'imos',\n",
    "                f\"./data/latest\",\n",
    "                'parquet'\n",
    "            )\n",
    "\n",
    "            self.logger.info(f\"Found {len(imo_numbers)} active ships\")\n",
    "\n",
    "            if not shipdata:\n",
    "                self.logger.error(f\"No ship data received - {response}\")\n",
    "                raise ValueError(f\"No ship data received - {response}\")\n",
    "            \n",
    "            # Store raw ship data\n",
    "            self.storage.write_file(\n",
    "                    shipdata,\n",
    "                    'ShipData',\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "            \n",
    "            # Transform and store ship data\n",
    "            ships_df = pl.DataFrame(shipdata)\n",
    "            if ships_df.columns == ['detail']:\n",
    "                self.logger.info(\"Request Error: see json file for details\")\n",
    "            \n",
    "            else:\n",
    "                ships_transformed, tables = self.processor.transform_shipdata(ships_df, run_timestamp)\n",
    "            \n",
    "                self.storage.write_file(\n",
    "                        ships_transformed,\n",
    "                        'ShipData',\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                    \n",
    "                # Process nested tables\n",
    "                for name, table in tables.items():\n",
    "                        self.storage.write_file(\n",
    "                            table,\n",
    "                            f\"ShipData_{name}\",\n",
    "                            f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process ship data: {str(e)}\")\n",
    "\n",
    "\n",
    "    def process_signals(self, run_timestamp: str, imo_numbers:List[str], current_signals_df: pl.DataFrame):\n",
    "        try:\n",
    "            \n",
    "            for imo in imo_numbers:\n",
    "\n",
    "                self.logger.info(f\"Processing signals data for {imo}\")\n",
    "\n",
    "                response, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "\n",
    "                if not signals:\n",
    "                    self.logger.error(f\"No signals data received - {response}\")\n",
    "                    ValueError(f\"No signals data received - {response}\")\n",
    "\n",
    "                # Store raw signals data\n",
    "                self.storage.write_file(\n",
    "                        signals,\n",
    "                        f'Signals_{imo}',\n",
    "                        f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                        'json'\n",
    "                    )\n",
    "                \n",
    "                # Transform and store signals data\n",
    "                signals_df = pl.DataFrame(signals)\n",
    "                if signals_df.columns == ['detail']:\n",
    "                    self.logger.info(\"Request Error: see json file for details\")\n",
    "                else:\n",
    "                    signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "                    self.storage.write_file(\n",
    "                            signals_transformed,\n",
    "                            f'Signals_{imo}',\n",
    "                            f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                    self.logger.info(f\"Signals data processed for {imo}\")\n",
    "                \n",
    "                    # Update Signals Mapping\n",
    "\n",
    "                    new_signal_mapping = signals_transformed.select([\"imo\", \"signal\", \"friendly_name\"]).unique()\n",
    "\n",
    "                    if current_signals_df is None:\n",
    "                        self.storage.write_file(\n",
    "                            new_signal_mapping,\n",
    "                            'signal_mapping',\n",
    "                            f\"./data/latest\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                        self.logger.info(f\"Signal Mapping updated for the first time\")\n",
    "                    else:        \n",
    "                        # Add new Signals to existing Signal Mapping\n",
    "                        \n",
    "                        updated_signals = pl.concat([current_signals_df, new_signal_mapping]).unique(subset=[\"imo\", \"signal\", \"friendly_name\"], keep=\"first\")\n",
    "                        \n",
    "                        self.storage.write_file(\n",
    "                                updated_signals,\n",
    "                                \"signal_mapping\",\n",
    "                                f\"./data/latest\",\n",
    "                                \"parquet\")\n",
    "                        self.logger.info(f\"Signal Mapping updated\")    \n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process signals data: {str(e)}\")\n",
    "\n",
    "    def process_timeseries(self, run_timestamp: str, imo_numbers:List[str], signal_mapping:pl.DataFrame, current_df:pl.DataFrame, run_start)-> pl.DataFrame:\n",
    "        try:\n",
    "            current_df = current_df\n",
    "            \n",
    "            for imo in imo_numbers:\n",
    "\n",
    "                self.logger.info(f\"Processing timeseries data for {imo}\")\n",
    "\n",
    "                response, timeseries = self.api_client.get_data(f\"fleet/{imo}/timeseries\")\n",
    "                #response, timeseries = self.api_client.get_data(f\"fleet/{imo}/timeseries?to_date={run_start}\")\n",
    "\n",
    "                if not timeseries:\n",
    "                        self.logger.error(f\"No timeseries data received - {response}\")\n",
    "                        no_data = True\n",
    "\n",
    "\n",
    "                # Store raw timeseries data\n",
    "                self.storage.write_file(\n",
    "                        timeseries,\n",
    "                        f'Timeseries_{imo}',\n",
    "                        f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                        'json'\n",
    "                    )\n",
    "            \n",
    "                \n",
    "                \n",
    "                # Transform and store timeseries data\n",
    "                timeseries_df = pl.DataFrame(timeseries)\n",
    "\n",
    "                if timeseries_df.columns == ['detail']:\n",
    "                    self.logger.info(\"Request Error: see json file for details\")\n",
    "                else:\n",
    "                    if no_data:\n",
    "                        timeseries_transformed = pl.DataFrame({\"signal\": [], \"signal_timestamp\": [], \"signal_value\": [], \"imo\": [], \"loaddate\": [], \"friendly_name\": []})\n",
    "                        gaps = pl.DataFrame()\n",
    "                    else:\n",
    "                    \n",
    "                        timeseries_transformed, gaps = self.processor.transform_timeseries(timeseries_df, imo, run_timestamp)\n",
    "\n",
    "                    # Enrich with friendly names\n",
    "                    timeseries_transformed = self.processor.enrich_timeseries_with_friendly_names(timeseries_transformed, signal_mapping)\n",
    "                    self.storage.write_file(\n",
    "                            timeseries_transformed,\n",
    "                            f\"Timeseries_{imo}\",\n",
    "                            f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                    current_df = pl.concat([current_df, timeseries_transformed])\n",
    "                    \n",
    "                    # Process gaps\n",
    "                    gaps_df = self.processor.process_gaps(pl.DataFrame(gaps))\n",
    "                    self.storage.write_file(\n",
    "                            gaps_df,\n",
    "                            f\"Gaps_{imo}\",\n",
    "                            f\"{self.config.gaps_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                \n",
    "        except Exception as e:            \n",
    "            self.logger.error(f\"Failed to process timeseries data: {str(e)}\")\n",
    "\n",
    "        return current_df\n",
    "\n",
    "\n",
    "    def run(self, mode: str = \"all\"):\n",
    "\n",
    "        try: \n",
    "\n",
    "            run_start = datetime.now(timezone.utc)\n",
    "            run_timestamp = run_start.strftime('%Y/%m/%d/%H/%M')\n",
    "            summary_filename = f\"{run_start.strftime('%Y%m%d')}\"\n",
    "            self.logger.info(f\"Starting pipeline run at {run_start}\")\n",
    "\n",
    "            # Initialize directories\n",
    "            os.makedirs(f\"{self.config.raw_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.config.transformed_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.config.gaps_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"./data/latest\", exist_ok=True)\n",
    "            os.makedirs(f\"./data/daily_summary\", exist_ok=True)\n",
    "\n",
    "            current_signals_df = self.storage.read_file(\"signal_mapping\", \"./data/latest\", \"parquet\")\n",
    "\n",
    "            # Read daily and historical data\n",
    "            daily_df = self.storage.read_file(summary_filename, \"./data/daily_summary\", \"parquet\")\n",
    "            hist_df = self.storage.read_file(\"ref_data\", \"./data/latest\", \"parquet\")\n",
    "            if hist_df.is_empty: hist_df = pl.DataFrame({\"signal\": [], \"signal_timestamp\": [], \"signal_value\": [], \"imo\": [], \"loaddate\": [], \"friendly_name\": [], \"tag\": []})\n",
    "            \n",
    "\n",
    "            if daily_df.is_empty : # means it is a new day\n",
    "\n",
    "                daily_df = pl.DataFrame({\"signal\": [], \"signal_timestamp\": [], \"signal_value\": [], \"imo\": [], \"loaddate\": [], \"friendly_name\": [], \"tag\": []})\n",
    "                \n",
    "                last_day = (run_start - timedelta(days=1)).strftime('%Y%m%d')\n",
    "                last_day_df = self.storage.read_file(last_day, \"./data/daily_summary\", \"parquet\")\n",
    "                \n",
    "                if last_day_df.is_empty: \n",
    "                    last_day_df = pl.DataFrame({\"signal\": [], \"signal_timestamp\": [], \"signal_value\": [], \"imo\": [], \"loaddate\": [], \"friendly_name\": [], \"tag\": []})\n",
    "                else:\n",
    "                    last_day_df = last_day_df.with_columns(pl.col(\"hist\").alias(\"tag\"))\n",
    "                \n",
    "                hist_df = pl.concat([hist_df, last_day_df])\n",
    "                hist_df = hist_df.filter(pl.col(\"loaddate\") > run_start - timedelta(days=self.config.history_days))\n",
    "                self.storage.write_file(hist_df, \n",
    "                                \"ref_data\", \n",
    "                                \"./data/latest\", \n",
    "                                \"parquet\")\n",
    "                \n",
    "            else: daily_df = daily_df.with_columns(pl.col(\"today\").alias(\"tag\"))\n",
    "\n",
    "            current_df = pl.DataFrame({\"signal\": [], \"signal_timestamp\": [], \"signal_value\": [], \"imo\": [], \"loaddate\": [], \"friendly_name\": []})\n",
    "\n",
    "            self.logger.info(f\"Loaded {hist_df.shape[0]} historical records and {daily_df.shape[0]} daily records\")\n",
    "\n",
    "            self.logger.info(f\"Processing data for mode: {mode}\")\n",
    "\n",
    "\n",
    "            if mode in [\"all\", \"fleet\"]:\n",
    "\n",
    "                # Process Shipdata\n",
    "                self.process_shipdata(run_timestamp)\n",
    "\n",
    "                # Get IMO Numbers and turn them into list\n",
    "                imo_numbers = self.storage.read_file(\n",
    "                    'imos',\n",
    "                    f\"./data/latest\",\n",
    "                    'parquet'\n",
    "                )            \n",
    "                imo_numbers = imo_numbers.to_series(0).to_list()\n",
    "\n",
    "                self.logger.info(f\"Processing signals for {len(imo_numbers)} ships\")\n",
    "\n",
    "                # Process Signals\n",
    "                self.process_signals(run_timestamp, imo_numbers, current_signals_df)\n",
    "\n",
    "                self.logger.info(\"All Signals processed\")\n",
    "\n",
    "\n",
    "            if mode in [\"all\", \"timeseries\"]:\n",
    "\n",
    "                self.logger.info(\"Start to Process Timeseries Data\")\n",
    "                # Get IMO Numbers from file in data\n",
    "                imo_numbers = self.storage.read_file(\n",
    "                    'imos',\n",
    "                    f\"./data/latest\",\n",
    "                    'parquet'\n",
    "                )\n",
    "                imo_numbers = imo_numbers.to_series(0).to_list()\n",
    "\n",
    "                # Get Signal-Mapping\n",
    "                signal_mapping = self.storage.read_file(\n",
    "                    'signal_mapping',\n",
    "                    f\"./data/latest\",\n",
    "                    'parquet'\n",
    "                )\n",
    "\n",
    "                \n",
    "                \n",
    "                # Process Timeseries\n",
    "                current_df = self.process_timeseries(run_timestamp, imo_numbers, signal_mapping, current_df, run_start)\n",
    "                current_df = current_df.with_columns(pl.lit(\"new\").alias(\"tag\"))\n",
    "\n",
    "                # Save Delta\n",
    "                summary_df = self.processor.update_daily_timeseries_summary(hist_df, daily_df, current_df)\n",
    "                self.storage.write_file(\n",
    "                    summary_df,\n",
    "                    summary_filename,\n",
    "                    f\"./data/daily_summary\",\n",
    "                    \"parquet\"\n",
    "                )\n",
    "            \n",
    "            run_end = datetime.now(timezone.utc)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline run failed at {run_end}: {str(e)}\") \n",
    "            raise\n",
    "\n",
    "        \n",
    "\n",
    "        return run_start, run_end\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logging()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if environment variables are set\n",
    "api_key = os.getenv('HOPPE_API_KEY')\n",
    "if not api_key:\n",
    "    logger.error(\"HOPPE_API_KEY environment variable not set\")\n",
    "\n",
    "# Configure pipeline\n",
    "config = Config(\n",
    "    base_url=os.getenv('HOPPE_BASE_URL', \"https://api.hoppe-sts.com/\"),\n",
    "    raw_path=os.getenv('RAW_PATH', \"./data/raw_data\"),\n",
    "    transformed_path=os.getenv('TRANSFORMED_PATH', \"./data/transformed_data\"),\n",
    "    gaps_path=os.getenv('GAPS_PATH', \"./data/gaps_data\"),\n",
    "    batch_size = int(os.getenv('BATCH_SIZE', \"1000\")),\n",
    "    max_workers=int(os.getenv('MAX_WORKERS', \"4\")),\n",
    "    days_to_keep=int(os.getenv('DAYS_TO_KEEP', \"90\")),\n",
    "    history_days=int(os.getenv('HISTORY_DAYS', \"5\"))\n",
    ")\n",
    "\n",
    "mode = \"timeseries\" # \"all\" or \"timeseries\" or \"fleet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:32:13,756 - Pipeline - INFO - Starting pipeline run at 2025-03-18 17:32:13.756138+00:00\n",
      "2025-03-18 18:32:13,783 - Data Storage - INFO - Reading from signal_mapping.parquet file successfully\n",
      "2025-03-18 18:32:13,786 - Data Storage - INFO - File ./data/daily_summary/20250318.parquet does not exist\n",
      "2025-03-18 18:32:13,792 - Data Storage - INFO - Reading from ref_data.parquet file successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:32:13,796 - Data Storage - INFO - Reading from 20250317.parquet file successfully\n",
      "2025-03-18 18:32:13,803 - Data Storage - INFO - Writting to ref_data.parquet file successfully\n",
      "2025-03-18 18:32:13,805 - Data Storage - INFO - Data saved to ./data/latest/ref_data.parquet\n",
      "2025-03-18 18:32:13,807 - Pipeline - INFO - Loaded 0 historical records and 0 daily records\n",
      "2025-03-18 18:32:13,808 - Pipeline - INFO - Processing data for mode: timeseries\n",
      "2025-03-18 18:32:13,809 - Pipeline - INFO - Start to Process Timeseries Data\n",
      "2025-03-18 18:32:13,813 - Data Storage - INFO - Reading from imos.parquet file successfully\n",
      "2025-03-18 18:32:13,817 - Data Storage - INFO - Reading from signal_mapping.parquet file successfully\n",
      "2025-03-18 18:32:13,818 - Pipeline - INFO - Processing timeseries data for 9400071\n",
      "2025-03-18 18:32:14,017 - API Client - INFO - Request for fleet/9400071/timeseries successful\n",
      "2025-03-18 18:32:14,017 - Pipeline - ERROR - No timeseries data received - <Response [200]>\n",
      "2025-03-18 18:32:14,021 - Data Storage - INFO - Writting to Timeseries_9400071.json file successfully\n",
      "2025-03-18 18:32:14,022 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9400071.json\n",
      "2025-03-18 18:32:14,026 - Data Storage - INFO - Writting to Timeseries_9400071.parquet file successfully\n",
      "2025-03-18 18:32:14,027 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9400071.parquet\n",
      "2025-03-18 18:32:14,030 - Data Storage - INFO - Writting to Gaps_9400071.parquet file successfully\n",
      "2025-03-18 18:32:14,031 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9400071.parquet\n",
      "2025-03-18 18:32:14,032 - Pipeline - INFO - Processing timeseries data for 9725512\n",
      "2025-03-18 18:32:14,665 - API Client - INFO - Request for fleet/9725512/timeseries successful\n",
      "2025-03-18 18:32:14,745 - Data Storage - INFO - Writting to Timeseries_9725512.json file successfully\n",
      "2025-03-18 18:32:14,746 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9725512.json\n",
      "2025-03-18 18:32:14,818 - Data Storage - INFO - Writting to Timeseries_9725512.parquet file successfully\n",
      "2025-03-18 18:32:14,818 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9725512.parquet\n",
      "2025-03-18 18:32:14,823 - Data Storage - INFO - Writting to Gaps_9725512.parquet file successfully\n",
      "2025-03-18 18:32:14,825 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9725512.parquet\n",
      "2025-03-18 18:32:14,826 - Pipeline - INFO - Processing timeseries data for 9725524\n",
      "2025-03-18 18:32:16,589 - API Client - INFO - Request for fleet/9725524/timeseries successful\n",
      "2025-03-18 18:32:16,954 - Data Storage - INFO - Writting to Timeseries_9725524.json file successfully\n",
      "2025-03-18 18:32:16,956 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9725524.json\n",
      "2025-03-18 18:32:18,061 - Data Storage - INFO - Writting to Timeseries_9725524.parquet file successfully\n",
      "2025-03-18 18:32:18,062 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9725524.parquet\n",
      "2025-03-18 18:32:18,065 - Data Storage - INFO - Writting to Gaps_9725524.parquet file successfully\n",
      "2025-03-18 18:32:18,066 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9725524.parquet\n",
      "2025-03-18 18:32:18,066 - Pipeline - INFO - Processing timeseries data for 9778399\n",
      "2025-03-18 18:32:18,159 - API Client - INFO - Request for fleet/9778399/timeseries successful\n",
      "2025-03-18 18:32:18,162 - Pipeline - ERROR - No timeseries data received - <Response [200]>\n",
      "2025-03-18 18:32:18,164 - Data Storage - INFO - Writting to Timeseries_9778399.json file successfully\n",
      "2025-03-18 18:32:18,165 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9778399.json\n",
      "2025-03-18 18:32:18,178 - Data Storage - INFO - Writting to Timeseries_9778399.parquet file successfully\n",
      "2025-03-18 18:32:18,179 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9778399.parquet\n",
      "2025-03-18 18:32:18,182 - Data Storage - INFO - Writting to Gaps_9778399.parquet file successfully\n",
      "2025-03-18 18:32:18,183 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9778399.parquet\n",
      "2025-03-18 18:32:18,184 - Pipeline - INFO - Processing timeseries data for 9696826\n",
      "2025-03-18 18:32:18,778 - API Client - INFO - Request for fleet/9696826/timeseries successful\n",
      "2025-03-18 18:32:18,863 - Data Storage - INFO - Writting to Timeseries_9696826.json file successfully\n",
      "2025-03-18 18:32:18,864 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9696826.json\n",
      "2025-03-18 18:32:18,951 - Data Storage - INFO - Writting to Timeseries_9696826.parquet file successfully\n",
      "2025-03-18 18:32:18,953 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9696826.parquet\n",
      "2025-03-18 18:32:18,960 - Data Storage - INFO - Writting to Gaps_9696826.parquet file successfully\n",
      "2025-03-18 18:32:18,961 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9696826.parquet\n",
      "2025-03-18 18:32:18,962 - Pipeline - INFO - Processing timeseries data for 9693290\n",
      "2025-03-18 18:32:20,646 - API Client - INFO - Request for fleet/9693290/timeseries successful\n",
      "2025-03-18 18:32:21,008 - Data Storage - INFO - Writting to Timeseries_9693290.json file successfully\n",
      "2025-03-18 18:32:21,010 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9693290.json\n",
      "2025-03-18 18:32:22,254 - Data Storage - INFO - Writting to Timeseries_9693290.parquet file successfully\n",
      "2025-03-18 18:32:22,255 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9693290.parquet\n",
      "2025-03-18 18:32:22,267 - Data Storage - INFO - Writting to Gaps_9693290.parquet file successfully\n",
      "2025-03-18 18:32:22,269 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9693290.parquet\n",
      "2025-03-18 18:32:22,270 - Pipeline - INFO - Processing timeseries data for 9306184\n",
      "2025-03-18 18:32:22,980 - API Client - INFO - Request for fleet/9306184/timeseries successful\n",
      "2025-03-18 18:32:23,078 - Data Storage - INFO - Writting to Timeseries_9306184.json file successfully\n",
      "2025-03-18 18:32:23,079 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9306184.json\n",
      "2025-03-18 18:32:23,167 - Data Storage - INFO - Writting to Timeseries_9306184.parquet file successfully\n",
      "2025-03-18 18:32:23,169 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9306184.parquet\n",
      "2025-03-18 18:32:23,211 - Data Storage - INFO - Writting to Gaps_9306184.parquet file successfully\n",
      "2025-03-18 18:32:23,212 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9306184.parquet\n",
      "2025-03-18 18:32:23,214 - Pipeline - INFO - Processing timeseries data for 9693331\n",
      "2025-03-18 18:32:23,308 - API Client - INFO - Request for fleet/9693331/timeseries successful\n",
      "2025-03-18 18:32:23,310 - Pipeline - ERROR - No timeseries data received - <Response [200]>\n",
      "2025-03-18 18:32:23,313 - Data Storage - INFO - Writting to Timeseries_9693331.json file successfully\n",
      "2025-03-18 18:32:23,314 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9693331.json\n",
      "2025-03-18 18:32:23,326 - Data Storage - INFO - Writting to Timeseries_9693331.parquet file successfully\n",
      "2025-03-18 18:32:23,327 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9693331.parquet\n",
      "2025-03-18 18:32:23,583 - Data Storage - INFO - Writting to Gaps_9693331.parquet file successfully\n",
      "2025-03-18 18:32:23,584 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9693331.parquet\n",
      "2025-03-18 18:32:23,584 - Pipeline - INFO - Processing timeseries data for 9306287\n",
      "2025-03-18 18:32:24,367 - API Client - INFO - Request for fleet/9306287/timeseries successful\n",
      "2025-03-18 18:32:24,469 - Data Storage - INFO - Writting to Timeseries_9306287.json file successfully\n",
      "2025-03-18 18:32:24,470 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9306287.json\n",
      "2025-03-18 18:32:24,563 - Data Storage - INFO - Writting to Timeseries_9306287.parquet file successfully\n",
      "2025-03-18 18:32:24,564 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9306287.parquet\n",
      "2025-03-18 18:32:25,641 - Data Storage - INFO - Writting to Gaps_9306287.parquet file successfully\n",
      "2025-03-18 18:32:25,642 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9306287.parquet\n",
      "2025-03-18 18:32:25,643 - Pipeline - INFO - Processing timeseries data for 9306160\n",
      "2025-03-18 18:32:26,454 - API Client - INFO - Request for fleet/9306160/timeseries successful\n",
      "2025-03-18 18:32:26,564 - Data Storage - INFO - Writting to Timeseries_9306160.json file successfully\n",
      "2025-03-18 18:32:26,565 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9306160.json\n",
      "2025-03-18 18:32:26,665 - Data Storage - INFO - Writting to Timeseries_9306160.parquet file successfully\n",
      "2025-03-18 18:32:26,666 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9306160.parquet\n",
      "2025-03-18 18:32:58,862 - Data Storage - INFO - Writting to Gaps_9306160.parquet file successfully\n",
      "2025-03-18 18:32:58,907 - Data Storage - INFO - Data saved to ./data/gaps_data/2025/03/18/17/32/Gaps_9306160.parquet\n",
      "2025-03-18 18:32:58,911 - Pipeline - INFO - Processing timeseries data for 9696838\n",
      "2025-03-18 18:32:59,565 - API Client - INFO - Request for fleet/9696838/timeseries successful\n",
      "2025-03-18 18:32:59,665 - Data Storage - INFO - Writting to Timeseries_9696838.json file successfully\n",
      "2025-03-18 18:32:59,667 - Data Storage - INFO - Data saved to ./data/raw_data/2025/03/18/17/32/Timeseries_9696838.json\n",
      "2025-03-18 18:32:59,778 - Data Storage - INFO - Writting to Timeseries_9696838.parquet file successfully\n",
      "2025-03-18 18:32:59,780 - Data Storage - INFO - Data saved to ./data/transformed_data/2025/03/18/17/32/Timeseries_9696838.parquet\n"
     ]
    }
   ],
   "source": [
    "# Create and run pipeline\n",
    "try:\n",
    "    pipeline = Pipeline(config, api_key)\n",
    "    run_start, run_end = pipeline.run(mode)  \n",
    "    logger.info(f\"Pipeline run completed at {run_end}: total runtime {run_end - run_start}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Pipeline run failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoppe-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
