{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Virtuelle Umgebung erstellen\n",
    "python -m venv hoppe-env\n",
    "\n",
    "# Umgebung aktivieren\n",
    "source hoppe-env/Scripts/activate\n",
    "\n",
    "# Pakete installiereb\n",
    "pip install -r requirements.txt --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import fabric  # Fabric-spezifische Bibliothek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptordner erstellen\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Unterordner erstellen\n",
    "for sub_dir in ['raw_data', 'transformed_data', 'gaps_data']:\n",
    "    os.makedirs(os.path.join('../data', sub_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging einrichten\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"hoppe_etl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion für Fabric Storage-Integration\n",
    "def setup_fabric_storage():\n",
    "    \"\"\"\n",
    "    Richtet die Verbindung zum Fabric Lakehouse ein\n",
    "    \"\"\"\n",
    "    # Annahme: In Fabric ist das Lakehouse bereits konfiguriert\n",
    "    try:\n",
    "        # Pfade für die verschiedenen Datenebenen\n",
    "        lakehouse_path = fabric.get_lakehouse_path()\n",
    "        raw_path = f\"{lakehouse_path}/Files/raw_data\"\n",
    "        transformed_path = f\"{lakehouse_path}/Files/transformed_data\"\n",
    "        gaps_path = f\"{lakehouse_path}/Files/gaps_data\"\n",
    "        \n",
    "        # Stellen Sie sicher, dass die Verzeichnisse existieren\n",
    "        os.makedirs(raw_path, exist_ok=True)\n",
    "        os.makedirs(transformed_path, exist_ok=True)\n",
    "        os.makedirs(gaps_path, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Fabric storage paths configured: {raw_path}, {transformed_path}, {gaps_path}\")\n",
    "        \n",
    "        return {\n",
    "            'RAW_PATH': raw_path,\n",
    "            'TRANSFORMED_PATH': transformed_path,\n",
    "            'GAPS_PATH': gaps_path\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to setup Fabric storage: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion für Fabric Secrets Management\n",
    "def get_fabric_secrets():\n",
    "    \"\"\"\n",
    "    Holt Secrets aus dem Fabric KeyVault\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Annahme: In Fabric sind die Geheimnisse als Parameter gespeichert\n",
    "        api_key = fabric.get_secret(\"HOPPE_API_KEY\")\n",
    "        db_connection = fabric.get_secret(\"MSSQL_CONNECTION_STRING\") \n",
    "        \n",
    "        if not api_key:\n",
    "            raise ValueError(\"HOPPE_API_KEY not found in Fabric secrets\")\n",
    "            \n",
    "        logger.info(\"Successfully retrieved secrets from Fabric\")\n",
    "        \n",
    "        return {\n",
    "            'HOPPE_API_KEY': api_key,\n",
    "            'MSSQL_CONNECTION_STRING': db_connection\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get Fabric secrets: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der ETL-Pipeline-Klassen\n",
    "from config import Config\n",
    "from pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fabric_pipeline(mode=\"all\"):\n",
    "    \"\"\"\n",
    "    Führt die ETL-Pipeline in Fabric aus\n",
    "    \n",
    "    Args:\n",
    "        mode (str): 'all', 'timeseries', oder 'fleet'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fabric Storage einrichten\n",
    "        storage_config = setup_fabric_storage()\n",
    "        \n",
    "        # Secrets holen\n",
    "        secrets = get_fabric_secrets()\n",
    "        \n",
    "        # API-Key überprüfen\n",
    "        api_key = secrets.get('HOPPE_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"HOPPE_API_KEY not found in environment or secrets\")\n",
    "        \n",
    "        # Pipeline konfigurieren\n",
    "        config = Config(\n",
    "            base_url=os.getenv('HOPPE_BASE_URL', \"https://api.hoppe-sts.com/\"),\n",
    "            raw_path=storage_config.get('RAW_PATH', \"./data/raw_data\"),\n",
    "            transformed_path=storage_config.get('TRANSFORMED_PATH', \"./data/transformed_data\"),\n",
    "            gaps_path=storage_config.get('GAPS_PATH', \"./data/gaps_data\"),\n",
    "            max_workers=int(os.getenv('MAX_WORKERS', \"8\")),\n",
    "            retry_attempts=int(os.getenv('RETRY_ATTEMPTS', \"5\")),\n",
    "            timeout=int(os.getenv('TIMEOUT', \"45\")),\n",
    "            days_to_keep=int(os.getenv('DAYS_TO_KEEP', \"90\")),\n",
    "            history_days=int(os.getenv('HISTORY_DAYS', \"5\"))\n",
    "        )\n",
    "        \n",
    "        # Pipeline erstellen und ausführen\n",
    "        pipeline = Pipeline(config, api_key)\n",
    "        run_timestamp = pipeline.run(mode)\n",
    "        \n",
    "        # Wenn Datenbankverbindung konfiguriert ist, in Datenbank speichern\n",
    "        db_connection_string = secrets.get('MSSQL_CONNECTION_STRING')\n",
    "        if db_connection_string:\n",
    "            try:\n",
    "                import sqlalchemy as sa\n",
    "                engine = sa.create_engine(db_connection_string)\n",
    "                logger.info(\"Database connection established\")\n",
    "                \n",
    "                # Daten verarbeiten und in Datenbank speichern\n",
    "                pipeline.process_and_store_to_db(engine, run_timestamp)\n",
    "                logger.info(\"Database integration completed successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Database integration failed: {str(e)}\")\n",
    "        else:\n",
    "            logger.warning(\"MSSQL_CONNECTION_STRING not set, skipping database integration\")\n",
    "        \n",
    "        # Protokollieren des Abschlusses für Fabric-Aktivitätsüberwachung\n",
    "        fabric.log_activity(f\"Pipeline run completed successfully with mode: {mode}\")\n",
    "        \n",
    "        return {\"status\": \"success\", \"run_timestamp\": run_timestamp}\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Pipeline failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        fabric.log_activity(f\"Pipeline run failed: {error_msg}\", status=\"error\")\n",
    "        \n",
    "        return {\"status\": \"error\", \"error\": error_msg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Täglich ausführen (vollständige Pipeline)\n",
    "\n",
    "# Diese Zelle wird als geplante tägliche Aktivität in Fabric konfiguriert\n",
    "# Sie führt die vollständige Pipeline aus\n",
    "\n",
    "result = run_fabric_pipeline(mode=\"all\")\n",
    "print(f\"Pipeline result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries Pipeline (alle 15 Minuten)\n",
    "\n",
    "# Diese Zelle wird als geplante 15-Minuten-Aktivität in Fabric konfiguriert\n",
    "# Sie führt nur den Timeseries-Teil der Pipeline aus\n",
    "\n",
    "result = run_fabric_pipeline(mode=\"timeseries\")\n",
    "print(f\"Timeseries pipeline result: {result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoppe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
