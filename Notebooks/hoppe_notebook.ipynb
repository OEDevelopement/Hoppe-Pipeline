{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL-Pipeline zum Import von Sensor-Daten\n",
    "\n",
    "## Aufgabenstellung\n",
    "Firma: Peter Döhle Schiffahrts-KG\n",
    "\n",
    "Daten: Signaldaten von 12+ Schiffen\n",
    "- Treibstoff Emissionen, Tankfüllstand, Geschwindigkeit, Gewicht,...\n",
    "- im 15 min Takt via Rest API verfügbar\n",
    "\n",
    "ursprüngliches Ziel: Skalierbare ETL-Strecke für den poc von Microsoft Fabric\n",
    "\n",
    "geändertes Ziel: Vergleich verschiedener ETL-Ansätze anhand festgelegter Kriterien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-Endpoints\n",
    "\n",
    "Response: JSON-Format\n",
    "- /fleet : Alle Schiffe inkl. technischer Informationen\n",
    "- /fleet/{imo}/signals: Auflistung der für das Schiff verfügbaren Sensordaten inkl. Erklärung\n",
    "- /fleet/{imo}/timeseries?{optionaleParameter}: Zeitreihe der Sensordaten des Schiffes in 5-min Abständen\n",
    "\n",
    "Zielbild: \n",
    "Zeitreihe mit verständlichen Signalnamen pivotisiert und vollständig historisiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herausforderungen\n",
    "\n",
    "- Teils starke Verschachtelung\n",
    "- Schlechte Verbindung des Schiffes \n",
    "    - Fehlende Daten als NULL gespeichert\n",
    "- Fehlende Daten teils nachträglich überschrieben \n",
    "    - überlappendes Laden notwendig\n",
    "- Veränderung der Anzahl der Signale \n",
    "- Pivotierung führt zu sehr vielen Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "```bash\n",
    "# Virtuelle Umgebung erstellen\n",
    "python -m venv hoppe-env\n",
    "\n",
    "# Umgebung aktivieren\n",
    "source hoppe-env/Scripts/activate\n",
    "\n",
    "# Pakete installiereb\n",
    "pip install -r requirements.txt --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der ETL-Pipeline-Klassen\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy.sql import text\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "from typing import Dict, Optional, Tuple, List, Union\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptordner erstellen\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Unterordner erstellen\n",
    "for sub_dir in ['raw_data', 'transformed_data', 'gaps_data']:\n",
    "    os.makedirs(os.path.join('../data', sub_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging einrichten\n",
    "def setup_logging():\n",
    "    \"\"\"Konfiguriert das Logging\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"pipeline.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(\"hoppe_etl_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for the data pipeline\"\"\"\n",
    "    base_url: str = \"https://api.hoppe-sts.com/\"\n",
    "    raw_path: str = \"./data/raw_data\"\n",
    "    transformed_path: str = \"./data/transformed_data\"\n",
    "    gaps_path: str = \"./data/gaps_data\"  # Neuer Pfad für Null-Wert-Lücken\n",
    "    batch_size: int = 1000\n",
    "    max_workers: int = 8  # Erhöhte Worker für bessere Parallelisierung\n",
    "    retry_attempts: int = 5  # Erhöhte Retry-Versuche\n",
    "    timeout: int = 45  # Erhöhter Timeout\n",
    "    days_to_keep: int = 90  # Daten werden für 90 Tage aufbewahrt\n",
    "    history_days: int = 5  # Letzten 5 Tage für Historie laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIClient:\n",
    "    \"\"\"Handles API communication with retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_key: str, timeout: int = 30):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.session = self._create_session(timeout)\n",
    "\n",
    "    def _create_session(self, timeout: int) -> requests.Session:\n",
    "        \"\"\"Creates requests session with retry logic\"\"\"\n",
    "        session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session.headers.update({\n",
    "            \"Authorization\": f\"ApiKey {self.api_key}\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        })\n",
    "        session.timeout = timeout\n",
    "        return session\n",
    "    \n",
    "    def get_data(self, relative_url: str, params: Optional[Dict] = None) -> Tuple[requests.Response, Optional[dict]]:\n",
    "        \"\"\"\n",
    "        Fetches data from API with error handling\n",
    "        \n",
    "        Args:\n",
    "            relative_url: API endpoint path\n",
    "            params: Optional query parameters\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (Response, JSON data)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            request_url = f\"{self.base_url}{relative_url}\"\n",
    "            response = self.session.get(request_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response, response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"API request failed: {str(e)}\")\n",
    "            if hasattr(e, 'response'):\n",
    "                return e.response, None\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "    \"\"\"Handles data storage operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def write_file(\n",
    "        self, \n",
    "        data: Union[List, Dict, pl.DataFrame],\n",
    "        filename: str,\n",
    "        path: str,\n",
    "        postfix: str\n",
    "    ) -> None:\n",
    "        \"\"\"Writes data to file system\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        full_path = f\"{path}/{filename}.{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            if postfix == 'json':\n",
    "                with open(full_path, 'w') as f:\n",
    "                    json.dump(data, f)\n",
    "            elif postfix == 'parquet':\n",
    "                if not isinstance(data, pl.DataFrame):\n",
    "                    if isinstance(data, list) or isinstance(data, dict):\n",
    "                        data = pl.DataFrame(data)\n",
    "                    else:\n",
    "                        raise ValueError(\"Data must be DataFrame, List, or Dict for parquet format\")\n",
    "                data.write_parquet(full_path, compression=\"snappy\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {postfix}\")\n",
    "                \n",
    "            logger.info(f\"Data saved to {full_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write file {full_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def cleanup_old_data(self, base_path: str, days_to_keep: int = 90) -> None:\n",
    "        \"\"\"\n",
    "        Löscht Daten, die älter als days_to_keep Tage sind\n",
    "        \"\"\"\n",
    "        try:\n",
    "            today = datetime.now()\n",
    "            cutoff_date = today - timedelta(days=days_to_keep)\n",
    "            \n",
    "            # Wandle in Pfad-Format um (Jahr/Monat/Tag)\n",
    "            cutoff_path = cutoff_date.strftime('%Y/%m/%d')\n",
    "            base_path = Path(base_path)\n",
    "            \n",
    "            if not base_path.exists():\n",
    "                return\n",
    "                \n",
    "            # Durchsuche alle Jahresordner\n",
    "            for year_dir in base_path.glob(\"*\"):\n",
    "                if not year_dir.is_dir() or not year_dir.name.isdigit():\n",
    "                    continue\n",
    "                    \n",
    "                year = int(year_dir.name)\n",
    "                \n",
    "                # Überspringe Ordner, die definitiv behalten werden sollen\n",
    "                if year > cutoff_date.year:\n",
    "                    continue\n",
    "                \n",
    "                # Behandle Jahre, die teilweise gelöscht werden müssen\n",
    "                if year == cutoff_date.year:\n",
    "                    for month_dir in year_dir.glob(\"*\"):\n",
    "                        if not month_dir.is_dir() or not month_dir.name.isdigit():\n",
    "                            continue\n",
    "                            \n",
    "                        month = int(month_dir.name)\n",
    "                        \n",
    "                        # Überspringe Monate, die definitiv behalten werden sollen\n",
    "                        if month > cutoff_date.month:\n",
    "                            continue\n",
    "                            \n",
    "                        # Behandle Monate, die teilweise gelöscht werden müssen\n",
    "                        if month == cutoff_date.month:\n",
    "                            for day_dir in month_dir.glob(\"*\"):\n",
    "                                if not day_dir.is_dir() or not day_dir.name.isdigit():\n",
    "                                    continue\n",
    "                                    \n",
    "                                day = int(day_dir.name)\n",
    "                                \n",
    "                                # Lösche Tage, die älter als der Cutoff sind\n",
    "                                if day < cutoff_date.day:\n",
    "                                    logger.info(f\"Removing old data directory: {day_dir}\")\n",
    "                                    # In Produktion wäre hier tatsächliches Löschen (shutil.rmtree)\n",
    "                                    # Für Sicherheit vorerst nur Logging\n",
    "                                    # import shutil\n",
    "                                    # shutil.rmtree(day_dir)\n",
    "                        \n",
    "                        # Lösche den gesamten Monat, wenn er älter als der Cutoff-Monat ist\n",
    "                        elif month < cutoff_date.month:\n",
    "                            logger.info(f\"Removing old data directory: {month_dir}\")\n",
    "                            # import shutil\n",
    "                            # shutil.rmtree(month_dir)\n",
    "                \n",
    "                # Lösche das gesamte Jahr, wenn es älter als das Cutoff-Jahr ist\n",
    "                elif year < cutoff_date.year:\n",
    "                    logger.info(f\"Removing old data directory: {year_dir}\")\n",
    "                    # import shutil\n",
    "                    # shutil.rmtree(year_dir)\n",
    "                    \n",
    "            logger.info(f\"Cleanup of data older than {cutoff_date.strftime('%Y-%m-%d')} completed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data cleanup: {str(e)}\")\n",
    "\n",
    "    def write_to_db(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        engine: sa.Engine,\n",
    "        table_name: str,\n",
    "        if_exists: str = \"replace\",\n",
    "        batch_size: int = 10000\n",
    "    ) -> None:\n",
    "        \"\"\"Writes DataFrame to database in batches\"\"\"\n",
    "        try:\n",
    "            # Convert to pandas for writing to database in batches\n",
    "            pdf = df.to_pandas()\n",
    "            total_rows = len(pdf)\n",
    "            \n",
    "            if total_rows == 0:\n",
    "                logger.warning(f\"No data to write to table {table_name}\")\n",
    "                return\n",
    "                \n",
    "            logger.info(f\"Writing {total_rows} rows to table {table_name}\")\n",
    "            \n",
    "            # Write in batches to avoid memory issues\n",
    "            for i in range(0, total_rows, batch_size):\n",
    "                end = min(i + batch_size, total_rows)\n",
    "                batch = pdf.iloc[i:end]\n",
    "                \n",
    "                # For first batch, replace or append based on if_exists parameter\n",
    "                if i == 0:\n",
    "                    batch.to_sql(\n",
    "                        table_name,\n",
    "                        engine,\n",
    "                        if_exists=if_exists,\n",
    "                        index=False,\n",
    "                        method='multi',\n",
    "                        chunksize=1000\n",
    "                    )\n",
    "                else:\n",
    "                    # For subsequent batches, always append\n",
    "                    batch.to_sql(\n",
    "                        table_name,\n",
    "                        engine,\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        method='multi',\n",
    "                        chunksize=1000\n",
    "                    )\n",
    "                    \n",
    "                logger.info(f\"Wrote batch {i//batch_size + 1} of {(total_rows-1)//batch_size + 1} to table {table_name}\")\n",
    "                \n",
    "            logger.info(f\"Data successfully written to table {table_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Database write failed: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def write_ts_to_msdb(\n",
    "        self,\n",
    "        df: pl.DataFrame, \n",
    "        engine: sa.Engine,\n",
    "        batch_size: int = 10000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Writes time series data to MSSQL database using a staging table approach.\n",
    "        \n",
    "        This specialized method:\n",
    "        1. Writes the dataframe to a staging table\n",
    "        2. Ensures the main table exists with the correct schema\n",
    "        3. Adds any missing columns to the main table\n",
    "        4. Merges data from staging to the main table using MERGE statement\n",
    "        \n",
    "        Parameters:\n",
    "            df (pl.DataFrame): Time series data to write\n",
    "            engine (sa.Engine): SQLAlchemy engine for database connection\n",
    "            batch_size (int): Number of rows to write in each batch\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If dataframe is empty, nothing to do\n",
    "            if len(df) == 0:\n",
    "                logger.warning(\"No time series data to write to database\")\n",
    "                return\n",
    "            \n",
    "            # Step 1: Write DataFrame to staging table\n",
    "            self.write_to_db(df, engine, \"TimeSeries_Staging\", if_exists=\"replace\", batch_size=batch_size)\n",
    "            logger.info(\"Data written to TimeSeries_Staging table\")\n",
    "            \n",
    "            # Step 2: Ensure main pivot table exists\n",
    "            create_pivot_table_sql = \"\"\"\n",
    "            IF OBJECT_ID('TimeSeries_pivot', 'U') IS NULL\n",
    "            BEGIN\n",
    "                CREATE TABLE TimeSeries_pivot (\n",
    "                    imo NVARCHAR(255) NOT NULL,\n",
    "                    signal_timestamp DATETIME NOT NULL,\n",
    "                    loaddate NVARCHAR(255),\n",
    "                    PRIMARY KEY (imo, signal_timestamp)\n",
    "                );\n",
    "            END\n",
    "            \"\"\"\n",
    "            \n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(create_pivot_table_sql))\n",
    "                conn.commit()\n",
    "                logger.info(\"Ensured TimeSeries_pivot table exists\")\n",
    "            \n",
    "            # Step 3: Add missing columns to main table\n",
    "            add_columns_sql = \"\"\"\n",
    "            DECLARE @column_name NVARCHAR(255)\n",
    "            DECLARE @sql NVARCHAR(MAX)\n",
    "\n",
    "            DECLARE column_cursor CURSOR FOR\n",
    "            SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS \n",
    "            WHERE TABLE_NAME = 'TimeSeries_Staging'\n",
    "            AND COLUMN_NAME NOT IN (SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'TimeSeries_pivot')\n",
    "            AND COLUMN_NAME NOT IN ('imo', 'signal_timestamp', 'loaddate')  -- Skip key columns and loaddate\n",
    "\n",
    "            OPEN column_cursor\n",
    "            FETCH NEXT FROM column_cursor INTO @column_name\n",
    "\n",
    "            WHILE @@FETCH_STATUS = 0\n",
    "            BEGIN\n",
    "                SET @sql = 'ALTER TABLE TimeSeries_pivot ADD [' + @column_name + '] FLOAT NULL'\n",
    "                EXEC sp_executesql @sql\n",
    "                FETCH NEXT FROM column_cursor INTO @column_name\n",
    "            END\n",
    "\n",
    "            CLOSE column_cursor\n",
    "            DEALLOCATE column_cursor\n",
    "            \"\"\"\n",
    "\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(add_columns_sql))\n",
    "                conn.commit()\n",
    "                logger.info(\"Added any missing columns to TimeSeries_pivot table\")\n",
    "            \n",
    "            # Step 4: Pivotieren der Daten in der Staging-Tabelle\n",
    "            # Dieses SQL transformiert die Daten von der Staging-Tabelle, wo sie im Format\n",
    "            # imo, signal, signal_timestamp, signal_value sind, in das Pivot-Format\n",
    "            pivot_staging_sql = \"\"\"\n",
    "            -- Create a temporary table to hold pivoted data\n",
    "            IF OBJECT_ID('tempdb..#TempPivot', 'U') IS NOT NULL\n",
    "                DROP TABLE #TempPivot;\n",
    "\n",
    "            -- Get the list of unique signals for dynamic pivot\n",
    "            DECLARE @columns NVARCHAR(MAX);\n",
    "            DECLARE @sql NVARCHAR(MAX);\n",
    "\n",
    "            -- Create a list of signals as columns for the PIVOT operation\n",
    "            SELECT @columns = STRING_AGG(QUOTENAME(signal), ',')\n",
    "            FROM (SELECT DISTINCT signal FROM TimeSeries_Staging) AS signals;\n",
    "\n",
    "            -- Prepare the dynamic SQL for pivoting\n",
    "            SET @sql = N'\n",
    "            SELECT imo, signal_timestamp, loaddate, ' + @columns + '\n",
    "            INTO #TempPivot\n",
    "            FROM (\n",
    "                SELECT imo, signal, signal_timestamp, signal_value, loaddate\n",
    "                FROM TimeSeries_Staging\n",
    "            ) AS src\n",
    "            PIVOT (\n",
    "                MAX(signal_value)\n",
    "                FOR signal IN (' + @columns + ')\n",
    "            ) AS pvt;\n",
    "            ';\n",
    "\n",
    "            -- Execute the dynamic SQL to create the temporary pivot table\n",
    "            EXEC sp_executesql @sql;\n",
    "\n",
    "            -- Create a MERGE statement to update the main table\n",
    "            SET @sql = N'\n",
    "            MERGE INTO TimeSeries_pivot AS target\n",
    "            USING #TempPivot AS source\n",
    "            ON target.imo = source.imo AND target.signal_timestamp = source.signal_timestamp\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET \n",
    "                    loaddate = source.loaddate' +\n",
    "                    -- Add column updates from temp table, excluding key columns\n",
    "                    (SELECT STRING_AGG(', ' + QUOTENAME(COLUMN_NAME) + ' = source.' + QUOTENAME(COLUMN_NAME), '')\n",
    "                     FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                     WHERE TABLE_NAME = 'TimeSeries_pivot'\n",
    "                     AND COLUMN_NAME NOT IN ('imo', 'signal_timestamp', 'loaddate')) + '\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (imo, signal_timestamp, loaddate' +\n",
    "                    -- Add columns from temp table, excluding key columns\n",
    "                    (SELECT STRING_AGG(', ' + QUOTENAME(COLUMN_NAME), '')\n",
    "                     FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                     WHERE TABLE_NAME = 'TimeSeries_pivot'\n",
    "                     AND COLUMN_NAME NOT IN ('imo', 'signal_timestamp', 'loaddate')) + ')\n",
    "                VALUES (source.imo, source.signal_timestamp, source.loaddate' +\n",
    "                    -- Add values from temp table, excluding key columns\n",
    "                    (SELECT STRING_AGG(', source.' + QUOTENAME(COLUMN_NAME), '')\n",
    "                     FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                     WHERE TABLE_NAME = 'TimeSeries_pivot'\n",
    "                     AND COLUMN_NAME NOT IN ('imo', 'signal_timestamp', 'loaddate')) + ');';\n",
    "\n",
    "            -- Execute the merge\n",
    "            EXEC sp_executesql @sql;\n",
    "\n",
    "            -- Clean up\n",
    "            DROP TABLE #TempPivot;\n",
    "            \"\"\"\n",
    "            \n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(pivot_staging_sql))\n",
    "                conn.commit()\n",
    "                logger.info(\"Data merged into TimeSeries_pivot table\")\n",
    "                \n",
    "            # Optional: Schreibe Gaps-Daten in separate Tabelle\n",
    "            gaps_table_sql = \"\"\"\n",
    "            IF OBJECT_ID('TimeSeries_Gaps', 'U') IS NULL\n",
    "            BEGIN\n",
    "                CREATE TABLE TimeSeries_Gaps (\n",
    "                    imo NVARCHAR(255) NOT NULL,\n",
    "                    signal NVARCHAR(255) NOT NULL,\n",
    "                    gap_start DATETIME NOT NULL,\n",
    "                    gap_end DATETIME NOT NULL,\n",
    "                    loaddate NVARCHAR(255),\n",
    "                    PRIMARY KEY (imo, signal, gap_start)\n",
    "                );\n",
    "            END\n",
    "            \"\"\"\n",
    "            \n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(gaps_table_sql))\n",
    "                conn.commit()\n",
    "                logger.info(\"Ensured TimeSeries_Gaps table exists\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"MSSQL database operation failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Processes raw API data into analytics-ready format\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_imo_numbers(data: List[dict]) -> List[str]:\n",
    "        \"\"\"Extracts IMO numbers from ship data\"\"\"\n",
    "        return [ship['imo'] for ship in data if ship.get('active', True)]\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_signals(signals: pl.DataFrame, run_timestamp: str) -> pl.DataFrame:\n",
    "        \"\"\"Transforms signals data\"\"\"\n",
    "        if len(signals) == 0:\n",
    "            return signals\n",
    "            \n",
    "        signals = (\n",
    "            signals.lazy()\n",
    "            .unnest(\"signals\")\n",
    "            .unpivot(index=\"imo\", variable_name=\"signal\")\n",
    "            .unnest(\"value\")\n",
    "        )\n",
    "\n",
    "        # Unnest remaining structs\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Struct:\n",
    "                signals = signals.unnest(column)\n",
    "\n",
    "        # Handle null columns\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Null:\n",
    "                signals = signals.with_columns(pl.col(column).cast(pl.String))\n",
    "        \n",
    "        # Add loaddate\n",
    "        signals = signals.with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        )\n",
    "                \n",
    "        return signals.collect()\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_timeseries(timeseries: pl.DataFrame, imo: str, run_timestamp: str) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Transforms time series data and extracts gaps data\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (transformed_data, gaps_data)\n",
    "        \"\"\"\n",
    "        if len(timeseries) == 0:\n",
    "            return timeseries, pl.DataFrame()\n",
    "        \n",
    "        # Initial transformation\n",
    "        transformed = (\n",
    "            timeseries.lazy()\n",
    "            .drop(\"timestamp\")\n",
    "            .unpivot(variable_name=\"signal\")\n",
    "            .unnest(\"value\")\n",
    "            .unpivot(\n",
    "                index=\"signal\",\n",
    "                variable_name=\"signal_timestamp\",\n",
    "                value_name=\"signal_value\",\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.lit(imo).alias(\"imo\"),\n",
    "                pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Identifiziere Lücken (NULL-Werte)\n",
    "        gaps = (\n",
    "            transformed\n",
    "            .filter(pl.col(\"signal_value\").is_null())\n",
    "            .select([\"imo\", \"signal\", \"signal_timestamp\", \"loaddate\"])\n",
    "            .with_columns(\n",
    "                pl.col(\"signal_timestamp\").alias(\"gap_start\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Entferne NULL-Werte aus dem Hauptdatensatz\n",
    "        data = transformed.filter(pl.col(\"signal_value\").is_not_null())\n",
    "        \n",
    "        return data.collect(), gaps.collect()\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_gaps(gaps_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Verarbeitet Lücken-Daten, um zusammenhängende Zeiträume zu identifizieren\n",
    "        \"\"\"\n",
    "        if len(gaps_df) == 0:\n",
    "            return pl.DataFrame()\n",
    "            \n",
    "        # Gruppiere nach IMO und Signal, sortiere nach Zeitstempel\n",
    "        result = []\n",
    "        \n",
    "        # Konvertiere zu Pandas für einfachere Gruppierung und Verarbeitung\n",
    "        # (In einer produktiven Umgebung kann dies für große Datensätze \n",
    "        # effizienter mit Polars-nativen Funktionen implementiert werden)\n",
    "        gaps_pd = gaps_df.to_pandas()\n",
    "        \n",
    "        for (imo, signal), group in gaps_pd.groupby(['imo', 'signal']):\n",
    "            group = group.sort_values('gap_start')\n",
    "            \n",
    "            # Parse timestamps to datetime\n",
    "            group['gap_start'] = pd.to_datetime(group['gap_start'])\n",
    "            \n",
    "            current_start = group['gap_start'].iloc[0]\n",
    "            prev_time = current_start\n",
    "            \n",
    "            for idx, row in group.iloc[1:].iterrows():\n",
    "                curr_time = row['gap_start']\n",
    "                \n",
    "                # Wenn mehr als 15 Minuten zwischen den Zeitstempeln liegen, \n",
    "                # betrachte es als neue Lücke\n",
    "                if (curr_time - prev_time) > timedelta(minutes=15):\n",
    "                    result.append({\n",
    "                        'imo': imo,\n",
    "                        'signal': signal,\n",
    "                        'gap_start': current_start.isoformat(),\n",
    "                        'gap_end': prev_time.isoformat(),\n",
    "                        'loaddate': row['loaddate']\n",
    "                    })\n",
    "                    current_start = curr_time\n",
    "                \n",
    "                prev_time = curr_time\n",
    "            \n",
    "            # Füge die letzte Lücke hinzu\n",
    "            result.append({\n",
    "                'imo': imo,\n",
    "                'signal': signal,\n",
    "                'gap_start': current_start.isoformat(),\n",
    "                'gap_end': prev_time.isoformat(),\n",
    "                'loaddate': group['loaddate'].iloc[-1]\n",
    "            })\n",
    "        \n",
    "        return pl.DataFrame(result)\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_ships(ships: pl.DataFrame, run_timestamp: str) -> Tuple[pl.DataFrame, Dict[str, pl.DataFrame]]:\n",
    "        \"\"\"Transforms ship data and extracts nested tables\"\"\"\n",
    "        ships = ships.lazy().unnest(\"data\")\n",
    "        \n",
    "        # Extract nested tables\n",
    "        tables = {}\n",
    "        for column, dtype in ships.collect_schema().items():\n",
    "            if dtype == pl.List(pl.Struct):\n",
    "                tables[column] = (\n",
    "                    ships.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .unnest(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    .collect()\n",
    "                )\n",
    "            elif dtype == pl.List:\n",
    "                tables[column] = (\n",
    "                    ships.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    .collect()\n",
    "                )\n",
    "\n",
    "        # Keep only non-list columns in main table\n",
    "        ships = ships.select(\n",
    "            pl.exclude([col for col, dtype in ships.collect_schema().items() if dtype == pl.List])\n",
    "        ).with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        ).collect()\n",
    "\n",
    "        return ships, tables\n",
    "    \n",
    "    @staticmethod\n",
    "    def enrich_timeseries_with_friendly_names(timeseries_df: pl.DataFrame, signals_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Fügt friendly_name aus der Signaldatei zu den Timeseries-Daten hinzu\n",
    "        \"\"\"\n",
    "        if len(timeseries_df) == 0 or len(signals_df) == 0:\n",
    "            return timeseries_df\n",
    "            \n",
    "        # Extrahiere Signal-Mapping (signal -> friendly_name)\n",
    "        signal_mapping = (\n",
    "            signals_df\n",
    "            .filter(pl.col(\"friendly_name\").is_not_null())\n",
    "            .select([\"signal\", \"friendly_name\"])\n",
    "            .unique()\n",
    "        )\n",
    "        \n",
    "        # Join mit Timeseries-Daten\n",
    "        return timeseries_df.join(\n",
    "            signal_mapping,\n",
    "            on=\"signal\",\n",
    "            how=\"left\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"Main data pipeline class\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, api_key: str):\n",
    "        self.config = config\n",
    "        self.api_client = APIClient(config.base_url, api_key, timeout=config.timeout)\n",
    "        self.processor = DataProcessor()\n",
    "        self.storage = DataStorage(config)\n",
    "        \n",
    "    def process_ship(self, imo: str, run_timestamp: str) -> None:\n",
    "        \"\"\"Processes data for a single ship\"\"\"\n",
    "        try:\n",
    "            # Get and process signals\n",
    "            _, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "            if signals:\n",
    "                signals_df = pl.DataFrame(signals)\n",
    "                self.storage.write_file(\n",
    "                    signals,\n",
    "                    f\"Signals_{imo}\",\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "                signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "                self.storage.write_file(\n",
    "                    signals_transformed,\n",
    "                    f\"Signals_{imo}\",\n",
    "                    f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                    'parquet'\n",
    "                )\n",
    "\n",
    "            # Get and process timeseries\n",
    "            _, timeseries = self.api_client.get_data(f\"fleet/{imo}/timeseries\")\n",
    "            if timeseries:\n",
    "                ts_df = pl.DataFrame(timeseries)\n",
    "                self.storage.write_file(\n",
    "                    timeseries,\n",
    "                    f\"Timeseries_{imo}\",\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "                # Transformieren und Lücken extrahieren\n",
    "                ts_transformed, gaps = self.processor.transform_timeseries(ts_df, imo, run_timestamp)\n",
    "                \n",
    "                self.storage.write_file(\n",
    "                    ts_transformed,\n",
    "                    f\"Timeseries_{imo}\",\n",
    "                    f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                    'parquet'\n",
    "                )\n",
    "                \n",
    "                # Prozessiere und speichere Lücken-Daten, falls vorhanden\n",
    "                if len(gaps) > 0:\n",
    "                    processed_gaps = self.processor.process_gaps(gaps)\n",
    "                    if len(processed_gaps) > 0:\n",
    "                        self.storage.write_file(\n",
    "                            processed_gaps,\n",
    "                            f\"Gaps_{imo}\",\n",
    "                            f\"{self.config.gaps_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process ship {imo}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def run(self, mode: str = \"all\") -> str:\n",
    "        \"\"\"\n",
    "        Runs the complete data pipeline\n",
    "        \n",
    "        Args:\n",
    "            mode (str): 'all' for complete pipeline, 'timeseries' for only timeseries data,\n",
    "                        'fleet' for only fleet and signals data\n",
    "                        \n",
    "        Returns:\n",
    "            str: The run timestamp in the format 'YYYY/MM/DD/HH/MM'\n",
    "        \"\"\"\n",
    "        run_start = datetime.now(timezone.utc)\n",
    "        run_timestamp = run_start.strftime('%Y/%m/%d/%H/%M')\n",
    "        \n",
    "        try:\n",
    "            # Initialize directories\n",
    "            os.makedirs(f\"{self.config.raw_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.config.transformed_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.config.gaps_path}/{run_timestamp}\", exist_ok=True)\n",
    "            \n",
    "            # Get ship data\n",
    "            if mode in [\"all\", \"fleet\"]:\n",
    "                _, ships = self.api_client.get_data(\"fleet\")\n",
    "                if not ships:\n",
    "                    raise ValueError(\"Failed to get ship data\")\n",
    "                    \n",
    "                # Process ships\n",
    "                imo_numbers = self.processor.get_imo_numbers(ships)\n",
    "                \n",
    "                # Store raw ship data\n",
    "                self.storage.write_file(\n",
    "                    ships,\n",
    "                    'ShipData',\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "                \n",
    "                # Transform and store ship data\n",
    "                ships_df = pl.DataFrame(ships)\n",
    "                ships_transformed, tables = self.processor.transform_ships(ships_df, run_timestamp)\n",
    "                self.storage.write_file(\n",
    "                    ships_transformed,\n",
    "                    'ShipData',\n",
    "                    f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                    'parquet'\n",
    "                )\n",
    "                \n",
    "                # Process nested tables\n",
    "                for name, table in tables.items():\n",
    "                    self.storage.write_file(\n",
    "                        table,\n",
    "                        f\"ShipData_{name}\",\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                \n",
    "                # Process signals for all ships\n",
    "                for imo in imo_numbers:\n",
    "                    _, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "                    if signals:\n",
    "                        signals_df = pl.DataFrame(signals)\n",
    "                        self.storage.write_file(\n",
    "                            signals,\n",
    "                            f\"Signals_{imo}\",\n",
    "                            f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                            'json'\n",
    "                        )\n",
    "                        signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "                        self.storage.write_file(\n",
    "                            signals_transformed,\n",
    "                            f\"Signals_{imo}\",\n",
    "                            f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "            \n",
    "            # Process timeseries data\n",
    "            if mode in [\"all\", \"timeseries\"]:\n",
    "                # Get ship data if not already loaded\n",
    "                if mode == \"timeseries\":\n",
    "                    _, ships = self.api_client.get_data(\"fleet\")\n",
    "                    if not ships:\n",
    "                        raise ValueError(\"Failed to get ship data\")\n",
    "                    imo_numbers = self.processor.get_imo_numbers(ships)\n",
    "                \n",
    "                # Process individual ships in parallel\n",
    "                with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                    # Lambda function to process only timeseries\n",
    "                    def process_ship_timeseries(imo):\n",
    "                        try:\n",
    "                            # Get and process timeseries\n",
    "                            _, timeseries = self.api_client.get_data(f\"fleet/{imo}/timeseries\")\n",
    "                            if timeseries:\n",
    "                                ts_df = pl.DataFrame(timeseries)\n",
    "                                self.storage.write_file(\n",
    "                                    timeseries,\n",
    "                                    f\"Timeseries_{imo}\",\n",
    "                                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                                    'json'\n",
    "                                )\n",
    "                                # Transformieren und Lücken extrahieren\n",
    "                                ts_transformed, gaps = self.processor.transform_timeseries(ts_df, imo, run_timestamp)\n",
    "                                \n",
    "                                # Combine with historical data (last 5 days)\n",
    "                                historical_data = self.load_historical_timeseries(imo, self.config.history_days)\n",
    "                                if len(historical_data) > 0:\n",
    "                                    # Combine historical and new data\n",
    "                                    combined = pl.concat([historical_data, ts_transformed])\n",
    "                                    # Deduplicate\n",
    "                                    ts_transformed = (\n",
    "                                        combined\n",
    "                                        .sort(by=[\"loaddate\"], descending=True)\n",
    "                                        .unique(subset=[\"imo\", \"signal\", \"signal_timestamp\"], keep=\"first\")\n",
    "                                    )\n",
    "                                \n",
    "                                self.storage.write_file(\n",
    "                                    ts_transformed,\n",
    "                                    f\"Timeseries_{imo}\",\n",
    "                                    f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                                    'parquet'\n",
    "                                )\n",
    "                                \n",
    "                                # Prozessiere und speichere Lücken-Daten, falls vorhanden\n",
    "                                if len(gaps) > 0:\n",
    "                                    processed_gaps = self.processor.process_gaps(gaps)\n",
    "                                    if len(processed_gaps) > 0:\n",
    "                                        self.storage.write_file(\n",
    "                                            processed_gaps,\n",
    "                                            f\"Gaps_{imo}\",\n",
    "                                            f\"{self.config.gaps_path}/{run_timestamp}\",\n",
    "                                            'parquet'\n",
    "                                        )\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Failed to process timeseries for ship {imo}: {str(e)}\")\n",
    "                    \n",
    "                    # Execute in parallel\n",
    "                    executor.map(process_ship_timeseries, imo_numbers)\n",
    "            \n",
    "            # Clean up old data\n",
    "            self.storage.cleanup_old_data(self.config.raw_path, self.config.days_to_keep)\n",
    "            self.storage.cleanup_old_data(self.config.transformed_path, self.config.days_to_keep)\n",
    "            self.storage.cleanup_old_data(self.config.gaps_path, self.config.days_to_keep)\n",
    "            \n",
    "            logger.info(f\"Pipeline run completed successfully in {datetime.now(timezone.utc) - run_start}\")\n",
    "            return run_timestamp\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            runtime = datetime.now(timezone.utc) - run_start\n",
    "            logger.info(f\"Pipeline completed in {runtime}\")\n",
    "            \n",
    "    def run_timeseries_only(self) -> str:\n",
    "        \"\"\"Convenience method to run only the timeseries part of the pipeline\"\"\"\n",
    "        return self.run(mode=\"timeseries\")\n",
    "        \n",
    "    def run_fleet_only(self) -> str:\n",
    "        \"\"\"Convenience method to run only the fleet part of the pipeline\"\"\"\n",
    "        return self.run(mode=\"fleet\")\n",
    "    \n",
    "    def load_historical_timeseries(self, imo: str, days: int = 5) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Lädt historische Timeseries-Daten für ein Schiff aus den letzten n Tagen\n",
    "        \"\"\"\n",
    "        try:\n",
    "            today = datetime.now()\n",
    "            \n",
    "            # Erstelle eine Liste von Pfaden für die letzten n Tage\n",
    "            paths = []\n",
    "            for i in range(days):\n",
    "                check_date = today - timedelta(days=i)\n",
    "                date_path = check_date.strftime('%Y/%m/%d')\n",
    "                \n",
    "                # Alle Unterordner des Tages durchsuchen (Stunden/Minuten)\n",
    "                full_path = Path(f\"{self.config.transformed_path}/{date_path}\")\n",
    "                \n",
    "                if full_path.exists():\n",
    "                    # Finde die letzten Runs des Tages\n",
    "                    for hour_dir in sorted(full_path.glob(\"*\"), reverse=True):\n",
    "                        if hour_dir.is_dir():\n",
    "                            for minute_dir in sorted(hour_dir.glob(\"*\"), reverse=True):\n",
    "                                if minute_dir.is_dir():\n",
    "                                    file_path = minute_dir / f\"Timeseries_{imo}.parquet\"\n",
    "                                    if file_path.exists():\n",
    "                                        paths.append(str(file_path))\n",
    "                                        # Nimm nur den letzten Run des Tages\n",
    "                                        break\n",
    "                            # Nimm nur die letzte Stunde des Tages\n",
    "                            break\n",
    "            \n",
    "            # Load and combine data\n",
    "            dfs = []\n",
    "            for path in paths:\n",
    "                try:\n",
    "                    df = pl.read_parquet(path)\n",
    "                    if len(df) > 0:\n",
    "                        dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to read {path}: {str(e)}\")\n",
    "            \n",
    "            if not dfs:\n",
    "                logger.info(f\"No historical timeseries data found for ship {imo}\")\n",
    "                return pl.DataFrame()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load historical timeseries for ship {imo}: {str(e)}\")\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "    def get_all_signals(self, run_timestamp: str) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Sammelt alle Signaldefinitionen aus den aktuellen Daten\n",
    "        \"\"\"\n",
    "        try:\n",
    "            signals_path = Path(f\"{self.config.transformed_path}/{run_timestamp}\")\n",
    "            all_signals = []\n",
    "            \n",
    "            for file in signals_path.glob(\"Signals_*.parquet\"):\n",
    "                try:\n",
    "                    df = pl.read_parquet(file)\n",
    "                    if len(df) > 0:\n",
    "                        # Extrahiere nur die relevanten Spalten für das Mapping\n",
    "                        signals_mapping = df.select([\n",
    "                            \"signal\", \"friendly_name\", \"unit\", \"object_code\", \"name_code\", \"group_name\", \"sub_group\"\n",
    "                        ])\n",
    "                        all_signals.append(signals_mapping)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to read {file}: {str(e)}\")\n",
    "            \n",
    "            if not all_signals:\n",
    "                logger.warning(\"No signal definitions found\")\n",
    "                return pl.DataFrame()\n",
    "                \n",
    "            # Combine all signal definitions and deduplicate\n",
    "            return pl.concat(all_signals).unique(subset=[\"signal\"], keep=\"first\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to collect signal definitions: {str(e)}\")\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "    def process_and_store_to_db(self, engine: sa.Engine, run_timestamp: str) -> None:\n",
    "        \"\"\"\n",
    "        Processes data and stores it to the SQL database\n",
    "        \n",
    "        This method:\n",
    "        1. Loads timeseries data from the current run\n",
    "        2. Enriches it with friendly names\n",
    "        3. Writes the combined data to the SQL database using the specialized method\n",
    "        4. Writes gaps data to a separate table\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Load timeseries data from current run\n",
    "            transformed_path = Path(f\"{self.config.transformed_path}/{run_timestamp}\")\n",
    "            \n",
    "            # Find all timeseries parquet files\n",
    "            timeseries_files = list(transformed_path.glob(\"Timeseries_*.parquet\"))\n",
    "            if not timeseries_files:\n",
    "                logger.warning(f\"No timeseries files found in {run_timestamp}\")\n",
    "                return\n",
    "            \n",
    "            # 2. Load and combine all timeseries data\n",
    "            ts_dfs = []\n",
    "            for file in timeseries_files:\n",
    "                try:\n",
    "                    df = pl.read_parquet(file)\n",
    "                    if len(df) > 0:\n",
    "                        ts_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to read {file}: {str(e)}\")\n",
    "            \n",
    "            if not ts_dfs:\n",
    "                logger.warning(\"No data loaded from timeseries files\")\n",
    "                return\n",
    "            \n",
    "            combined_ts = pl.concat(ts_dfs)\n",
    "            logger.info(f\"Combined {len(ts_dfs)} timeseries files with {len(combined_ts)} rows total\")\n",
    "            \n",
    "            # 3. Get all signal definitions for enrichment\n",
    "            all_signals = self.get_all_signals(run_timestamp)\n",
    "            \n",
    "            # 4. Enrich timeseries data with friendly names\n",
    "            if len(all_signals) > 0:\n",
    "                combined_ts = self.processor.enrich_timeseries_with_friendly_names(combined_ts, all_signals)\n",
    "                logger.info(\"Enriched timeseries data with friendly names\")\n",
    "            \n",
    "            # 5. Write to database using the specialized method\n",
    "            if len(combined_ts) > 0:\n",
    "                self.storage.write_ts_to_msdb(combined_ts, engine)\n",
    "                logger.info(\"Successfully processed and stored timeseries data to database\")\n",
    "            \n",
    "            # 6. Process and store gaps data\n",
    "            gaps_path = Path(f\"{self.config.gaps_path}/{run_timestamp}\")\n",
    "            if gaps_path.exists():\n",
    "                gaps_files = list(gaps_path.glob(\"Gaps_*.parquet\"))\n",
    "                gaps_dfs = []\n",
    "                \n",
    "                for file in gaps_files:\n",
    "                    try:\n",
    "                        df = pl.read_parquet(file)\n",
    "                        if len(df) > 0:\n",
    "                            gaps_dfs.append(df)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to read gaps file {file}: {str(e)}\")\n",
    "                \n",
    "                if gaps_dfs:\n",
    "                    combined_gaps = pl.concat(gaps_dfs)\n",
    "                    logger.info(f\"Combined {len(gaps_dfs)} gaps files with {len(combined_gaps)} rows total\")\n",
    "                    \n",
    "                    # Write gaps data to a separate table\n",
    "                    self.storage.write_to_db(combined_gaps, engine, \"TimeSeries_Gaps\", if_exists=\"append\")\n",
    "                    logger.info(\"Successfully stored gaps data to database\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process and store data to database: {str(e)}\")\n",
    "            raise\n",
    "                \n",
    "            # Combine all dataframes and keep only the latest value for each combination\n",
    "            combined = pl.concat(dfs)\n",
    "            \n",
    "            # Dedupliziere die Daten - behalte nur den neuesten Eintrag für jede Kombination von imo, signal und signal_timestamp\n",
    "            deduplicated = (\n",
    "                combined\n",
    "                .sort(by=[\"loaddate\"], descending=True)\n",
    "                .unique(subset=[\"imo\", \"signal\", \"signal_timestamp\"], keep=\"first\")\n",
    "            )\n",
    "            \n",
    "            return deduplicated\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load historical timeseries for ship {imo}: {str(e)}\")\n",
    "            return pl.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Starten\n",
    "Hauptfunktionsaufruf für die Pipeline\n",
    "\n",
    "mode (str): 'all', 'timeseries', oder 'fleet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = \"all\"\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if environment variables are set\n",
    "api_key = os.getenv('HOPPE_API_KEY')\n",
    "if not api_key:\n",
    "    logger.error(\"HOPPE_API_KEY environment variable not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "config = Config(\n",
    "    base_url=os.getenv('HOPPE_BASE_URL', \"https://api.hoppe-sts.com/\"),\n",
    "    raw_path=os.getenv('RAW_PATH', \"./data/raw_data\"),\n",
    "    transformed_path=os.getenv('TRANSFORMED_PATH', \"./data/transformed_data\"),\n",
    "    gaps_path=os.getenv('GAPS_PATH', \"./data/gaps_data\"),\n",
    "    max_workers=int(os.getenv('MAX_WORKERS', \"8\")),\n",
    "    retry_attempts=int(os.getenv('RETRY_ATTEMPTS', \"5\")),\n",
    "    timeout=int(os.getenv('TIMEOUT', \"45\")),\n",
    "    days_to_keep=int(os.getenv('DAYS_TO_KEEP', \"90\")),\n",
    "    history_days=int(os.getenv('HISTORY_DAYS', \"5\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 23:46:03,661 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/ShipData.json\n",
      "2025-03-10 23:46:03,862 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/ShipData.parquet\n",
      "2025-03-10 23:46:03,882 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/ShipData_main_engines.parquet\n",
      "2025-03-10 23:46:03,910 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/ShipData_aux_engines.parquet\n",
      "2025-03-10 23:46:10,415 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9400071.json\n",
      "2025-03-10 23:46:10,465 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9400071.parquet\n",
      "2025-03-10 23:46:10,676 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9725512.json\n",
      "2025-03-10 23:46:10,698 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9725512.parquet\n",
      "2025-03-10 23:46:10,898 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9725524.json\n",
      "2025-03-10 23:46:10,922 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9725524.parquet\n",
      "2025-03-10 23:46:11,029 - hoppe_etl_pipeline - ERROR - API request failed: 404 Client Error: Not Found for url: https://api.hoppe-sts.com/fleet/9778399/signals\n",
      "2025-03-10 23:46:11,248 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9696826.json\n",
      "2025-03-10 23:46:11,278 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9696826.parquet\n",
      "2025-03-10 23:46:11,470 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9693290.json\n",
      "2025-03-10 23:46:11,500 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9693290.parquet\n",
      "2025-03-10 23:46:11,757 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9306184.json\n",
      "2025-03-10 23:46:11,782 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9306184.parquet\n",
      "2025-03-10 23:46:12,015 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9693331.json\n",
      "2025-03-10 23:46:12,052 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9693331.parquet\n",
      "2025-03-10 23:46:12,297 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9306287.json\n",
      "2025-03-10 23:46:12,334 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9306287.parquet\n",
      "2025-03-10 23:46:12,579 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9306160.json\n",
      "2025-03-10 23:46:12,613 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9306160.parquet\n",
      "2025-03-10 23:46:12,727 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9696838.json\n",
      "2025-03-10 23:46:12,779 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9696838.parquet\n",
      "2025-03-10 23:46:13,244 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9693329.json\n",
      "2025-03-10 23:46:13,313 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9693329.parquet\n",
      "2025-03-10 23:46:13,442 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9395525.json\n",
      "2025-03-10 23:46:13,478 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9395525.parquet\n",
      "2025-03-10 23:46:13,788 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Signals_9726566.json\n",
      "2025-03-10 23:46:13,840 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Signals_9726566.parquet\n",
      "2025-03-10 23:46:14,864 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9306184.json\n",
      "2025-03-10 23:46:15,015 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9306184\n",
      "2025-03-10 23:46:15,032 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9306184.parquet\n",
      "2025-03-10 23:46:15,326 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9306184.parquet\n",
      "2025-03-10 23:46:18,548 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9306287.json\n",
      "2025-03-10 23:46:18,772 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9306287\n",
      "2025-03-10 23:46:18,810 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9306287.parquet\n",
      "2025-03-10 23:46:19,392 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9306287.parquet\n",
      "2025-03-10 23:46:19,638 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9693290.json\n",
      "2025-03-10 23:46:20,090 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9693290\n",
      "2025-03-10 23:46:20,141 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9693290.parquet\n",
      "2025-03-10 23:46:22,082 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9693290.parquet\n",
      "2025-03-10 23:46:22,868 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9696838.json\n",
      "2025-03-10 23:46:22,912 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9306160.json\n",
      "2025-03-10 23:46:22,968 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9696826.json\n",
      "2025-03-10 23:46:23,002 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9725512.json\n",
      "2025-03-10 23:46:23,048 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9696838\n",
      "2025-03-10 23:46:23,098 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9696838.parquet\n",
      "2025-03-10 23:46:23,260 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9306160\n",
      "2025-03-10 23:46:23,290 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9696838.parquet\n",
      "2025-03-10 23:46:23,298 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9696826\n",
      "2025-03-10 23:46:23,299 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9725512\n",
      "2025-03-10 23:46:23,300 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9306160.parquet\n",
      "2025-03-10 23:46:23,448 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9725512.parquet\n",
      "2025-03-10 23:46:23,464 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9696826.parquet\n",
      "2025-03-10 23:46:23,570 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9306160.parquet\n",
      "2025-03-10 23:46:24,054 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9725512.parquet\n",
      "2025-03-10 23:46:24,722 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9696826.parquet\n",
      "2025-03-10 23:46:31,872 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9725524.json\n",
      "2025-03-10 23:46:39,973 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9725524\n",
      "2025-03-10 23:46:42,013 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9725524.parquet\n",
      "2025-03-10 23:47:15,967 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9726566.json\n",
      "2025-03-10 23:47:17,583 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9726566\n",
      "2025-03-10 23:47:17,597 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9725524.parquet\n",
      "2025-03-10 23:47:17,729 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9726566.parquet\n",
      "2025-03-10 23:47:21,661 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9726566.parquet\n",
      "2025-03-10 23:47:25,215 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/10/22/45/Timeseries_9693329.json\n",
      "2025-03-10 23:47:28,023 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9693329\n",
      "2025-03-10 23:47:28,111 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/10/22/45/Timeseries_9693329.parquet\n",
      "2025-03-10 23:47:36,277 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/10/22/45/Gaps_9693329.parquet\n",
      "2025-03-10 23:47:36,406 - hoppe_etl_pipeline - INFO - Cleanup of data older than 2024-12-10 completed\n",
      "2025-03-10 23:47:36,411 - hoppe_etl_pipeline - INFO - Cleanup of data older than 2024-12-10 completed\n",
      "2025-03-10 23:47:36,416 - hoppe_etl_pipeline - INFO - Cleanup of data older than 2024-12-10 completed\n",
      "2025-03-10 23:47:36,419 - hoppe_etl_pipeline - INFO - Pipeline run completed successfully in 0:01:39.304465\n",
      "2025-03-10 23:47:36,420 - hoppe_etl_pipeline - INFO - Pipeline completed in 0:01:39.305476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and run pipeline\n",
    "pipeline = Pipeline(config, api_key)\n",
    "run_timestamp = pipeline.run(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If database connection is configured, store to database\n",
    "db_connection_string = os.getenv('MSSQL_CONNECTION_STRING')\n",
    "if db_connection_string:\n",
    "    try:\n",
    "        engine = sa.create_engine(db_connection_string)\n",
    "        logger.info(\"Database connection established\")\n",
    "        \n",
    "        # Process and store data to database\n",
    "        pipeline.process_and_store_to_db(engine, run_timestamp)\n",
    "        logger.info(\"Database integration completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database integration failed: {str(e)}\")\n",
    "else:\n",
    "    logger.warning(\"MSSQL_CONNECTION_STRING not set, skipping database integration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_timeseries_files(base_path: str, max_days: int = None) -> list:\n",
    "    \"\"\"\n",
    "    Findet alle Timeseries-Parquet-Dateien im angegebenen Verzeichnis\n",
    "    \n",
    "    Args:\n",
    "        base_path: Basispfad zum Durchsuchen\n",
    "        max_days: Optional, maximale Anzahl der zu berücksichtigenden Tage (neueste zuerst)\n",
    "        \n",
    "    Returns:\n",
    "        Liste von Pfaden zu Timeseries-Dateien\n",
    "    \"\"\"\n",
    "    logger.info(f\"Durchsuche Verzeichnis {base_path} nach Timeseries-Dateien\")\n",
    "    \n",
    "    base_dir = Path(base_path)\n",
    "    if not base_dir.exists() or not base_dir.is_dir():\n",
    "        logger.error(f\"Verzeichnis {base_path} existiert nicht oder ist kein Verzeichnis\")\n",
    "        return []\n",
    "    \n",
    "    # Finde alle Jahr-Verzeichnisse, sortiere absteigend für neueste zuerst\n",
    "    years = sorted([d for d in base_dir.glob(\"*\") if d.is_dir() and d.name.isdigit()], \n",
    "                   key=lambda x: x.name, reverse=True)\n",
    "    \n",
    "    all_files = []\n",
    "    days_processed = 0\n",
    "    \n",
    "    # Durchlaufe Jahre, Monate, Tage\n",
    "    for year_dir in years:\n",
    "        months = sorted([d for d in year_dir.glob(\"*\") if d.is_dir() and d.name.isdigit()], \n",
    "                        key=lambda x: x.name, reverse=True)\n",
    "        \n",
    "        for month_dir in months:\n",
    "            days = sorted([d for d in month_dir.glob(\"*\") if d.is_dir() and d.name.isdigit()], \n",
    "                          key=lambda x: x.name, reverse=True)\n",
    "            \n",
    "            for day_dir in days:\n",
    "                if max_days is not None and days_processed >= max_days:\n",
    "                    break\n",
    "                \n",
    "                # Finde alle Stunden und Minuten für diesen Tag\n",
    "                ts_files = []\n",
    "                hour_dirs = sorted([d for d in day_dir.glob(\"*\") if d.is_dir()], \n",
    "                                   key=lambda x: x.name, reverse=True)\n",
    "                \n",
    "                for hour_dir in hour_dirs:\n",
    "                    minute_dirs = sorted([d for d in hour_dir.glob(\"*\") if d.is_dir()], \n",
    "                                         key=lambda x: x.name, reverse=True)\n",
    "                    \n",
    "                    for minute_dir in minute_dirs:\n",
    "                        # Finde alle Timeseries-Dateien in diesem Minuten-Verzeichnis\n",
    "                        files = list(minute_dir.glob(\"Timeseries_*.parquet\"))\n",
    "                        if files:\n",
    "                            ts_files.extend(files)\n",
    "                            # Nur der letzte Run des Tages\n",
    "                            break\n",
    "                    \n",
    "                    if ts_files:\n",
    "                        # Nur die letzte Stunde des Tages\n",
    "                        break\n",
    "                \n",
    "                all_files.extend(ts_files)\n",
    "                days_processed += 1\n",
    "                \n",
    "                if max_days is not None and days_processed >= max_days:\n",
    "                    logger.info(f\"Maximale Anzahl von Tagen ({max_days}) erreicht\")\n",
    "                    break\n",
    "            \n",
    "            if max_days is not None and days_processed >= max_days:\n",
    "                break\n",
    "        \n",
    "        if max_days is not None and days_processed >= max_days:\n",
    "            break\n",
    "    \n",
    "    logger.info(f\"Gefunden: {len(all_files)} Timeseries-Dateien aus {days_processed} Tagen\")\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_timeseries(file_paths: list) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lädt und kombiniert alle Timeseries-Dateien\n",
    "    \n",
    "    Args:\n",
    "        file_paths: Liste der zu ladenden Dateipfade\n",
    "        \n",
    "    Returns:\n",
    "        Polars DataFrame mit kombinierten Daten\n",
    "    \"\"\"\n",
    "    logger.info(f\"Lade {len(file_paths)} Timeseries-Dateien\")\n",
    "    \n",
    "    all_dfs = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            if idx % 10 == 0:\n",
    "                logger.info(f\"Verarbeite Datei {idx + 1} von {len(file_paths)}\")\n",
    "            \n",
    "            df = pl.read_parquet(file_path)\n",
    "            rows = len(df)\n",
    "            total_rows += rows\n",
    "            \n",
    "            if rows > 0:\n",
    "                all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Lesen von {file_path}: {str(e)}\")\n",
    "    \n",
    "    if not all_dfs:\n",
    "        logger.warning(\"Keine Daten geladen\")\n",
    "        return pl.DataFrame()\n",
    "    \n",
    "    # Kombiniere alle DataFrames\n",
    "    combined_df = pl.concat(all_dfs)\n",
    "    logger.info(f\"Insgesamt {total_rows} Zeilen geladen, {len(combined_df)} vor Deduplizierung\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_deduplicate(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Entfernt Nullwerte und Duplikate aus dem DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Zu bereinigender DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Bereinigter DataFrame\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Entferne Zeilen mit Nullwerten\n",
    "    no_nulls = df.filter(pl.col(\"signal_value\").is_not_null())\n",
    "    logger.info(f\"Zeilen nach Entfernen von Nullwerten: {len(no_nulls)}\")\n",
    "    \n",
    "    # Sortiere nach Ladedatum (absteigend) und entferne Duplikate\n",
    "    deduplicated = (\n",
    "        no_nulls\n",
    "        .sort(by=[\"loaddate\"], descending=True)\n",
    "        .unique(subset=[\"imo\", \"signal\", \"signal_timestamp\"], keep=\"first\")\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Zeilen nach Deduplizierung: {len(deduplicated)}\")\n",
    "    return deduplicated\n",
    "\n",
    "def pivot_timeseries(df: pl.DataFrame, max_signals: int = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Pivotisiert die Timeseries-Daten: Signal wird zu Spalten\n",
    "    \n",
    "    Args:\n",
    "        df: Zu pivotisierender DataFrame\n",
    "        max_signals: Optional, maximale Anzahl von Signalen zu verarbeiten\n",
    "                    (für Speicher- und Leistungsoptimierung)\n",
    "        \n",
    "    Returns:\n",
    "        Pivotisierter DataFrame\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    logger.info(\"Pivotisiere Daten\")\n",
    "    \n",
    "    # Identifiziere eindeutige Signale\n",
    "    unique_signals = df.select(\"signal\").unique()\n",
    "    signal_count = len(unique_signals)\n",
    "    \n",
    "    logger.info(f\"Gefunden: {signal_count} eindeutige Signale\")\n",
    "    \n",
    "    if max_signals is not None and signal_count > max_signals:\n",
    "        logger.warning(f\"Zu viele Signale ({signal_count}), begrenze auf {max_signals}\")\n",
    "        \n",
    "        # Verwende die häufigsten Signale, wenn zu viele vorhanden sind\n",
    "        signal_counts = df.group_by(\"signal\").count().sort(by=\"count\", descending=True)\n",
    "        top_signals = signal_counts.head(max_signals).select(\"signal\")\n",
    "        \n",
    "        # Filtere DataFrame auf die häufigsten Signale\n",
    "        df = df.join(top_signals, on=\"signal\")\n",
    "        logger.info(f\"DataFrame auf {len(df)} Zeilen mit Top-{max_signals} Signalen reduziert\")\n",
    "    \n",
    "    try:\n",
    "        # Pivotisiere den DataFrame\n",
    "        pivoted = df.pivot(\n",
    "            values=\"signal_value\",\n",
    "            index=[\"imo\", \"signal_timestamp\", \"loaddate\"],\n",
    "            columns=\"signal\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Pivotisierter DataFrame hat {len(pivoted)} Zeilen und {len(pivoted.columns)} Spalten\")\n",
    "        return pivoted\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fehler bei der Pivotisierung: {str(e)}\")\n",
    "        \n",
    "        # Alternative Methode mit expliziter Speicherverwaltung\n",
    "        logger.info(\"Versuche alternative Pivotisierungsmethode...\")\n",
    "        \n",
    "        # Gruppieren nach imo, timestamp, loaddate\n",
    "        pivoted_dfs = []\n",
    "        \n",
    "        for sig in df.select(\"signal\").unique().to_series():\n",
    "            try:\n",
    "                # Filtern für dieses Signal\n",
    "                signal_df = df.filter(pl.col(\"signal\") == sig)\n",
    "                \n",
    "                # Umbenennen der signal_value-Spalte zum Signalnamen\n",
    "                renamed = signal_df.select(\n",
    "                    [\"imo\", \"signal_timestamp\", \"loaddate\", \n",
    "                     pl.col(\"signal_value\").alias(sig)]\n",
    "                )\n",
    "                \n",
    "                pivoted_dfs.append(renamed)\n",
    "            except Exception as sub_e:\n",
    "                logger.error(f\"Fehler bei der Verarbeitung von Signal {sig}: {str(sub_e)}\")\n",
    "        \n",
    "        if not pivoted_dfs:\n",
    "            logger.error(\"Keine Daten nach alternativer Pivotisierung\")\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "        # Join all signal dataframes\n",
    "        result = pivoted_dfs[0]\n",
    "        for idx, signal_df in enumerate(pivoted_dfs[1:], 1):\n",
    "            if idx % 10 == 0:\n",
    "                logger.info(f\"Verbinde Signal {idx} von {len(pivoted_dfs) - 1}\")\n",
    "                \n",
    "            try:\n",
    "                result = result.join(\n",
    "                    signal_df, \n",
    "                    on=[\"imo\", \"signal_timestamp\", \"loaddate\"], \n",
    "                    how=\"outer\"\n",
    "                )\n",
    "            except Exception as join_e:\n",
    "                logger.error(f\"Fehler beim Verbinden von Signal {idx}: {str(join_e)}\")\n",
    "        \n",
    "        logger.info(f\"Alternative Pivotisierung: {len(result)} Zeilen, {len(result.columns)} Spalten\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 23:53:13,252 - hoppe_etl_pipeline - INFO - Starte Verarbeitung um 2025-03-10 23:53:13.252570\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "logger.info(f\"Starte Verarbeitung um {start_time}\")\n",
    "\n",
    "base_path = \"./data/transformed_data\"\n",
    "output = \"./pivoted_timeseries.parquet\"\n",
    "max_days = None\n",
    "max_signals = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 23:53:19,437 - hoppe_etl_pipeline - INFO - Durchsuche Verzeichnis ./data/transformed_data nach Timeseries-Dateien\n",
      "2025-03-10 23:53:19,454 - hoppe_etl_pipeline - INFO - Gefunden: 10 Timeseries-Dateien aus 1 Tagen\n",
      "2025-03-10 23:53:19,455 - hoppe_etl_pipeline - INFO - Lade 10 Timeseries-Dateien\n",
      "2025-03-10 23:53:19,456 - hoppe_etl_pipeline - INFO - Verarbeite Datei 1 von 10\n",
      "2025-03-10 23:53:19,957 - hoppe_etl_pipeline - INFO - Insgesamt 1235318 Zeilen geladen, 1235318 vor Deduplizierung\n",
      "2025-03-10 23:53:19,959 - hoppe_etl_pipeline - INFO - Zeilen nach Entfernen von Nullwerten: 1235318\n",
      "2025-03-10 23:53:20,368 - hoppe_etl_pipeline - INFO - Zeilen nach Deduplizierung: 1235318\n",
      "2025-03-10 23:53:20,370 - hoppe_etl_pipeline - INFO - Pivotisiere Daten\n",
      "2025-03-10 23:53:20,434 - hoppe_etl_pipeline - INFO - Gefunden: 218 eindeutige Signale\n",
      "C:\\Users\\Privat\\AppData\\Local\\Temp\\ipykernel_29988\\2044662848.py:64: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
      "  pivoted = df.pivot(\n",
      "2025-03-10 23:53:21,006 - hoppe_etl_pipeline - INFO - Pivotisierter DataFrame hat 11556 Zeilen und 221 Spalten\n"
     ]
    }
   ],
   "source": [
    "# Finde Dateien\n",
    "ts_files = find_timeseries_files(base_path, max_days)\n",
    "\n",
    "if not ts_files:\n",
    "    logger.error(\"Keine Timeseries-Dateien gefunden\")\n",
    "\n",
    "# Lade und kombiniere Daten\n",
    "combined_df = load_and_combine_timeseries(ts_files)\n",
    "\n",
    "if len(combined_df) == 0:\n",
    "    logger.error(\"Keine Daten geladen\")\n",
    "\n",
    "# Bereinige Daten\n",
    "clean_df = clean_and_deduplicate(combined_df)\n",
    "\n",
    "if len(clean_df) == 0:\n",
    "    logger.error(\"Keine Daten nach Bereinigung\")\n",
    "\n",
    "# Pivotisiere Daten\n",
    "pivoted_df = pivot_timeseries(clean_df, max_signals)\n",
    "\n",
    "if len(pivoted_df) == 0:\n",
    "    logger.error(\"Keine Daten nach Pivotisierung\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 23:54:20,425 - hoppe_etl_pipeline - INFO - Pivotisierte Daten gespeichert nach ./pivoted_timeseries.parquet\n",
      "2025-03-10 23:54:20,471 - hoppe_etl_pipeline - INFO - Statistiken der pivotisierten Daten:\n",
      "2025-03-10 23:54:20,472 - hoppe_etl_pipeline - INFO -   Anzahl der Zeilen: 11556\n",
      "2025-03-10 23:54:20,475 - hoppe_etl_pipeline - INFO -   Anzahl der Spalten: 221\n",
      "2025-03-10 23:54:20,478 - hoppe_etl_pipeline - INFO -   Eindeutige IMOs: 10\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Python311\\Lib\\logging\\__init__.py\", line 1113, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"D:\\Python311\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode characters in position 81-108: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"D:\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"D:\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"D:\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3047, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3102, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3489, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Google Drive\\Meine Ablage\\01_Uni\\03_Data Science\\03_Semester\\DAU_Datenanalyse in Unternehmen\\Hoppe-Pipeline\\hoppe-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3549, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Privat\\AppData\\Local\\Temp\\ipykernel_29988\\10630565.py\", line 21, in <module>\n",
      "    logger.info(f\"  {key}: {value}\")\n",
      "Message: '  Zeitraum: shape: (1, 1)\\n┌──────────────────────────┐\\n│ signal_timestamp         │\\n│ ---                      │\\n│ str                      │\\n╞══════════════════════════╡\\n│ 2025-03-09T22:46:30.000Z │\\n└──────────────────────────┘ bis shape: (1, 1)\\n┌──────────────────────────┐\\n│ signal_timestamp         │\\n│ ---                      │\\n│ str                      │\\n╞══════════════════════════╡\\n│ 2025-03-10T22:45:00.000Z │\\n└──────────────────────────┘'\n",
      "Arguments: ()\n",
      "2025-03-10 23:54:20,491 - hoppe_etl_pipeline - INFO -   Zeitraum: shape: (1, 1)\n",
      "┌──────────────────────────┐\n",
      "│ signal_timestamp         │\n",
      "│ ---                      │\n",
      "│ str                      │\n",
      "╞══════════════════════════╡\n",
      "│ 2025-03-09T22:46:30.000Z │\n",
      "└──────────────────────────┘ bis shape: (1, 1)\n",
      "┌──────────────────────────┐\n",
      "│ signal_timestamp         │\n",
      "│ ---                      │\n",
      "│ str                      │\n",
      "╞══════════════════════════╡\n",
      "│ 2025-03-10T22:45:00.000Z │\n",
      "└──────────────────────────┘\n",
      "2025-03-10 23:54:20,836 - hoppe_etl_pipeline - INFO - Verarbeitung abgeschlossen nach 0:01:07.583863, Endzeit: 2025-03-10 23:54:20.836433\n"
     ]
    }
   ],
   "source": [
    "# Speichere Ergebnis\n",
    "try:\n",
    "    # Erstelle das Ausgabeverzeichnis, falls es nicht existiert\n",
    "    output_dir = os.path.dirname(output)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    pivoted_df.write_parquet(output, compression=\"snappy\")\n",
    "    logger.info(f\"Pivotisierte Daten gespeichert nach {output}\")\n",
    "    \n",
    "    # Berechne Statistiken\n",
    "    stats = {\n",
    "        \"Anzahl der Zeilen\": len(pivoted_df),\n",
    "        \"Anzahl der Spalten\": len(pivoted_df.columns),\n",
    "        \"Eindeutige IMOs\": len(pivoted_df.select(\"imo\").unique()),\n",
    "        \"Zeitraum\": f\"{pivoted_df.select('signal_timestamp').min()[0]} bis {pivoted_df.select('signal_timestamp').max()[0]}\"\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Statistiken der pivotisierten Daten:\")\n",
    "    for key, value in stats.items():\n",
    "        logger.info(f\"  {key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Fehler beim Speichern der Daten: {str(e)}\")\n",
    "\n",
    "# Beende Zeitmessung\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "logger.info(f\"Verarbeitung abgeschlossen nach {duration}, Endzeit: {end_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoppe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
