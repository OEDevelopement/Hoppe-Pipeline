{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL-Pipeline zum Import von Sensor-Daten\n",
    "\n",
    "## Aufgabenstellung\n",
    "Firma: Peter Döhle Schiffahrts-KG\n",
    "\n",
    "Daten: Signaldaten von 12+ Schiffen\n",
    "- Treibstoff Emissionen, Tankfüllstand, Geschwindigkeit, Gewicht,...\n",
    "- im 15 min Takt via Rest API verfügbar\n",
    "\n",
    "ursprüngliches Ziel: Skalierbare ETL-Strecke für den poc von Microsoft Fabric\n",
    "\n",
    "geändertes Ziel: Vergleich verschiedener ETL-Ansätze anhand festgelegter Kriterien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-Endpoints\n",
    "\n",
    "Response: JSON-Format\n",
    "- /fleet : Alle Schiffe inkl. technischer Informationen\n",
    "- /fleet/{imo}/signals: Auflistung der für das Schiff verfügbaren Sensordaten inkl. Erklärung\n",
    "- /fleet/{imo}/timeseries?{optionaleParameter}: Zeitreihe der Sensordaten des Schiffes in 5-min Abständen\n",
    "\n",
    "Zielbild: \n",
    "Zeitreihe mit verständlichen Signalnamen pivotisiert und vollständig historisiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herausforderungen\n",
    "\n",
    "- Teils starke Verschachtelung\n",
    "- Schlechte Verbindung des Schiffes \n",
    "    - Fehlende Daten als NULL gespeichert\n",
    "- Fehlende Daten teils nachträglich überschrieben \n",
    "    - überlappendes Laden notwendig\n",
    "- Veränderung der Anzahl der Signale \n",
    "- Pivotierung führt zu sehr vielen Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "```bash\n",
    "# Virtuelle Umgebung erstellen\n",
    "python -m venv hoppe-env\n",
    "\n",
    "# Umgebung aktivieren\n",
    "source hoppe-env/Scripts/activate\n",
    "\n",
    "# Pakete installiereb\n",
    "pip install -r requirements.txt --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der relevanten packages\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy.sql import text\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "from typing import Dict, Optional, Tuple, List, Union\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptordner erstellen\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Unterordner erstellen\n",
    "for sub_dir in ['raw_data', 'transformed_data', 'gaps_data']:\n",
    "    os.makedirs(os.path.join('../data', sub_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Konfiguriert das Logging\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"logs/pipeline.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(\"hoppe_etl_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for the data pipeline\"\"\"\n",
    "    base_url: str = \"https://api.hoppe-sts.com/\"\n",
    "    raw_path: str = \"./data/raw_data\"\n",
    "    transformed_path: str = \"./data/transformed_data\"\n",
    "    gaps_path: str = \"./data/gaps_data\"  # Neuer Pfad für Null-Wert-Lücken\n",
    "    batch_size: int = 1000\n",
    "    max_workers: int = 8  # Erhöhte Worker für bessere Parallelisierung\n",
    "    retry_attempts: int = 5  # Erhöhte Retry-Versuche\n",
    "    timeout: int = 60  # Erhöhter Timeout\n",
    "    max_retry_timeout: int = 300  # Maximaler Gesamttimeout (z.B. 5 Minuten)\n",
    "    days_to_keep: int = 90  # Daten werden für 90 Tage aufbewahrt\n",
    "    history_days: int = 5  # Letzten 5 Tage für Historie laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIClient:\n",
    "    \"\"\"Handles API communication with retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_key: str, timeout: int = 30, verify_ssl: bool = True):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.verify_ssl = verify_ssl\n",
    "        self.session = self._create_session(timeout)\n",
    "\n",
    "    def _create_session(self, timeout: int) -> requests.Session:\n",
    "        \"\"\"Creates requests session with retry logic\"\"\"\n",
    "        session = requests.Session()\n",
    "        session.verify = self.verify_ssl  # SSL-Verifizierung konfigurierbar\n",
    "        \n",
    "        # Unterdrücke SSL-Warnungen, wenn Verifizierung deaktiviert ist\n",
    "        if not self.verify_ssl:\n",
    "            import urllib3\n",
    "            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "            \n",
    "        retry_strategy = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session.headers.update({\n",
    "            \"Authorization\": f\"ApiKey {self.api_key}\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        })\n",
    "        session.timeout = timeout\n",
    "        return session\n",
    "    \n",
    "    def get_data(self, relative_url: str, params: Optional[Dict] = None) -> Tuple[Optional[requests.Response], Optional[dict]]:\n",
    "        \"\"\"\n",
    "        Fetches data from API with error handling\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (Response, JSON data)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            request_url = f\"{self.base_url}{relative_url}\"\n",
    "            response = self.session.get(request_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response, response.json()\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            logger.error(f\"SSL-Zertifikatsfehler: {str(e)}\")\n",
    "            return None, None\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            logger.error(f\"Timeout bei API-Anfrage: {str(e)}\")\n",
    "            return None, None\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            logger.error(f\"Verbindungsfehler: {str(e)}\")\n",
    "            return None, None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"API request failed: {str(e)}\")\n",
    "            if hasattr(e, 'response'):\n",
    "                return e.response, None\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "    \"\"\"Handles data storage operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def write_file(\n",
    "        self, \n",
    "        data: Union[List, Dict, pl.DataFrame],\n",
    "        filename: str,\n",
    "        path: str,\n",
    "        postfix: str\n",
    "    ) -> None:\n",
    "        \"\"\"Writes data to file system\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        full_path = f\"{path}/{filename}.{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            if postfix == 'json':\n",
    "                with open(full_path, 'w') as f:\n",
    "                    json.dump(data, f)\n",
    "            elif postfix == 'parquet':\n",
    "                if not isinstance(data, pl.DataFrame):\n",
    "                    if isinstance(data, list) or isinstance(data, dict):\n",
    "                        data = pl.DataFrame(data)\n",
    "                    else:\n",
    "                        raise ValueError(\"Data must be DataFrame, List, or Dict for parquet format\")\n",
    "                data.write_parquet(full_path, compression=\"snappy\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {postfix}\")\n",
    "                \n",
    "            logger.info(f\"Data saved to {full_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to write file {full_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def cleanup_old_data(self, base_path: str, days_to_keep: int = 90) -> None:\n",
    "        \"\"\"\n",
    "        Löscht Daten, die älter als days_to_keep Tage sind\n",
    "        \"\"\"\n",
    "        try:\n",
    "            today = datetime.now()\n",
    "            cutoff_date = today - timedelta(days=days_to_keep)\n",
    "            \n",
    "            # Wandle in Pfad-Format um (Jahr/Monat/Tag)\n",
    "            cutoff_path = cutoff_date.strftime('%Y/%m/%d')\n",
    "            base_path = Path(base_path)\n",
    "            \n",
    "            if not base_path.exists():\n",
    "                return\n",
    "                \n",
    "            # Durchsuche alle Jahresordner\n",
    "            for year_dir in base_path.glob(\"*\"):\n",
    "                if not year_dir.is_dir() or not year_dir.name.isdigit():\n",
    "                    continue\n",
    "                    \n",
    "                year = int(year_dir.name)\n",
    "                \n",
    "                # Überspringe Ordner, die definitiv behalten werden sollen\n",
    "                if year > cutoff_date.year:\n",
    "                    continue\n",
    "                \n",
    "                # Behandle Jahre, die teilweise gelöscht werden müssen\n",
    "                if year == cutoff_date.year:\n",
    "                    for month_dir in year_dir.glob(\"*\"):\n",
    "                        if not month_dir.is_dir() or not month_dir.name.isdigit():\n",
    "                            continue\n",
    "                            \n",
    "                        month = int(month_dir.name)\n",
    "                        \n",
    "                        # Überspringe Monate, die definitiv behalten werden sollen\n",
    "                        if month > cutoff_date.month:\n",
    "                            continue\n",
    "                            \n",
    "                        # Behandle Monate, die teilweise gelöscht werden müssen\n",
    "                        if month == cutoff_date.month:\n",
    "                            for day_dir in month_dir.glob(\"*\"):\n",
    "                                if not day_dir.is_dir() or not day_dir.name.isdigit():\n",
    "                                    continue\n",
    "                                    \n",
    "                                day = int(day_dir.name)\n",
    "                                \n",
    "                                # Lösche Tage, die älter als der Cutoff sind\n",
    "                                if day < cutoff_date.day:\n",
    "                                    logger.info(f\"Removing old data directory: {day_dir}\")\n",
    "                                    # In Produktion wäre hier tatsächliches Löschen (shutil.rmtree)\n",
    "                                    # Für Sicherheit vorerst nur Logging\n",
    "                                    # import shutil\n",
    "                                    # shutil.rmtree(day_dir)\n",
    "                        \n",
    "                        # Lösche den gesamten Monat, wenn er älter als der Cutoff-Monat ist\n",
    "                        elif month < cutoff_date.month:\n",
    "                            logger.info(f\"Removing old data directory: {month_dir}\")\n",
    "                            # import shutil\n",
    "                            # shutil.rmtree(month_dir)\n",
    "                \n",
    "                # Lösche das gesamte Jahr, wenn es älter als das Cutoff-Jahr ist\n",
    "                elif year < cutoff_date.year:\n",
    "                    logger.info(f\"Removing old data directory: {year_dir}\")\n",
    "                    # import shutil\n",
    "                    # shutil.rmtree(year_dir)\n",
    "                    \n",
    "            logger.info(f\"Cleanup of data older than {cutoff_date.strftime('%Y-%m-%d')} completed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data cleanup: {str(e)}\")\n",
    "\n",
    "    def write_to_db(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        engine: sa.Engine,\n",
    "        table_name: str,\n",
    "        if_exists: str = \"replace\",\n",
    "        batch_size: int = 10000\n",
    "    ) -> None:\n",
    "        \"\"\"Writes DataFrame to database in batches\"\"\"\n",
    "        try:\n",
    "            # Convert to pandas for writing to database in batches\n",
    "            pdf = df.to_pandas()\n",
    "            total_rows = len(pdf)\n",
    "            \n",
    "            if total_rows == 0:\n",
    "                logger.warning(f\"No data to write to table {table_name}\")\n",
    "                return\n",
    "                \n",
    "            logger.info(f\"Writing {total_rows} rows to table {table_name}\")\n",
    "            \n",
    "            # Write in batches to avoid memory issues\n",
    "            for i in range(0, total_rows, batch_size):\n",
    "                end = min(i + batch_size, total_rows)\n",
    "                batch = pdf.iloc[i:end]\n",
    "                \n",
    "                # For first batch, replace or append based on if_exists parameter\n",
    "                if i == 0:\n",
    "                    batch.to_sql(\n",
    "                        table_name,\n",
    "                        engine,\n",
    "                        if_exists=if_exists,\n",
    "                        index=False,\n",
    "                        method='multi',\n",
    "                        chunksize=1000\n",
    "                    )\n",
    "                else:\n",
    "                    # For subsequent batches, always append\n",
    "                    batch.to_sql(\n",
    "                        table_name,\n",
    "                        engine,\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        method='multi',\n",
    "                        chunksize=1000\n",
    "                    )\n",
    "                    \n",
    "                logger.info(f\"Wrote batch {i//batch_size + 1} of {(total_rows-1)//batch_size + 1} to table {table_name}\")\n",
    "                \n",
    "            logger.info(f\"Data successfully written to table {table_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Database write failed: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def write_ts_to_msdb(\n",
    "        self,\n",
    "        df: pl.DataFrame, \n",
    "        engine: sa.Engine,\n",
    "        batch_size: int = 10000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Writes time series data to MSSQL database using a staging table approach.\n",
    "        \n",
    "        This specialized method:\n",
    "        1. Writes the dataframe to a staging table\n",
    "        2. Ensures the main table exists with the correct schema\n",
    "        3. Adds any missing columns to the main table\n",
    "        4. Merges data from staging to the main table using MERGE statement\n",
    "        \n",
    "        Parameters:\n",
    "            df (pl.DataFrame): Time series data to write\n",
    "            engine (sa.Engine): SQLAlchemy engine for database connection\n",
    "            batch_size (int): Number of rows to write in each batch\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If dataframe is empty, nothing to do\n",
    "            if len(df) == 0:\n",
    "                logger.warning(\"No time series data to write to database\")\n",
    "                return\n",
    "            \n",
    "            # Step 1: Write DataFrame to staging table\n",
    "            self.write_to_db(df, engine, \"TimeSeries_Staging\", if_exists=\"replace\", batch_size=batch_size)\n",
    "            logger.info(\"Data written to TimeSeries_Staging table\")\n",
    "            \n",
    "            # Step 2: Ensure main pivot table exists\n",
    "            create_pivot_table_sql = \"\"\"\n",
    "            IF OBJECT_ID('TimeSeries_pivot', 'U') IS NULL\n",
    "            BEGIN\n",
    "                CREATE TABLE TimeSeries_pivot (\n",
    "                    imo NVARCHAR(255) NOT NULL,\n",
    "                    signal_timestamp DATETIME NOT NULL,\n",
    "                    loaddate NVARCHAR(255),\n",
    "                    PRIMARY KEY (imo, signal_timestamp)\n",
    "                );\n",
    "            END\n",
    "            \"\"\"\n",
    "            \n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(create_pivot_table_sql))\n",
    "                conn.commit()\n",
    "                logger.info(\"Ensured TimeSeries_pivot table exists\")\n",
    "            \n",
    "            # Step 3: Add missing columns to main table\n",
    "            add_columns_sql = \"\"\"\n",
    "            DECLARE @column_name NVARCHAR(255)\n",
    "            DECLARE @sql NVARCHAR(MAX)\n",
    "\n",
    "            DECLARE column_cursor CURSOR FOR\n",
    "            SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS \n",
    "            WHERE TABLE_NAME = 'TimeSeries_Staging'\n",
    "            AND COLUMN_NAME NOT IN (SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'TimeSeries_pivot')\n",
    "            AND COLUMN_NAME NOT IN ('imo', 'signal_timestamp', 'loaddate')  -- Skip key columns and loaddate\n",
    "\n",
    "            OPEN column_cursor\n",
    "            FETCH NEXT FROM column_cursor INTO @column_name\n",
    "\n",
    "            WHILE @@FETCH_STATUS = 0\n",
    "            BEGIN\n",
    "                SET @sql = 'ALTER TABLE TimeSeries_pivot ADD [' + @column_name + '] FLOAT NULL'\n",
    "                EXEC sp_executesql @sql\n",
    "                FETCH NEXT FROM column_cursor INTO @column_name\n",
    "            END\n",
    "\n",
    "            CLOSE column_cursor\n",
    "            DEALLOCATE column_cursor\n",
    "            \"\"\"\n",
    "\n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(add_columns_sql))\n",
    "                conn.commit()\n",
    "                logger.info(\"Added any missing columns to TimeSeries_pivot table\")\n",
    "            \n",
    "            # Step 4: Pivotieren der Daten in der Staging-Tabelle\n",
    "            # Dieses SQL transformiert die Daten von der Staging-Tabelle, wo sie im Format\n",
    "            # imo, signal, signal_timestamp, signal_value sind, in das Pivot-Format\n",
    "            pivot_staging_sql = \"\"\"\n",
    "            -- Create a temporary table to hold pivoted data\n",
    "            IF OBJECT_ID('tempdb..#TempPivot', 'U') IS NOT NULL\n",
    "                DROP TABLE #TempPivot;\n",
    "\n",
    "            -- Get the list of unique signals for dynamic pivot\n",
    "            DECLARE @columns NVARCHAR(MAX);\n",
    "            DECLARE @sql NVARCHAR(MAX);\n",
    "\n",
    "            -- Create a list of signals as columns for the PIVOT operation\n",
    "            SELECT @columns = STRING_AGG(QUOTENAME(signal), ',')\n",
    "            FROM (SELECT DISTINCT signal FROM TimeSeries_Staging) AS signals;\n",
    "\n",
    "            -- Prepare the dynamic SQL for pivoting\n",
    "            SET @sql = N'\n",
    "            SELECT imo, signal_timestamp, loaddate, ' + @columns + '\n",
    "            INTO #TempPivot\n",
    "            FROM (\n",
    "                SELECT imo, signal, signal_timestamp, signal_value, loaddate\n",
    "                FROM TimeSeries_Staging\n",
    "            ) AS src\n",
    "            PIVOT (\n",
    "                MAX(signal_value)\n",
    "                FOR signal IN (' + @columns + ')\n",
    "            ) AS pvt;\n",
    "            ';\n",
    "\n",
    "            -- Execute the dynamic SQL to create the temporary pivot table\n",
    "            EXEC sp_executesql @sql;\n",
    "\n",
    "            -- Create a MERGE statement to update the main table\n",
    "            SET @sql = N'\n",
    "            MERGE INTO TimeSeries_pivot AS target\n",
    "            USING #TempPivot AS source\n",
    "            ON target.imo = source.imo AND target.signal_timestamp = source.signal_timestamp\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET \n",
    "                    loaddate = source.loaddate' +\n",
    "                    -- Add column updates from temp table, excluding key columns\n",
    "                    (SELECT STRING_AGG(', ' + QUOTENAME(COLUMN_NAME) + ' = source.' + QUOTENAME(COLUMN_NAME), '')\n",
    "                     FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                     WHERE TABLE_NAME = 'TimeSeries_pivot'\n",
    "                     AND COLUMN_NAME NOT IN ('imo', 'signal_timestamp', 'loaddate')) + '\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (imo, signal_timestamp, loaddate' +\n",
    "                    -- Add columns from temp table, excluding key columns\n",
    "                    (SELECT STRING_AGG(', ' + QUOTENAME(COLUMN_NAME), '')\n",
    "                     FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                     WHERE TABLE_NAME = 'TimeSeries_pivot'\n",
    "                     AND COLUMN_NAME NOT IN ('imo', 'signal_timestamp', 'loaddate')) + ')\n",
    "                VALUES (source.imo, source.signal_timestamp, source.loaddate' +\n",
    "                    -- Add values from temp table, excluding key columns\n",
    "                    (SELECT STRING_AGG(', source.' + QUOTENAME(COLUMN_NAME), '')\n",
    "                     FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                     WHERE TABLE_NAME = 'TimeSeries_pivot'\n",
    "                     AND COLUMN_NAME NOT IN ('imo', 'signal_timestamp', 'loaddate')) + ');';\n",
    "\n",
    "            -- Execute the merge\n",
    "            EXEC sp_executesql @sql;\n",
    "\n",
    "            -- Clean up\n",
    "            DROP TABLE #TempPivot;\n",
    "            \"\"\"\n",
    "            \n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(pivot_staging_sql))\n",
    "                conn.commit()\n",
    "                logger.info(\"Data merged into TimeSeries_pivot table\")\n",
    "                \n",
    "            # Optional: Schreibe Gaps-Daten in separate Tabelle\n",
    "            gaps_table_sql = \"\"\"\n",
    "            IF OBJECT_ID('TimeSeries_Gaps', 'U') IS NULL\n",
    "            BEGIN\n",
    "                CREATE TABLE TimeSeries_Gaps (\n",
    "                    imo NVARCHAR(255) NOT NULL,\n",
    "                    signal NVARCHAR(255) NOT NULL,\n",
    "                    gap_start DATETIME NOT NULL,\n",
    "                    gap_end DATETIME NOT NULL,\n",
    "                    loaddate NVARCHAR(255),\n",
    "                    PRIMARY KEY (imo, signal, gap_start)\n",
    "                );\n",
    "            END\n",
    "            \"\"\"\n",
    "            \n",
    "            with engine.connect() as conn:\n",
    "                conn.execute(text(gaps_table_sql))\n",
    "                conn.commit()\n",
    "                logger.info(\"Ensured TimeSeries_Gaps table exists\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"MSSQL database operation failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Processes raw API data into analytics-ready format\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_imo_numbers(data: List[dict]) -> List[str]:\n",
    "        \"\"\"Extracts IMO numbers from ship data\"\"\"\n",
    "        return [ship['imo'] for ship in data if ship.get('active', True)]\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_signals(signals: pl.DataFrame, run_timestamp: str) -> pl.DataFrame:\n",
    "        \"\"\"Transforms signals data\"\"\"\n",
    "        if len(signals) == 0:\n",
    "            return signals\n",
    "            \n",
    "        signals = (\n",
    "            signals.lazy()\n",
    "            .unnest(\"signals\")\n",
    "            .unpivot(index=\"imo\", variable_name=\"signal\")\n",
    "            .unnest(\"value\")\n",
    "        )\n",
    "\n",
    "        # Unnest remaining structs\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Struct:\n",
    "                signals = signals.unnest(column)\n",
    "\n",
    "        # Handle null columns\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Null:\n",
    "                signals = signals.with_columns(pl.col(column).cast(pl.String))\n",
    "        \n",
    "        # Add loaddate\n",
    "        signals = signals.with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        )\n",
    "                \n",
    "        return signals.collect()\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_timeseries(timeseries: pl.DataFrame, imo: str, run_timestamp: str) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Transforms time series data and extracts gaps data\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (transformed_data, gaps_data)\n",
    "        \"\"\"\n",
    "        if len(timeseries) == 0:\n",
    "            return timeseries, pl.DataFrame()\n",
    "        \n",
    "        # Initial transformation\n",
    "        transformed = (\n",
    "            timeseries.lazy()\n",
    "            .drop(\"timestamp\")\n",
    "            .unpivot(variable_name=\"signal\")\n",
    "            .unnest(\"value\")\n",
    "            .unpivot(\n",
    "                index=\"signal\",\n",
    "                variable_name=\"signal_timestamp\",\n",
    "                value_name=\"signal_value\",\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.lit(imo).alias(\"imo\"),\n",
    "                pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Identifiziere Lücken (NULL-Werte)\n",
    "        gaps = (\n",
    "            transformed\n",
    "            .filter(pl.col(\"signal_value\").is_null())\n",
    "            .select([\"imo\", \"signal\", \"signal_timestamp\", \"loaddate\"])\n",
    "            .with_columns(\n",
    "                pl.col(\"signal_timestamp\").alias(\"gap_start\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Entferne NULL-Werte aus dem Hauptdatensatz\n",
    "        data = transformed.filter(pl.col(\"signal_value\").is_not_null())\n",
    "        \n",
    "        return data.collect(), gaps.collect()\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_gaps(gaps_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Verarbeitet Lücken-Daten, um zusammenhängende Zeiträume zu identifizieren\n",
    "        \"\"\"\n",
    "        if len(gaps_df) == 0:\n",
    "            return pl.DataFrame()\n",
    "            \n",
    "        # Gruppiere nach IMO und Signal, sortiere nach Zeitstempel\n",
    "        result = []\n",
    "        \n",
    "        # Konvertiere zu Pandas für einfachere Gruppierung und Verarbeitung\n",
    "        # (In einer produktiven Umgebung kann dies für große Datensätze \n",
    "        # effizienter mit Polars-nativen Funktionen implementiert werden)\n",
    "        gaps_pd = gaps_df.to_pandas()\n",
    "        \n",
    "        for (imo, signal), group in gaps_pd.groupby(['imo', 'signal']):\n",
    "            group = group.sort_values('gap_start')\n",
    "            \n",
    "            # Parse timestamps to datetime\n",
    "            group['gap_start'] = pd.to_datetime(group['gap_start'])\n",
    "            \n",
    "            current_start = group['gap_start'].iloc[0]\n",
    "            prev_time = current_start\n",
    "            \n",
    "            for idx, row in group.iloc[1:].iterrows():\n",
    "                curr_time = row['gap_start']\n",
    "                \n",
    "                # Wenn mehr als 15 Minuten zwischen den Zeitstempeln liegen, \n",
    "                # betrachte es als neue Lücke\n",
    "                if (curr_time - prev_time) > timedelta(minutes=15):\n",
    "                    result.append({\n",
    "                        'imo': imo,\n",
    "                        'signal': signal,\n",
    "                        'gap_start': current_start.isoformat(),\n",
    "                        'gap_end': prev_time.isoformat(),\n",
    "                        'loaddate': row['loaddate']\n",
    "                    })\n",
    "                    current_start = curr_time\n",
    "                \n",
    "                prev_time = curr_time\n",
    "            \n",
    "            # Füge die letzte Lücke hinzu\n",
    "            result.append({\n",
    "                'imo': imo,\n",
    "                'signal': signal,\n",
    "                'gap_start': current_start.isoformat(),\n",
    "                'gap_end': prev_time.isoformat(),\n",
    "                'loaddate': group['loaddate'].iloc[-1]\n",
    "            })\n",
    "        \n",
    "        return pl.DataFrame(result)\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_ships(ships: pl.DataFrame, run_timestamp: str) -> Tuple[pl.DataFrame, Dict[str, pl.DataFrame]]:\n",
    "        \"\"\"Transforms ship data and extracts nested tables\"\"\"\n",
    "        ships = ships.lazy().unnest(\"data\")\n",
    "        \n",
    "        # Extract nested tables\n",
    "        tables = {}\n",
    "        for column, dtype in ships.collect_schema().items():\n",
    "            if dtype == pl.List(pl.Struct):\n",
    "                tables[column] = (\n",
    "                    ships.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .unnest(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    .collect()\n",
    "                )\n",
    "            elif dtype == pl.List:\n",
    "                tables[column] = (\n",
    "                    ships.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    .collect()\n",
    "                )\n",
    "\n",
    "        # Keep only non-list columns in main table\n",
    "        ships = ships.select(\n",
    "            pl.exclude([col for col, dtype in ships.collect_schema().items() if dtype == pl.List])\n",
    "        ).with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        ).collect()\n",
    "\n",
    "        return ships, tables\n",
    "    \n",
    "    @staticmethod\n",
    "    def enrich_timeseries_with_friendly_names(timeseries_df: pl.DataFrame, signals_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Fügt friendly_name aus der Signaldatei zu den Timeseries-Daten hinzu\n",
    "        \"\"\"\n",
    "        if len(timeseries_df) == 0 or len(signals_df) == 0:\n",
    "            return timeseries_df\n",
    "            \n",
    "        # Extrahiere Signal-Mapping (signal -> friendly_name)\n",
    "        signal_mapping = (\n",
    "            signals_df\n",
    "            .filter(pl.col(\"friendly_name\").is_not_null())\n",
    "            .select([\"signal\", \"friendly_name\"])\n",
    "            .unique()\n",
    "        )\n",
    "        \n",
    "        # Join mit Timeseries-Daten\n",
    "        return timeseries_df.join(\n",
    "            signal_mapping,\n",
    "            on=\"signal\",\n",
    "            how=\"left\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"Main data pipeline class\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, api_key: str, verify_ssl: bool = True):\n",
    "        self.config = config\n",
    "        self.api_client = APIClient(config.base_url, api_key, timeout=config.timeout, verify_ssl=verify_ssl)\n",
    "        self.processor = DataProcessor()\n",
    "        self.storage = DataStorage(config)\n",
    "        \n",
    "    def process_ship(self, imo: str, run_timestamp: str) -> None:\n",
    "        \"\"\"Processes data for a single ship\"\"\"\n",
    "        try:\n",
    "            # Get and process signals\n",
    "            _, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "            if signals:\n",
    "                signals_df = pl.DataFrame(signals)\n",
    "                self.storage.write_file(\n",
    "                    signals,\n",
    "                    f\"Signals_{imo}\",\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "                signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "                self.storage.write_file(\n",
    "                    signals_transformed,\n",
    "                    f\"Signals_{imo}\",\n",
    "                    f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                    'parquet'\n",
    "                )\n",
    "\n",
    "            # Get and process timeseries\n",
    "            _, timeseries = self.api_client.get_data(f\"fleet/{imo}/timeseries\")\n",
    "            if timeseries:\n",
    "                ts_df = pl.DataFrame(timeseries)\n",
    "                self.storage.write_file(\n",
    "                    timeseries,\n",
    "                    f\"Timeseries_{imo}\",\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "                # Transformieren und Lücken extrahieren\n",
    "                ts_transformed, gaps = self.processor.transform_timeseries(ts_df, imo, run_timestamp)\n",
    "                \n",
    "                self.storage.write_file(\n",
    "                    ts_transformed,\n",
    "                    f\"Timeseries_{imo}\",\n",
    "                    f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                    'parquet'\n",
    "                )\n",
    "                \n",
    "                # Prozessiere und speichere Lücken-Daten, falls vorhanden\n",
    "                if len(gaps) > 0:\n",
    "                    processed_gaps = self.processor.process_gaps(gaps)\n",
    "                    if len(processed_gaps) > 0:\n",
    "                        self.storage.write_file(\n",
    "                            processed_gaps,\n",
    "                            f\"Gaps_{imo}\",\n",
    "                            f\"{self.config.gaps_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process ship {imo}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def run(self, mode: str = \"all\") -> Tuple[str, Dict]:\n",
    "        \"\"\"\n",
    "        Runs the complete data pipeline\n",
    "        \n",
    "        Args:\n",
    "            mode (str): 'all' for complete pipeline, 'timeseries' for only timeseries data,\n",
    "                        'fleet' for only fleet and signals data\n",
    "                        \n",
    "        Returns:\n",
    "            Tuple[str, Dict]: (run_timestamp, stats) mit Statistiken zur Ausführung\n",
    "        \"\"\"\n",
    "        run_start = datetime.now(timezone.utc)\n",
    "        run_timestamp = run_start.strftime('%Y/%m/%d/%H/%M')\n",
    "\n",
    "        # Tracking-Statistiken\n",
    "        stats = {\n",
    "            \"success\": True,\n",
    "            \"ships_total\": 0,\n",
    "            \"ships_processed\": 0,\n",
    "            \"ships_failed\": 0,\n",
    "            \"api_calls_total\": 0,\n",
    "            \"api_calls_success\": 0,\n",
    "            \"api_calls_failed\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "        \n",
    "        # Thread-sicherer Counter für API-Aufrufe\n",
    "        api_counters = {\n",
    "            \"total\": 0,\n",
    "            \"success\": 0,\n",
    "            \"failed\": 0,\n",
    "        }\n",
    "        \n",
    "        # Thread-sichere Liste für Fehler\n",
    "        all_errors = []\n",
    "        \n",
    "        try:\n",
    "            # Initialize directories\n",
    "            os.makedirs(f\"{self.config.raw_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.config.transformed_path}/{run_timestamp}\", exist_ok=True)\n",
    "            os.makedirs(f\"{self.config.gaps_path}/{run_timestamp}\", exist_ok=True)\n",
    "            \n",
    "            # Get ship data\n",
    "            if mode in [\"all\", \"fleet\"]:\n",
    "                response, ships = self.api_client.get_data(\"fleet\")\n",
    "                api_counters[\"total\"] += 1\n",
    "                \n",
    "                if not ships:\n",
    "                    api_counters[\"failed\"] += 1\n",
    "                    error_msg = \"Failed to get ship data\"\n",
    "                    all_errors.append(error_msg)\n",
    "                    raise ValueError(error_msg)\n",
    "                else:\n",
    "                    api_counters[\"success\"] += 1\n",
    "                    \n",
    "                # Process ships\n",
    "                imo_numbers = self.processor.get_imo_numbers(ships)\n",
    "                \n",
    "                # Store raw ship data\n",
    "                self.storage.write_file(\n",
    "                    ships,\n",
    "                    'ShipData',\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "                \n",
    "                # Transform and store ship data\n",
    "                ships_df = pl.DataFrame(ships)\n",
    "                ships_transformed, tables = self.processor.transform_ships(ships_df, run_timestamp)\n",
    "                self.storage.write_file(\n",
    "                    ships_transformed,\n",
    "                    'ShipData',\n",
    "                    f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                    'parquet'\n",
    "                )\n",
    "                \n",
    "                # Process nested tables\n",
    "                for name, table in tables.items():\n",
    "                    self.storage.write_file(\n",
    "                        table,\n",
    "                        f\"ShipData_{name}\",\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                \n",
    "                # Process signals for all ships\n",
    "                for imo in imo_numbers:\n",
    "                    response, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "                    api_counters[\"total\"] += 1\n",
    "                    \n",
    "                    if not signals:\n",
    "                        api_counters[\"failed\"] += 1\n",
    "                        all_errors.append(f\"Failed to get signals for ship {imo}\")\n",
    "                        continue\n",
    "                        \n",
    "                    api_counters[\"success\"] += 1\n",
    "                    signals_df = pl.DataFrame(signals)\n",
    "                    self.storage.write_file(\n",
    "                        signals,\n",
    "                        f\"Signals_{imo}\",\n",
    "                        f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                        'json'\n",
    "                    )\n",
    "                    signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "                    self.storage.write_file(\n",
    "                        signals_transformed,\n",
    "                        f\"Signals_{imo}\",\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "            \n",
    "            # Process timeseries data\n",
    "            if mode in [\"all\", \"timeseries\"]:\n",
    "                # Get ship data if not already loaded\n",
    "                if mode == \"timeseries\":\n",
    "                    response, ships = self.api_client.get_data(\"fleet\")\n",
    "                    api_counters[\"total\"] += 1\n",
    "                    \n",
    "                    if not ships:\n",
    "                        api_counters[\"failed\"] += 1\n",
    "                        error_msg = \"Failed to get ship data\"\n",
    "                        all_errors.append(error_msg)\n",
    "                        raise ValueError(error_msg)\n",
    "                    else:\n",
    "                        api_counters[\"success\"] += 1\n",
    "                        \n",
    "                imo_numbers = self.processor.get_imo_numbers(ships)\n",
    "                \n",
    "                # Lokale Thread-spezifische Statistiken\n",
    "                ship_results = []\n",
    "                \n",
    "                # Process individual ships in parallel\n",
    "                with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                    # Funktion zur Verarbeitung von Zeitreihendaten für ein Schiff\n",
    "                    def process_ship_timeseries(imo):\n",
    "                        ship_stats = {\"imo\": imo, \"success\": False, \"errors\": [], \"api_calls\": 0, \"api_success\": 0}\n",
    "                        try:\n",
    "                            # Get and process timeseries\n",
    "                            response, timeseries = self.api_client.get_data(f\"fleet/{imo}/timeseries\")\n",
    "                            ship_stats[\"api_calls\"] += 1\n",
    "                            \n",
    "                            if response is None:\n",
    "                                error_msg = f\"No response from server for ship {imo}\"\n",
    "                                ship_stats[\"errors\"].append(error_msg)\n",
    "                                logger.error(f\"API_ERROR|{imo}|timeseries|{error_msg}\")\n",
    "                                return ship_stats\n",
    "                            \n",
    "                            if not timeseries:\n",
    "                                error_msg = f\"Empty timeseries data for ship {imo}\"\n",
    "                                ship_stats[\"errors\"].append(error_msg)\n",
    "                                logger.warning(f\"API_WARNING|{imo}|timeseries|{error_msg}\")\n",
    "                                return ship_stats\n",
    "                            \n",
    "                            ship_stats[\"api_success\"] += 1\n",
    "                            \n",
    "                            # Verarbeitung\n",
    "                            ts_df = pl.DataFrame(timeseries)\n",
    "                            self.storage.write_file(\n",
    "                                timeseries,\n",
    "                                f\"Timeseries_{imo}\",\n",
    "                                f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                                'json'\n",
    "                            )\n",
    "                            \n",
    "                            # Transformieren und Lücken extrahieren\n",
    "                            ts_transformed, gaps = self.processor.transform_timeseries(ts_df, imo, run_timestamp)\n",
    "                            \n",
    "                            # Combine with historical data (last 5 days)\n",
    "                            historical_data = self.load_historical_timeseries(imo, self.config.history_days)\n",
    "                            if len(historical_data) > 0:\n",
    "                                # Combine historical and new data\n",
    "                                combined = pl.concat([historical_data, ts_transformed])\n",
    "                                # Deduplicate\n",
    "                                ts_transformed = (\n",
    "                                    combined\n",
    "                                    .sort(by=[\"loaddate\"], descending=True)\n",
    "                                    .unique(subset=[\"imo\", \"signal\", \"signal_timestamp\"], keep=\"first\")\n",
    "                                )\n",
    "                            \n",
    "                            self.storage.write_file(\n",
    "                                ts_transformed,\n",
    "                                f\"Timeseries_{imo}\",\n",
    "                                f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                                'parquet'\n",
    "                            )\n",
    "                            \n",
    "                            # Prozessiere und speichere Lücken-Daten, falls vorhanden\n",
    "                            if len(gaps) > 0:\n",
    "                                processed_gaps = self.processor.process_gaps(gaps)\n",
    "                                if len(processed_gaps) > 0:\n",
    "                                    self.storage.write_file(\n",
    "                                        processed_gaps,\n",
    "                                        f\"Gaps_{imo}\",\n",
    "                                        f\"{self.config.gaps_path}/{run_timestamp}\",\n",
    "                                        'parquet'\n",
    "                                    )\n",
    "                                    \n",
    "                            ship_stats[\"success\"] = True\n",
    "                            return ship_stats\n",
    "                        except Exception as e:\n",
    "                            error_msg = f\"Failed to process timeseries for ship {imo}: {str(e)}\"\n",
    "                            ship_stats[\"errors\"].append(error_msg)\n",
    "                            logger.error(error_msg)\n",
    "                            return ship_stats\n",
    "                    \n",
    "                    # Execute in parallel\n",
    "                    ship_results = list(executor.map(process_ship_timeseries, imo_numbers))\n",
    "                \n",
    "                # Verarbeite die Ergebnisse und aktualisiere die Gesamtstatistik\n",
    "                stats[\"ships_total\"] = len(imo_numbers)\n",
    "                \n",
    "                for ship_result in ship_results:\n",
    "                    if ship_result[\"success\"]:\n",
    "                        stats[\"ships_processed\"] += 1\n",
    "                    else:\n",
    "                        stats[\"ships_failed\"] += 1\n",
    "                        all_errors.extend(ship_result[\"errors\"])\n",
    "                    \n",
    "                    api_counters[\"total\"] += ship_result.get(\"api_calls\", 0)\n",
    "                    api_counters[\"success\"] += ship_result.get(\"api_success\", 0)\n",
    "                    api_counters[\"failed\"] += ship_result.get(\"api_calls\", 0) - ship_result.get(\"api_success\", 0)\n",
    "                \n",
    "                stats[\"ship_results\"] = ship_results\n",
    "            \n",
    "            # Clean up old data\n",
    "            self.storage.cleanup_old_data(self.config.raw_path, self.config.days_to_keep)\n",
    "            self.storage.cleanup_old_data(self.config.transformed_path, self.config.days_to_keep)\n",
    "            self.storage.cleanup_old_data(self.config.gaps_path, self.config.days_to_keep)\n",
    "            \n",
    "            # Finale Statistiken zusammenstellen\n",
    "            stats[\"api_calls_total\"] = api_counters[\"total\"]\n",
    "            stats[\"api_calls_success\"] = api_counters[\"success\"]\n",
    "            stats[\"api_calls_failed\"] = api_counters[\"failed\"]\n",
    "            stats[\"errors\"] = all_errors\n",
    "            \n",
    "            if stats[\"ships_failed\"] > 0:\n",
    "                stats[\"success\"] = False\n",
    "            \n",
    "            logger.info(f\"Pipeline run completed with stats: {stats}\")\n",
    "            return run_timestamp, stats\n",
    "                \n",
    "        except Exception as e:\n",
    "            stats[\"success\"] = False\n",
    "            all_errors.append(f\"Critical error: {str(e)}\")\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            \n",
    "            # Finale Statistiken zusammenstellen\n",
    "            stats[\"api_calls_total\"] = api_counters[\"total\"]\n",
    "            stats[\"api_calls_success\"] = api_counters[\"success\"]\n",
    "            stats[\"api_calls_failed\"] = api_counters[\"failed\"]\n",
    "            stats[\"errors\"] = all_errors\n",
    "            \n",
    "            return run_timestamp, stats\n",
    "            \n",
    "        finally:\n",
    "            runtime = datetime.now(timezone.utc) - run_start\n",
    "            logger.info(f\"Pipeline completed in {runtime}\")\n",
    "            \n",
    "    def run_timeseries_only(self) -> str:\n",
    "        \"\"\"Convenience method to run only the timeseries part of the pipeline\"\"\"\n",
    "        return self.run(mode=\"timeseries\")\n",
    "        \n",
    "    def run_fleet_only(self) -> str:\n",
    "        \"\"\"Convenience method to run only the fleet part of the pipeline\"\"\"\n",
    "        return self.run(mode=\"fleet\")\n",
    "    \n",
    "    def load_historical_timeseries(self, imo: str, days: int = 5) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Lädt historische Timeseries-Daten für ein Schiff aus den letzten n Tagen\n",
    "        \"\"\"\n",
    "        try:\n",
    "            today = datetime.now()\n",
    "            \n",
    "            # Erstelle eine Liste von Pfaden für die letzten n Tage\n",
    "            paths = []\n",
    "            for i in range(days):\n",
    "                check_date = today - timedelta(days=i)\n",
    "                date_path = check_date.strftime('%Y/%m/%d')\n",
    "                \n",
    "                # Alle Unterordner des Tages durchsuchen (Stunden/Minuten)\n",
    "                full_path = Path(f\"{self.config.transformed_path}/{date_path}\")\n",
    "                \n",
    "                if full_path.exists():\n",
    "                    # Finde die letzten Runs des Tages\n",
    "                    for hour_dir in sorted(full_path.glob(\"*\"), reverse=True):\n",
    "                        if hour_dir.is_dir():\n",
    "                            for minute_dir in sorted(hour_dir.glob(\"*\"), reverse=True):\n",
    "                                if minute_dir.is_dir():\n",
    "                                    file_path = minute_dir / f\"Timeseries_{imo}.parquet\"\n",
    "                                    if file_path.exists():\n",
    "                                        paths.append(str(file_path))\n",
    "                                        # Nimm nur den letzten Run des Tages\n",
    "                                        break\n",
    "                            # Nimm nur die letzte Stunde des Tages\n",
    "                            break\n",
    "            \n",
    "            # Load and combine data\n",
    "            dfs = []\n",
    "            for path in paths:\n",
    "                try:\n",
    "                    df = pl.read_parquet(path)\n",
    "                    if len(df) > 0:\n",
    "                        dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to read {path}: {str(e)}\")\n",
    "            \n",
    "            if not dfs:\n",
    "                logger.info(f\"No historical timeseries data found for ship {imo}\")\n",
    "                return pl.DataFrame()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load historical timeseries for ship {imo}: {str(e)}\")\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "    def get_all_signals(self, run_timestamp: str) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Sammelt alle Signaldefinitionen aus den aktuellen Daten\n",
    "        \"\"\"\n",
    "        try:\n",
    "            signals_path = Path(f\"{self.config.transformed_path}/{run_timestamp}\")\n",
    "            all_signals = []\n",
    "            \n",
    "            for file in signals_path.glob(\"Signals_*.parquet\"):\n",
    "                try:\n",
    "                    df = pl.read_parquet(file)\n",
    "                    if len(df) > 0:\n",
    "                        # Extrahiere nur die relevanten Spalten für das Mapping\n",
    "                        signals_mapping = df.select([\n",
    "                            \"signal\", \"friendly_name\", \"unit\", \"object_code\", \"name_code\", \"group_name\", \"sub_group\"\n",
    "                        ])\n",
    "                        all_signals.append(signals_mapping)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to read {file}: {str(e)}\")\n",
    "            \n",
    "            if not all_signals:\n",
    "                logger.warning(\"No signal definitions found\")\n",
    "                return pl.DataFrame()\n",
    "                \n",
    "            # Combine all signal definitions and deduplicate\n",
    "            return pl.concat(all_signals).unique(subset=[\"signal\"], keep=\"first\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to collect signal definitions: {str(e)}\")\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "    def process_and_store_to_db(self, engine: sa.Engine, run_timestamp: str) -> None:\n",
    "        \"\"\"\n",
    "        Processes data and stores it to the SQL database\n",
    "        \n",
    "        This method:\n",
    "        1. Loads timeseries data from the current run\n",
    "        2. Enriches it with friendly names\n",
    "        3. Writes the combined data to the SQL database using the specialized method\n",
    "        4. Writes gaps data to a separate table\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Load timeseries data from current run\n",
    "            transformed_path = Path(f\"{self.config.transformed_path}/{run_timestamp}\")\n",
    "            \n",
    "            # Find all timeseries parquet files\n",
    "            timeseries_files = list(transformed_path.glob(\"Timeseries_*.parquet\"))\n",
    "            if not timeseries_files:\n",
    "                logger.warning(f\"No timeseries files found in {run_timestamp}\")\n",
    "                return\n",
    "            \n",
    "            # 2. Load and combine all timeseries data\n",
    "            ts_dfs = []\n",
    "            for file in timeseries_files:\n",
    "                try:\n",
    "                    df = pl.read_parquet(file)\n",
    "                    if len(df) > 0:\n",
    "                        ts_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to read {file}: {str(e)}\")\n",
    "            \n",
    "            if not ts_dfs:\n",
    "                logger.warning(\"No data loaded from timeseries files\")\n",
    "                return\n",
    "            \n",
    "            combined_ts = pl.concat(ts_dfs)\n",
    "            logger.info(f\"Combined {len(ts_dfs)} timeseries files with {len(combined_ts)} rows total\")\n",
    "            \n",
    "            # 3. Get all signal definitions for enrichment\n",
    "            all_signals = self.get_all_signals(run_timestamp)\n",
    "            \n",
    "            # 4. Enrich timeseries data with friendly names\n",
    "            if len(all_signals) > 0:\n",
    "                combined_ts = self.processor.enrich_timeseries_with_friendly_names(combined_ts, all_signals)\n",
    "                logger.info(\"Enriched timeseries data with friendly names\")\n",
    "            \n",
    "            # 5. Write to database using the specialized method\n",
    "            if len(combined_ts) > 0:\n",
    "                self.storage.write_ts_to_msdb(combined_ts, engine)\n",
    "                logger.info(\"Successfully processed and stored timeseries data to database\")\n",
    "            \n",
    "            # 6. Process and store gaps data\n",
    "            gaps_path = Path(f\"{self.config.gaps_path}/{run_timestamp}\")\n",
    "            if gaps_path.exists():\n",
    "                gaps_files = list(gaps_path.glob(\"Gaps_*.parquet\"))\n",
    "                gaps_dfs = []\n",
    "                \n",
    "                for file in gaps_files:\n",
    "                    try:\n",
    "                        df = pl.read_parquet(file)\n",
    "                        if len(df) > 0:\n",
    "                            gaps_dfs.append(df)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to read gaps file {file}: {str(e)}\")\n",
    "                \n",
    "                if gaps_dfs:\n",
    "                    combined_gaps = pl.concat(gaps_dfs)\n",
    "                    logger.info(f\"Combined {len(gaps_dfs)} gaps files with {len(combined_gaps)} rows total\")\n",
    "                    \n",
    "                    # Write gaps data to a separate table\n",
    "                    self.storage.write_to_db(combined_gaps, engine, \"TimeSeries_Gaps\", if_exists=\"append\")\n",
    "                    logger.info(\"Successfully stored gaps data to database\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process and store data to database: {str(e)}\")\n",
    "            raise\n",
    "                \n",
    "            # Combine all dataframes and keep only the latest value for each combination\n",
    "            combined = pl.concat(dfs)\n",
    "            \n",
    "            # Dedupliziere die Daten - behalte nur den neuesten Eintrag für jede Kombination von imo, signal und signal_timestamp\n",
    "            deduplicated = (\n",
    "                combined\n",
    "                .sort(by=[\"loaddate\"], descending=True)\n",
    "                .unique(subset=[\"imo\", \"signal\", \"signal_timestamp\"], keep=\"first\")\n",
    "            )\n",
    "            \n",
    "            return deduplicated\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load historical timeseries for ship {imo}: {str(e)}\")\n",
    "            return pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesPivotExporter:\n",
    "    \"\"\"Exports pivoted timeseries data from raw timeseries files\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the TimeseriesPivotExporter with configuration\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration object with paths and settings\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(\"hoppe_etl_pipeline\")\n",
    "    \n",
    "    def find_timeseries_files(self, base_path: str, max_days: int = None) -> list:\n",
    "        \"\"\"\n",
    "        Finds all Timeseries-Parquet files in the specified directory\n",
    "        \n",
    "        Args:\n",
    "            base_path: Base path to search\n",
    "            max_days: Optional, maximum number of days to consider (newest first)\n",
    "            \n",
    "        Returns:\n",
    "            List of paths to Timeseries files\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Durchsuche Verzeichnis {base_path} nach Timeseries-Dateien\")\n",
    "        \n",
    "        base_dir = Path(base_path)\n",
    "        if not base_dir.exists() or not base_dir.is_dir():\n",
    "            self.logger.error(f\"Verzeichnis {base_path} existiert nicht oder ist kein Verzeichnis\")\n",
    "            return []\n",
    "        \n",
    "        # Find all year directories, sort descending for newest first\n",
    "        years = sorted([d for d in base_dir.glob(\"*\") if d.is_dir() and d.name.isdigit()], \n",
    "                       key=lambda x: x.name, reverse=True)\n",
    "        \n",
    "        all_files = []\n",
    "        days_processed = 0\n",
    "        \n",
    "        # Traverse years, months, days\n",
    "        for year_dir in years:\n",
    "            months = sorted([d for d in year_dir.glob(\"*\") if d.is_dir() and d.name.isdigit()], \n",
    "                            key=lambda x: x.name, reverse=True)\n",
    "            \n",
    "            for month_dir in months:\n",
    "                days = sorted([d for d in month_dir.glob(\"*\") if d.is_dir() and d.name.isdigit()], \n",
    "                              key=lambda x: x.name, reverse=True)\n",
    "                \n",
    "                for day_dir in days:\n",
    "                    if max_days is not None and days_processed >= max_days:\n",
    "                        break\n",
    "                    \n",
    "                    # Find all hours and minutes for this day\n",
    "                    ts_files = []\n",
    "                    hour_dirs = sorted([d for d in day_dir.glob(\"*\") if d.is_dir()], \n",
    "                                       key=lambda x: x.name, reverse=True)\n",
    "                    \n",
    "                    for hour_dir in hour_dirs:\n",
    "                        minute_dirs = sorted([d for d in hour_dir.glob(\"*\") if d.is_dir()], \n",
    "                                             key=lambda x: x.name, reverse=True)\n",
    "                        \n",
    "                        for minute_dir in minute_dirs:\n",
    "                            # Find all timeseries files in this minute directory\n",
    "                            files = list(minute_dir.glob(\"Timeseries_*.parquet\"))\n",
    "                            if files:\n",
    "                                ts_files.extend(files)\n",
    "                                # Only the last run of the day\n",
    "                                break\n",
    "                        \n",
    "                        if ts_files:\n",
    "                            # Only the last hour of the day\n",
    "                            break\n",
    "                    \n",
    "                    all_files.extend(ts_files)\n",
    "                    days_processed += 1\n",
    "                    \n",
    "                    if max_days is not None and days_processed >= max_days:\n",
    "                        self.logger.info(f\"Maximale Anzahl von Tagen ({max_days}) erreicht\")\n",
    "                        break\n",
    "                \n",
    "                if max_days is not None and days_processed >= max_days:\n",
    "                    break\n",
    "            \n",
    "            if max_days is not None and days_processed >= max_days:\n",
    "                break\n",
    "        \n",
    "        self.logger.info(f\"Gefunden: {len(all_files)} Timeseries-Dateien aus {days_processed} Tagen\")\n",
    "        return all_files\n",
    "\n",
    "    def load_and_combine_timeseries(self, file_paths: list) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads and combines all timeseries files\n",
    "        \n",
    "        Args:\n",
    "            file_paths: List of file paths to load\n",
    "            \n",
    "        Returns:\n",
    "            Polars DataFrame with combined data\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Lade {len(file_paths)} Timeseries-Dateien\")\n",
    "        \n",
    "        all_dfs = []\n",
    "        total_rows = 0\n",
    "        \n",
    "        for idx, file_path in enumerate(file_paths):\n",
    "            try:\n",
    "                if idx % 10 == 0:\n",
    "                    self.logger.info(f\"Verarbeite Datei {idx + 1} von {len(file_paths)}\")\n",
    "                \n",
    "                df = pl.read_parquet(file_path)\n",
    "                rows = len(df)\n",
    "                total_rows += rows\n",
    "                \n",
    "                if rows > 0:\n",
    "                    all_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Fehler beim Lesen von {file_path}: {str(e)}\")\n",
    "        \n",
    "        if not all_dfs:\n",
    "            self.logger.warning(\"Keine Daten geladen\")\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "        # Combine all DataFrames\n",
    "        combined_df = pl.concat(all_dfs)\n",
    "        self.logger.info(f\"Insgesamt {total_rows} Zeilen geladen, {len(combined_df)} vor Deduplizierung\")\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "    def clean_and_deduplicate(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Removes null values and duplicates from the DataFrame\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to clean\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        if len(df) == 0:\n",
    "            return df\n",
    "        \n",
    "        # Remove rows with null values\n",
    "        no_nulls = df.filter(pl.col(\"signal_value\").is_not_null())\n",
    "        self.logger.info(f\"Zeilen nach Entfernen von Nullwerten: {len(no_nulls)}\")\n",
    "        \n",
    "        # Sort by load date (descending) and remove duplicates\n",
    "        deduplicated = (\n",
    "            no_nulls\n",
    "            .sort(by=[\"loaddate\"], descending=True)\n",
    "            .unique(subset=[\"imo\", \"signal\", \"signal_timestamp\"], keep=\"first\")\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Zeilen nach Deduplizierung: {len(deduplicated)}\")\n",
    "        return deduplicated\n",
    "\n",
    "    def pivot_timeseries(self, df: pl.DataFrame, max_signals: int = None) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Pivots the timeseries data: Signal becomes columns\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to pivot\n",
    "            max_signals: Optional, maximum number of signals to process\n",
    "                        (for memory and performance optimization)\n",
    "            \n",
    "        Returns:\n",
    "            Pivoted DataFrame\n",
    "        \"\"\"\n",
    "        if len(df) == 0:\n",
    "            return df\n",
    "        \n",
    "        self.logger.info(\"Pivotisiere Daten\")\n",
    "        \n",
    "        # Identify unique signals\n",
    "        unique_signals = df.select(\"signal\").unique()\n",
    "        signal_count = len(unique_signals)\n",
    "        \n",
    "        self.logger.info(f\"Gefunden: {signal_count} eindeutige Signale\")\n",
    "        \n",
    "        if max_signals is not None and signal_count > max_signals:\n",
    "            self.logger.warning(f\"Zu viele Signale ({signal_count}), begrenze auf {max_signals}\")\n",
    "            \n",
    "            # Use the most frequent signals if there are too many\n",
    "            signal_counts = df.group_by(\"signal\").count().sort(by=\"count\", descending=True)\n",
    "            top_signals = signal_counts.head(max_signals).select(\"signal\")\n",
    "            \n",
    "            # Filter DataFrame for the most frequent signals\n",
    "            df = df.join(top_signals, on=\"signal\")\n",
    "            self.logger.info(f\"DataFrame auf {len(df)} Zeilen mit Top-{max_signals} Signalen reduziert\")\n",
    "        \n",
    "        try:\n",
    "            # Pivot the DataFrame\n",
    "            pivoted = df.pivot(\n",
    "                values=\"signal_value\",\n",
    "                index=[\"imo\", \"signal_timestamp\", \"loaddate\"],\n",
    "                columns=\"signal\"\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Pivotisierter DataFrame hat {len(pivoted)} Zeilen und {len(pivoted.columns)} Spalten\")\n",
    "            return pivoted\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Fehler bei der Pivotisierung: {str(e)}\")\n",
    "            \n",
    "            # Alternative method with explicit memory management\n",
    "            self.logger.info(\"Versuche alternative Pivotisierungsmethode...\")\n",
    "            \n",
    "            # Group by imo, timestamp, loaddate\n",
    "            pivoted_dfs = []\n",
    "            \n",
    "            for sig in df.select(\"signal\").unique().to_series():\n",
    "                try:\n",
    "                    # Filter for this signal\n",
    "                    signal_df = df.filter(pl.col(\"signal\") == sig)\n",
    "                    \n",
    "                    # Rename the signal_value column to the signal name\n",
    "                    renamed = signal_df.select(\n",
    "                        [\"imo\", \"signal_timestamp\", \"loaddate\", \n",
    "                         pl.col(\"signal_value\").alias(sig)]\n",
    "                    )\n",
    "                    \n",
    "                    pivoted_dfs.append(renamed)\n",
    "                except Exception as sub_e:\n",
    "                    self.logger.error(f\"Fehler bei der Verarbeitung von Signal {sig}: {str(sub_e)}\")\n",
    "            \n",
    "            if not pivoted_dfs:\n",
    "                self.logger.error(\"Keine Daten nach alternativer Pivotisierung\")\n",
    "                return pl.DataFrame()\n",
    "            \n",
    "            # Join all signal dataframes\n",
    "            result = pivoted_dfs[0]\n",
    "            for idx, signal_df in enumerate(pivoted_dfs[1:], 1):\n",
    "                if idx % 10 == 0:\n",
    "                    self.logger.info(f\"Verbinde Signal {idx} von {len(pivoted_dfs) - 1}\")\n",
    "                    \n",
    "                try:\n",
    "                    result = result.join(\n",
    "                        signal_df, \n",
    "                        on=[\"imo\", \"signal_timestamp\", \"loaddate\"], \n",
    "                        how=\"outer\"\n",
    "                    )\n",
    "                except Exception as join_e:\n",
    "                    self.logger.error(f\"Fehler beim Verbinden von Signal {idx}: {str(join_e)}\")\n",
    "            \n",
    "            self.logger.info(f\"Alternative Pivotisierung: {len(result)} Zeilen, {len(result.columns)} Spalten\")\n",
    "            return result\n",
    "\n",
    "    def export_pivoted_data(self, output_path: str, max_days: int = None, max_signals: int = None) -> str:\n",
    "        \"\"\"\n",
    "        Main method to process and export pivoted timeseries data\n",
    "        \n",
    "        Args:\n",
    "            output_path: Where to save the pivoted data\n",
    "            max_days: Optional, maximum number of days to process\n",
    "            max_signals: Optional, maximum number of signals to include in pivoting\n",
    "            \n",
    "        Returns:\n",
    "            Path to output file if successful, None otherwise\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        self.logger.info(f\"Starte Verarbeitung um {start_time}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Find timeseries files\n",
    "            ts_files = self.find_timeseries_files(self.config.transformed_path, max_days)\n",
    "            \n",
    "            if not ts_files:\n",
    "                self.logger.error(\"Keine Timeseries-Dateien gefunden\")\n",
    "                return None\n",
    "                \n",
    "            # 2. Load and combine data\n",
    "            combined_df = self.load_and_combine_timeseries(ts_files)\n",
    "            \n",
    "            if len(combined_df) == 0:\n",
    "                self.logger.error(\"Keine Daten geladen\")\n",
    "                return None\n",
    "                \n",
    "            # 3. Clean and deduplicate data\n",
    "            clean_df = self.clean_and_deduplicate(combined_df)\n",
    "            \n",
    "            if len(clean_df) == 0:\n",
    "                self.logger.error(\"Keine Daten nach Bereinigung\")\n",
    "                return None\n",
    "                \n",
    "            # 4. Pivot data\n",
    "            pivoted_df = self.pivot_timeseries(clean_df, max_signals)\n",
    "            \n",
    "            if len(pivoted_df) == 0:\n",
    "                self.logger.error(\"Keine Daten nach Pivotisierung\")\n",
    "                return None\n",
    "                \n",
    "            # 5. Save result\n",
    "            # Create output directory if it doesn't exist\n",
    "            output_dir = os.path.dirname(output_path)\n",
    "            if output_dir:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "            pivoted_df.write_parquet(output_path, compression=\"snappy\")\n",
    "            self.logger.info(f\"Pivotisierte Daten gespeichert nach {output_path}\")\n",
    "            \n",
    "            # 6. Calculate statistics\n",
    "            stats = {\n",
    "                \"Anzahl der Zeilen\": len(pivoted_df),\n",
    "                \"Anzahl der Spalten\": len(pivoted_df.columns),\n",
    "                \"Eindeutige IMOs\": len(pivoted_df.select(\"imo\").unique()),\n",
    "                \"Zeitraum\": f\"{pivoted_df.select('signal_timestamp').min()} bis {pivoted_df.select('signal_timestamp').max()}\"\n",
    "            }\n",
    "            \n",
    "            self.logger.info(\"Statistiken der pivotisierten Daten:\")\n",
    "            for key, value in stats.items():\n",
    "                self.logger.info(f\"  {key}: {value}\")\n",
    "                \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Fehler bei der Verarbeitung: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # End timing\n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            self.logger.info(f\"Verarbeitung abgeschlossen nach {duration}, Endzeit: {end_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Starten\n",
    "Hauptfunktionsaufruf für die Pipeline\n",
    "\n",
    "mode (str): 'all', 'timeseries', oder 'fleet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = \"all\"\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if environment variables are set\n",
    "api_key = os.getenv('HOPPE_API_KEY')\n",
    "if not api_key:\n",
    "    logger.error(\"HOPPE_API_KEY environment variable not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "config = Config(\n",
    "    base_url=os.getenv('HOPPE_BASE_URL', \"https://api.hoppe-sts.com/\"),\n",
    "    raw_path=os.getenv('RAW_PATH', \"./data/raw_data\"),\n",
    "    transformed_path=os.getenv('TRANSFORMED_PATH', \"./data/transformed_data\"),\n",
    "    gaps_path=os.getenv('GAPS_PATH', \"./data/gaps_data\"),\n",
    "    max_workers=int(os.getenv('MAX_WORKERS', \"4\")),\n",
    "    retry_attempts=int(os.getenv('RETRY_ATTEMPTS', \"10\")),\n",
    "    timeout=int(os.getenv('TIMEOUT', \"45\")),\n",
    "    days_to_keep=int(os.getenv('DAYS_TO_KEEP', \"90\")),\n",
    "    history_days=int(os.getenv('HISTORY_DAYS', \"5\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 18:00:33,872 - hoppe_etl_pipeline - WARNING - API_WARNING|9400071|timeseries|Empty timeseries data for ship 9400071\n",
      "2025-03-12 18:00:33,899 - hoppe_etl_pipeline - WARNING - API_WARNING|9778399|timeseries|Empty timeseries data for ship 9778399\n",
      "2025-03-12 18:00:34,542 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/12/17/00/Timeseries_9725512.json\n",
      "2025-03-12 18:00:34,608 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9725512\n",
      "2025-03-12 18:00:34,614 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/12/17/00/Timeseries_9725512.parquet\n",
      "2025-03-12 18:00:34,794 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/12/17/00/Gaps_9725512.parquet\n",
      "2025-03-12 18:00:35,387 - urllib3.connectionpool - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9725524/fc549dd2-edad-4b60-94f5-7d5cb7ac032b.json?AWSAccessKeyId=ASIAU3FAITLP2WCF4CJT&Signature=QDVEa1A0Im6aU4gphddO9g5RE%2BQ%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHgaCWV1LXdlc3QtMSJHMEUCIQCMvkYiNK01QkMQMmMoHM39l4TUX2h4JipEqqXj1lHGowIgRmZNUpVnNXiUjhoUIWYNPuAYxuG7NPVl0fy4frFVT2sqjwMIwf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDAiWIQMRNTUCBx50tyrjAr%2B57K0PrBRm2c65UQzjUWAZrVSFNhdFBwQd6p2m6hkbhYtBxJxClsM69dxpoaZwjh5QjgK%2BSc8mAHnuBcHDud%2Fqadu%2FqVl0ZP4FLtNogM9q5G4vI%2FCxvNoIO7Sq6mwUykGln5srS%2BpEcRCR%2F6mFfZaDYc8kizMj5zWQKEf1OKCsmhV6hhiMHlZKEB9JjspTpcOEBzIoYL%2FvvSFgjUkhZSWcjvNAP880SJotUuwUbB2RFJKjpl%2BOD0NlQSGShxM78DIfs%2Bz2MM9LrDy9lXrw6RTpnJ8xkAAVFYMMuUgoPwBDS5uT4WdaKPY6CQUQ213ajjxvgIdvUHQoDs0D9wu2brIcyZLvSSHQP%2FvH18%2FXyJRPH7FBjWkrE5QatdVjWO0uVg%2FO3usSN3XVp%2Baa18Iud2GBS33Z%2FAOcdzf17MXHb9wEDEpJH3Lerfy7w%2FeKglAtvfa0hAxX5v4g6YNwvevyFx%2B3BCkwld3GvgY6ngE3aV9dAnZy9zB%2BhDzUX4P5JlclT2Bq4pc%2Bxb%2Bb6LB9CBdodGsWE4nTRRB0tQb6EqVeQOk8oK3U1FYEbDA43uY0NkPWgNUwSTb72jOinICAEQe7AUfKjZG2AHn%2BU7Ftatt9GXXUkTMtkhBgCNelHzGjIoMfhVMHjAC79M3RJrVAMAdvq5gsoq113YHJh7e2o5bDPIqRSXEjLAMIUSABNQ%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:00:35,468 - urllib3.connectionpool - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693290/53ffc4ba-c499-4993-964d-a9f1daa546ed.json?AWSAccessKeyId=ASIAU3FAITLPQUBGFEQC&Signature=rIs%2F4KlaLqOb6Nmsyb68s1vY0lo%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJIMEYCIQD8JwYvSFFYTWX3nnHrNX0pEE%2BjHW2TduSFhEYESC3Y%2FgIhAIV%2FivODh7Vb7eCWWfr0ooSZBsPNNhmDGsV8kWCf3B9QKo8DCML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMzMzMTk2MDczNjk1IgzVoFEZ51gzeOCPeIEq4wLEbabTORW9VQygM0re8H7HtS%2FS%2B%2FUIe6NPHGnTGhnTxabt3i%2B2UzyW0GRRgUb3%2BzoqsAKCAoxeEmA1hh7K9jlyWpmjOWAM%2BjvQqYPjjV1y7lvLb6hzhRWzcNAHbA6WCilzNkxKxndbPnZp4s65Q0l%2B470LtE%2Fp13MYapWWzPbfmm9IpRWXe3NTtQh7U%2FbIK85l860QVbX5nzsgp%2BGpn6WFj27zBdXN%2FS215PnlTYDlD7cEo1WrmCLrl6SG63mW9%2F%2BCUXpSOWh4xaMDhoEyF85aUzfsKD7RVghS4PKr4GqedCAdAIAfeA78FVMNal1uHTTvtSW7duUqjEF4win%2BNKPPu2GYKtfI9pFxc%2BOVzDM8xqONuvmQqle6Lrm54hoqKafV43FqMi7%2B5LLowo%2BjVZ9WCqmaQowxzwBU%2BLWEiIAS9KysV0v0WHEWbWBizOolmMFQwm35Acy72Gi%2BITcwpYsALnApMK7yxr4GOp0BJOeHPVWX2I%2FyiudXFVDYbmHSm6KP5CfprHvBp13VcSmxcy33TEylKM32O6xW4NyYWkmYX%2BcGcjL1%2Bggny3TiDO7zAA7Jel18CACdSjTVL8kl64s6monZ2Vu9sAbvVodlxDf5ZwugQy75fv03LZO0jqSBJyo2iqaBiV1p2t1SSYbSDsI0QGz14BvQpHhKn0v7HwbdDgzblx3L87ZCIA%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:00:35,791 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/12/17/00/Timeseries_9306184.json\n",
      "2025-03-12 18:00:35,934 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9306184\n",
      "2025-03-12 18:00:35,941 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/12/17/00/Timeseries_9306184.parquet\n",
      "2025-03-12 18:00:36,023 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/12/17/00/Gaps_9306184.parquet\n",
      "2025-03-12 18:00:36,175 - hoppe_etl_pipeline - WARNING - API_WARNING|9693331|timeseries|Empty timeseries data for ship 9693331\n",
      "2025-03-12 18:00:37,123 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/12/17/00/Timeseries_9306287.json\n",
      "2025-03-12 18:00:37,244 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9306287\n",
      "2025-03-12 18:00:37,252 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/12/17/00/Timeseries_9306287.parquet\n",
      "2025-03-12 18:00:37,457 - urllib3.connectionpool - WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9725524/fc549dd2-edad-4b60-94f5-7d5cb7ac032b.json?AWSAccessKeyId=ASIAU3FAITLP2WCF4CJT&Signature=QDVEa1A0Im6aU4gphddO9g5RE%2BQ%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHgaCWV1LXdlc3QtMSJHMEUCIQCMvkYiNK01QkMQMmMoHM39l4TUX2h4JipEqqXj1lHGowIgRmZNUpVnNXiUjhoUIWYNPuAYxuG7NPVl0fy4frFVT2sqjwMIwf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDAiWIQMRNTUCBx50tyrjAr%2B57K0PrBRm2c65UQzjUWAZrVSFNhdFBwQd6p2m6hkbhYtBxJxClsM69dxpoaZwjh5QjgK%2BSc8mAHnuBcHDud%2Fqadu%2FqVl0ZP4FLtNogM9q5G4vI%2FCxvNoIO7Sq6mwUykGln5srS%2BpEcRCR%2F6mFfZaDYc8kizMj5zWQKEf1OKCsmhV6hhiMHlZKEB9JjspTpcOEBzIoYL%2FvvSFgjUkhZSWcjvNAP880SJotUuwUbB2RFJKjpl%2BOD0NlQSGShxM78DIfs%2Bz2MM9LrDy9lXrw6RTpnJ8xkAAVFYMMuUgoPwBDS5uT4WdaKPY6CQUQ213ajjxvgIdvUHQoDs0D9wu2brIcyZLvSSHQP%2FvH18%2FXyJRPH7FBjWkrE5QatdVjWO0uVg%2FO3usSN3XVp%2Baa18Iud2GBS33Z%2FAOcdzf17MXHb9wEDEpJH3Lerfy7w%2FeKglAtvfa0hAxX5v4g6YNwvevyFx%2B3BCkwld3GvgY6ngE3aV9dAnZy9zB%2BhDzUX4P5JlclT2Bq4pc%2Bxb%2Bb6LB9CBdodGsWE4nTRRB0tQb6EqVeQOk8oK3U1FYEbDA43uY0NkPWgNUwSTb72jOinICAEQe7AUfKjZG2AHn%2BU7Ftatt9GXXUkTMtkhBgCNelHzGjIoMfhVMHjAC79M3RJrVAMAdvq5gsoq113YHJh7e2o5bDPIqRSXEjLAMIUSABNQ%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:00:37,458 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/12/17/00/Gaps_9306287.parquet\n",
      "2025-03-12 18:00:37,528 - urllib3.connectionpool - WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693290/53ffc4ba-c499-4993-964d-a9f1daa546ed.json?AWSAccessKeyId=ASIAU3FAITLPQUBGFEQC&Signature=rIs%2F4KlaLqOb6Nmsyb68s1vY0lo%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJIMEYCIQD8JwYvSFFYTWX3nnHrNX0pEE%2BjHW2TduSFhEYESC3Y%2FgIhAIV%2FivODh7Vb7eCWWfr0ooSZBsPNNhmDGsV8kWCf3B9QKo8DCML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMzMzMTk2MDczNjk1IgzVoFEZ51gzeOCPeIEq4wLEbabTORW9VQygM0re8H7HtS%2FS%2B%2FUIe6NPHGnTGhnTxabt3i%2B2UzyW0GRRgUb3%2BzoqsAKCAoxeEmA1hh7K9jlyWpmjOWAM%2BjvQqYPjjV1y7lvLb6hzhRWzcNAHbA6WCilzNkxKxndbPnZp4s65Q0l%2B470LtE%2Fp13MYapWWzPbfmm9IpRWXe3NTtQh7U%2FbIK85l860QVbX5nzsgp%2BGpn6WFj27zBdXN%2FS215PnlTYDlD7cEo1WrmCLrl6SG63mW9%2F%2BCUXpSOWh4xaMDhoEyF85aUzfsKD7RVghS4PKr4GqedCAdAIAfeA78FVMNal1uHTTvtSW7duUqjEF4win%2BNKPPu2GYKtfI9pFxc%2BOVzDM8xqONuvmQqle6Lrm54hoqKafV43FqMi7%2B5LLowo%2BjVZ9WCqmaQowxzwBU%2BLWEiIAS9KysV0v0WHEWbWBizOolmMFQwm35Acy72Gi%2BITcwpYsALnApMK7yxr4GOp0BJOeHPVWX2I%2FyiudXFVDYbmHSm6KP5CfprHvBp13VcSmxcy33TEylKM32O6xW4NyYWkmYX%2BcGcjL1%2Bggny3TiDO7zAA7Jel18CACdSjTVL8kl64s6monZ2Vu9sAbvVodlxDf5ZwugQy75fv03LZO0jqSBJyo2iqaBiV1p2t1SSYbSDsI0QGz14BvQpHhKn0v7HwbdDgzblx3L87ZCIA%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:00:38,384 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/12/17/00/Timeseries_9306160.json\n",
      "2025-03-12 18:00:38,469 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9306160\n",
      "2025-03-12 18:00:38,475 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/12/17/00/Timeseries_9306160.parquet\n",
      "2025-03-12 18:00:38,568 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/12/17/00/Gaps_9306160.parquet\n",
      "2025-03-12 18:00:39,258 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/12/17/00/Timeseries_9696838.json\n",
      "2025-03-12 18:00:39,321 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9696838\n",
      "2025-03-12 18:00:39,327 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/12/17/00/Timeseries_9696838.parquet\n",
      "2025-03-12 18:00:39,382 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/12/17/00/Gaps_9696838.parquet\n",
      "2025-03-12 18:00:41,525 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9725524/fc549dd2-edad-4b60-94f5-7d5cb7ac032b.json?AWSAccessKeyId=ASIAU3FAITLP2WCF4CJT&Signature=QDVEa1A0Im6aU4gphddO9g5RE%2BQ%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHgaCWV1LXdlc3QtMSJHMEUCIQCMvkYiNK01QkMQMmMoHM39l4TUX2h4JipEqqXj1lHGowIgRmZNUpVnNXiUjhoUIWYNPuAYxuG7NPVl0fy4frFVT2sqjwMIwf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDAiWIQMRNTUCBx50tyrjAr%2B57K0PrBRm2c65UQzjUWAZrVSFNhdFBwQd6p2m6hkbhYtBxJxClsM69dxpoaZwjh5QjgK%2BSc8mAHnuBcHDud%2Fqadu%2FqVl0ZP4FLtNogM9q5G4vI%2FCxvNoIO7Sq6mwUykGln5srS%2BpEcRCR%2F6mFfZaDYc8kizMj5zWQKEf1OKCsmhV6hhiMHlZKEB9JjspTpcOEBzIoYL%2FvvSFgjUkhZSWcjvNAP880SJotUuwUbB2RFJKjpl%2BOD0NlQSGShxM78DIfs%2Bz2MM9LrDy9lXrw6RTpnJ8xkAAVFYMMuUgoPwBDS5uT4WdaKPY6CQUQ213ajjxvgIdvUHQoDs0D9wu2brIcyZLvSSHQP%2FvH18%2FXyJRPH7FBjWkrE5QatdVjWO0uVg%2FO3usSN3XVp%2Baa18Iud2GBS33Z%2FAOcdzf17MXHb9wEDEpJH3Lerfy7w%2FeKglAtvfa0hAxX5v4g6YNwvevyFx%2B3BCkwld3GvgY6ngE3aV9dAnZy9zB%2BhDzUX4P5JlclT2Bq4pc%2Bxb%2Bb6LB9CBdodGsWE4nTRRB0tQb6EqVeQOk8oK3U1FYEbDA43uY0NkPWgNUwSTb72jOinICAEQe7AUfKjZG2AHn%2BU7Ftatt9GXXUkTMtkhBgCNelHzGjIoMfhVMHjAC79M3RJrVAMAdvq5gsoq113YHJh7e2o5bDPIqRSXEjLAMIUSABNQ%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:00:41,595 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693290/53ffc4ba-c499-4993-964d-a9f1daa546ed.json?AWSAccessKeyId=ASIAU3FAITLPQUBGFEQC&Signature=rIs%2F4KlaLqOb6Nmsyb68s1vY0lo%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJIMEYCIQD8JwYvSFFYTWX3nnHrNX0pEE%2BjHW2TduSFhEYESC3Y%2FgIhAIV%2FivODh7Vb7eCWWfr0ooSZBsPNNhmDGsV8kWCf3B9QKo8DCML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMzMzMTk2MDczNjk1IgzVoFEZ51gzeOCPeIEq4wLEbabTORW9VQygM0re8H7HtS%2FS%2B%2FUIe6NPHGnTGhnTxabt3i%2B2UzyW0GRRgUb3%2BzoqsAKCAoxeEmA1hh7K9jlyWpmjOWAM%2BjvQqYPjjV1y7lvLb6hzhRWzcNAHbA6WCilzNkxKxndbPnZp4s65Q0l%2B470LtE%2Fp13MYapWWzPbfmm9IpRWXe3NTtQh7U%2FbIK85l860QVbX5nzsgp%2BGpn6WFj27zBdXN%2FS215PnlTYDlD7cEo1WrmCLrl6SG63mW9%2F%2BCUXpSOWh4xaMDhoEyF85aUzfsKD7RVghS4PKr4GqedCAdAIAfeA78FVMNal1uHTTvtSW7duUqjEF4win%2BNKPPu2GYKtfI9pFxc%2BOVzDM8xqONuvmQqle6Lrm54hoqKafV43FqMi7%2B5LLowo%2BjVZ9WCqmaQowxzwBU%2BLWEiIAS9KysV0v0WHEWbWBizOolmMFQwm35Acy72Gi%2BITcwpYsALnApMK7yxr4GOp0BJOeHPVWX2I%2FyiudXFVDYbmHSm6KP5CfprHvBp13VcSmxcy33TEylKM32O6xW4NyYWkmYX%2BcGcjL1%2Bggny3TiDO7zAA7Jel18CACdSjTVL8kl64s6monZ2Vu9sAbvVodlxDf5ZwugQy75fv03LZO0jqSBJyo2iqaBiV1p2t1SSYbSDsI0QGz14BvQpHhKn0v7HwbdDgzblx3L87ZCIA%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:00:42,644 - hoppe_etl_pipeline - INFO - Data saved to ./data/raw_data/2025/03/12/17/00/Timeseries_9696826.json\n",
      "2025-03-12 18:00:42,812 - hoppe_etl_pipeline - INFO - No historical timeseries data found for ship 9696826\n",
      "2025-03-12 18:00:42,819 - hoppe_etl_pipeline - INFO - Data saved to ./data/transformed_data/2025/03/12/17/00/Timeseries_9696826.parquet\n",
      "2025-03-12 18:00:43,134 - hoppe_etl_pipeline - INFO - Data saved to ./data/gaps_data/2025/03/12/17/00/Gaps_9696826.parquet\n",
      "2025-03-12 18:00:43,228 - hoppe_etl_pipeline - WARNING - API_WARNING|9395525|timeseries|Empty timeseries data for ship 9395525\n",
      "2025-03-12 18:00:45,245 - urllib3.connectionpool - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693329/9c07d7af-f836-43b2-b80c-49fb0a07474b.json?AWSAccessKeyId=ASIAU3FAITLPSFNF5NFY&Signature=reObIHxae6FA%2FGfNtFo4iYNSqdA%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCID4zsOz%2BX7bcommmebuNUwJTxgZsOZZA2Mc4EfVdXP5ZAiEAmfZZlBEGnutekonbRo6muj5s1gDaEJdwIzZdyS7hgjAqjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDDqDvlR36UwKIaziryrjAhwU0EtMqDmT9kNIK0MyLtASKbl1ETmYy8GpSoDph4pFGTCaPLjvPqRd5nZ0j4coQA1K789hJok2DWan5A0nPDCUzE5pAW3uZ57fQaBqVYKxVQ8c5TfcUTGzObNgb1z%2BchpF%2BGMui8pAOeKwsx0c3U0imGGiB7KCWxWLIWD4M78WK4ivXexFyBE0aYlA2o7JaqFUaaY1r40wIVJIAKPSIqmX03EHSqfoNhqPtdbsGaECdZycbPkPmqOn8nArWQlQmMuuek0RF7aIMZR%2B9wX7EtwQdhOzmd5uZKXHWCwfw%2FhK%2BYFxh4AT1z%2B7ku0ZNA%2Bsd2W15cNoaxFx8BalTTtbRvWktEpd2jkp%2FNUA2TB4%2BsBBYrnC%2FlNpqMO4NL0RaGxj45neuPL1%2BpNwIpmolvWL71yI8ESwPiGhRoYg6GXfMwlBL7WGk3DwA3ljrJl0LZmEZCD9%2BdUBhP7s5AXAebOXIbz4f9Aw8PnGvgY6ngGsKnEHiZGK0wnklobaTjOJ7a1UhSTAxdvYVH%2BPYIXtSwUT6gAumbs7GhSCUTnX0CtvXR5479IgM9BSDJnWsJ9DCnNl6CmcHg1ohAy1fqcd18bQ9M8VBi9eu%2F9Fli805KBLuTietOi4WDtY6f9ivKVVPnWVJ0yz7MmGBHC8cpWEBGHvIMyEaKXP9Qo0Se1PsIYfxznJMFSrcUS%2FG8kp7Q%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:00:45,283 - urllib3.connectionpool - WARNING - Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9726566/6ac7fcce-075d-46dc-a8c8-58c967e74097.json?AWSAccessKeyId=ASIAU3FAITLP7CVGPW7T&Signature=HGeWYhQjO2vilONFH6nHMRasFNk%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCIDzI6njr%2B7WvSAaIa5syTx%2Bq82VWX2O%2FU2nHkW1s4aGwAiEAw%2BB27agLpyCi8co3ca57il3%2BAt5No8J1k1y7qEfO3S0qjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDFPd9F%2FoyxXGYi6r2CrjApusXU5HIgNGKVOAaee4S7q2qLCr7E0dxmbVZ93QR9n7D7idGQ%2BYMK2nInkGB8xXVK7TO5%2F%2BHUzyap1bPUFQl9KrqT%2BnDmO6M7o%2BR2lVPZnxiHsTT1Fyrvtf%2Bx5sx0MIAHBlCGj3unbR6JZEJiQpmqjskRk7Vxp2zMK09HdhxIUgzsmxoew7GiRQPlMkTckhmzMQVF1Zn2kMIYaF6YYOZyMVd1ScgorNcNmOvaJH07QTPa8XeLNlOaeKbWyfaMERJp41U5874ILUblYEiNFNFZ9%2FGqlfNkTmVX5giDbtY%2F%2BoOQ09QebMd2GwTO4VtUlGtXw4%2FOuTaLJF2Z23gZyf%2BHwgfq0DENctlg3HXODq2wKhkQWwoEuUQwE3egz6gi5rYaPR6FiesCcy090SZ8VAcAIl1HjhocmoBwMJG8QJjzWaO24Km%2BIBNXyAH4YYyEu8yDwf80aUswQE4%2B4hFjA2noybmhowgPvGvgY6ngHFbK6FQ3Vw0vhGon5RvJv5ZjrzsXvYU7VYIReXNM26R0Au9TP3ACW5fHu58E0mIxu8vKCYM14vlUqAA%2FhrBLzXzNMJLFstjDFB0trYRUYNQg97M38Dw1LtW1ZgXr2kX5WO8xUPSIIuQ0j9562WHnl15n1zIbIR5d5EilN4hDwJhldJ9cWr9ngudc5vk31gs3T49w5byhAScxloP9%2FT%2FQ%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:00:47,312 - urllib3.connectionpool - WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693329/9c07d7af-f836-43b2-b80c-49fb0a07474b.json?AWSAccessKeyId=ASIAU3FAITLPSFNF5NFY&Signature=reObIHxae6FA%2FGfNtFo4iYNSqdA%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCID4zsOz%2BX7bcommmebuNUwJTxgZsOZZA2Mc4EfVdXP5ZAiEAmfZZlBEGnutekonbRo6muj5s1gDaEJdwIzZdyS7hgjAqjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDDqDvlR36UwKIaziryrjAhwU0EtMqDmT9kNIK0MyLtASKbl1ETmYy8GpSoDph4pFGTCaPLjvPqRd5nZ0j4coQA1K789hJok2DWan5A0nPDCUzE5pAW3uZ57fQaBqVYKxVQ8c5TfcUTGzObNgb1z%2BchpF%2BGMui8pAOeKwsx0c3U0imGGiB7KCWxWLIWD4M78WK4ivXexFyBE0aYlA2o7JaqFUaaY1r40wIVJIAKPSIqmX03EHSqfoNhqPtdbsGaECdZycbPkPmqOn8nArWQlQmMuuek0RF7aIMZR%2B9wX7EtwQdhOzmd5uZKXHWCwfw%2FhK%2BYFxh4AT1z%2B7ku0ZNA%2Bsd2W15cNoaxFx8BalTTtbRvWktEpd2jkp%2FNUA2TB4%2BsBBYrnC%2FlNpqMO4NL0RaGxj45neuPL1%2BpNwIpmolvWL71yI8ESwPiGhRoYg6GXfMwlBL7WGk3DwA3ljrJl0LZmEZCD9%2BdUBhP7s5AXAebOXIbz4f9Aw8PnGvgY6ngGsKnEHiZGK0wnklobaTjOJ7a1UhSTAxdvYVH%2BPYIXtSwUT6gAumbs7GhSCUTnX0CtvXR5479IgM9BSDJnWsJ9DCnNl6CmcHg1ohAy1fqcd18bQ9M8VBi9eu%2F9Fli805KBLuTietOi4WDtY6f9ivKVVPnWVJ0yz7MmGBHC8cpWEBGHvIMyEaKXP9Qo0Se1PsIYfxznJMFSrcUS%2FG8kp7Q%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:00:47,349 - urllib3.connectionpool - WARNING - Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9726566/6ac7fcce-075d-46dc-a8c8-58c967e74097.json?AWSAccessKeyId=ASIAU3FAITLP7CVGPW7T&Signature=HGeWYhQjO2vilONFH6nHMRasFNk%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCIDzI6njr%2B7WvSAaIa5syTx%2Bq82VWX2O%2FU2nHkW1s4aGwAiEAw%2BB27agLpyCi8co3ca57il3%2BAt5No8J1k1y7qEfO3S0qjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDFPd9F%2FoyxXGYi6r2CrjApusXU5HIgNGKVOAaee4S7q2qLCr7E0dxmbVZ93QR9n7D7idGQ%2BYMK2nInkGB8xXVK7TO5%2F%2BHUzyap1bPUFQl9KrqT%2BnDmO6M7o%2BR2lVPZnxiHsTT1Fyrvtf%2Bx5sx0MIAHBlCGj3unbR6JZEJiQpmqjskRk7Vxp2zMK09HdhxIUgzsmxoew7GiRQPlMkTckhmzMQVF1Zn2kMIYaF6YYOZyMVd1ScgorNcNmOvaJH07QTPa8XeLNlOaeKbWyfaMERJp41U5874ILUblYEiNFNFZ9%2FGqlfNkTmVX5giDbtY%2F%2BoOQ09QebMd2GwTO4VtUlGtXw4%2FOuTaLJF2Z23gZyf%2BHwgfq0DENctlg3HXODq2wKhkQWwoEuUQwE3egz6gi5rYaPR6FiesCcy090SZ8VAcAIl1HjhocmoBwMJG8QJjzWaO24Km%2BIBNXyAH4YYyEu8yDwf80aUswQE4%2B4hFjA2noybmhowgPvGvgY6ngHFbK6FQ3Vw0vhGon5RvJv5ZjrzsXvYU7VYIReXNM26R0Au9TP3ACW5fHu58E0mIxu8vKCYM14vlUqAA%2FhrBLzXzNMJLFstjDFB0trYRUYNQg97M38Dw1LtW1ZgXr2kX5WO8xUPSIIuQ0j9562WHnl15n1zIbIR5d5EilN4hDwJhldJ9cWr9ngudc5vk31gs3T49w5byhAScxloP9%2FT%2FQ%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:00:49,673 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9725524/fc549dd2-edad-4b60-94f5-7d5cb7ac032b.json?AWSAccessKeyId=ASIAU3FAITLP2WCF4CJT&Signature=QDVEa1A0Im6aU4gphddO9g5RE%2BQ%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHgaCWV1LXdlc3QtMSJHMEUCIQCMvkYiNK01QkMQMmMoHM39l4TUX2h4JipEqqXj1lHGowIgRmZNUpVnNXiUjhoUIWYNPuAYxuG7NPVl0fy4frFVT2sqjwMIwf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDAiWIQMRNTUCBx50tyrjAr%2B57K0PrBRm2c65UQzjUWAZrVSFNhdFBwQd6p2m6hkbhYtBxJxClsM69dxpoaZwjh5QjgK%2BSc8mAHnuBcHDud%2Fqadu%2FqVl0ZP4FLtNogM9q5G4vI%2FCxvNoIO7Sq6mwUykGln5srS%2BpEcRCR%2F6mFfZaDYc8kizMj5zWQKEf1OKCsmhV6hhiMHlZKEB9JjspTpcOEBzIoYL%2FvvSFgjUkhZSWcjvNAP880SJotUuwUbB2RFJKjpl%2BOD0NlQSGShxM78DIfs%2Bz2MM9LrDy9lXrw6RTpnJ8xkAAVFYMMuUgoPwBDS5uT4WdaKPY6CQUQ213ajjxvgIdvUHQoDs0D9wu2brIcyZLvSSHQP%2FvH18%2FXyJRPH7FBjWkrE5QatdVjWO0uVg%2FO3usSN3XVp%2Baa18Iud2GBS33Z%2FAOcdzf17MXHb9wEDEpJH3Lerfy7w%2FeKglAtvfa0hAxX5v4g6YNwvevyFx%2B3BCkwld3GvgY6ngE3aV9dAnZy9zB%2BhDzUX4P5JlclT2Bq4pc%2Bxb%2Bb6LB9CBdodGsWE4nTRRB0tQb6EqVeQOk8oK3U1FYEbDA43uY0NkPWgNUwSTb72jOinICAEQe7AUfKjZG2AHn%2BU7Ftatt9GXXUkTMtkhBgCNelHzGjIoMfhVMHjAC79M3RJrVAMAdvq5gsoq113YHJh7e2o5bDPIqRSXEjLAMIUSABNQ%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:00:49,673 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693290/53ffc4ba-c499-4993-964d-a9f1daa546ed.json?AWSAccessKeyId=ASIAU3FAITLPQUBGFEQC&Signature=rIs%2F4KlaLqOb6Nmsyb68s1vY0lo%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJIMEYCIQD8JwYvSFFYTWX3nnHrNX0pEE%2BjHW2TduSFhEYESC3Y%2FgIhAIV%2FivODh7Vb7eCWWfr0ooSZBsPNNhmDGsV8kWCf3B9QKo8DCML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMzMzMTk2MDczNjk1IgzVoFEZ51gzeOCPeIEq4wLEbabTORW9VQygM0re8H7HtS%2FS%2B%2FUIe6NPHGnTGhnTxabt3i%2B2UzyW0GRRgUb3%2BzoqsAKCAoxeEmA1hh7K9jlyWpmjOWAM%2BjvQqYPjjV1y7lvLb6hzhRWzcNAHbA6WCilzNkxKxndbPnZp4s65Q0l%2B470LtE%2Fp13MYapWWzPbfmm9IpRWXe3NTtQh7U%2FbIK85l860QVbX5nzsgp%2BGpn6WFj27zBdXN%2FS215PnlTYDlD7cEo1WrmCLrl6SG63mW9%2F%2BCUXpSOWh4xaMDhoEyF85aUzfsKD7RVghS4PKr4GqedCAdAIAfeA78FVMNal1uHTTvtSW7duUqjEF4win%2BNKPPu2GYKtfI9pFxc%2BOVzDM8xqONuvmQqle6Lrm54hoqKafV43FqMi7%2B5LLowo%2BjVZ9WCqmaQowxzwBU%2BLWEiIAS9KysV0v0WHEWbWBizOolmMFQwm35Acy72Gi%2BITcwpYsALnApMK7yxr4GOp0BJOeHPVWX2I%2FyiudXFVDYbmHSm6KP5CfprHvBp13VcSmxcy33TEylKM32O6xW4NyYWkmYX%2BcGcjL1%2Bggny3TiDO7zAA7Jel18CACdSjTVL8kl64s6monZ2Vu9sAbvVodlxDf5ZwugQy75fv03LZO0jqSBJyo2iqaBiV1p2t1SSYbSDsI0QGz14BvQpHhKn0v7HwbdDgzblx3L87ZCIA%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:00:51,377 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693329/9c07d7af-f836-43b2-b80c-49fb0a07474b.json?AWSAccessKeyId=ASIAU3FAITLPSFNF5NFY&Signature=reObIHxae6FA%2FGfNtFo4iYNSqdA%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCID4zsOz%2BX7bcommmebuNUwJTxgZsOZZA2Mc4EfVdXP5ZAiEAmfZZlBEGnutekonbRo6muj5s1gDaEJdwIzZdyS7hgjAqjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDDqDvlR36UwKIaziryrjAhwU0EtMqDmT9kNIK0MyLtASKbl1ETmYy8GpSoDph4pFGTCaPLjvPqRd5nZ0j4coQA1K789hJok2DWan5A0nPDCUzE5pAW3uZ57fQaBqVYKxVQ8c5TfcUTGzObNgb1z%2BchpF%2BGMui8pAOeKwsx0c3U0imGGiB7KCWxWLIWD4M78WK4ivXexFyBE0aYlA2o7JaqFUaaY1r40wIVJIAKPSIqmX03EHSqfoNhqPtdbsGaECdZycbPkPmqOn8nArWQlQmMuuek0RF7aIMZR%2B9wX7EtwQdhOzmd5uZKXHWCwfw%2FhK%2BYFxh4AT1z%2B7ku0ZNA%2Bsd2W15cNoaxFx8BalTTtbRvWktEpd2jkp%2FNUA2TB4%2BsBBYrnC%2FlNpqMO4NL0RaGxj45neuPL1%2BpNwIpmolvWL71yI8ESwPiGhRoYg6GXfMwlBL7WGk3DwA3ljrJl0LZmEZCD9%2BdUBhP7s5AXAebOXIbz4f9Aw8PnGvgY6ngGsKnEHiZGK0wnklobaTjOJ7a1UhSTAxdvYVH%2BPYIXtSwUT6gAumbs7GhSCUTnX0CtvXR5479IgM9BSDJnWsJ9DCnNl6CmcHg1ohAy1fqcd18bQ9M8VBi9eu%2F9Fli805KBLuTietOi4WDtY6f9ivKVVPnWVJ0yz7MmGBHC8cpWEBGHvIMyEaKXP9Qo0Se1PsIYfxznJMFSrcUS%2FG8kp7Q%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:00:51,414 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9726566/6ac7fcce-075d-46dc-a8c8-58c967e74097.json?AWSAccessKeyId=ASIAU3FAITLP7CVGPW7T&Signature=HGeWYhQjO2vilONFH6nHMRasFNk%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCIDzI6njr%2B7WvSAaIa5syTx%2Bq82VWX2O%2FU2nHkW1s4aGwAiEAw%2BB27agLpyCi8co3ca57il3%2BAt5No8J1k1y7qEfO3S0qjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDFPd9F%2FoyxXGYi6r2CrjApusXU5HIgNGKVOAaee4S7q2qLCr7E0dxmbVZ93QR9n7D7idGQ%2BYMK2nInkGB8xXVK7TO5%2F%2BHUzyap1bPUFQl9KrqT%2BnDmO6M7o%2BR2lVPZnxiHsTT1Fyrvtf%2Bx5sx0MIAHBlCGj3unbR6JZEJiQpmqjskRk7Vxp2zMK09HdhxIUgzsmxoew7GiRQPlMkTckhmzMQVF1Zn2kMIYaF6YYOZyMVd1ScgorNcNmOvaJH07QTPa8XeLNlOaeKbWyfaMERJp41U5874ILUblYEiNFNFZ9%2FGqlfNkTmVX5giDbtY%2F%2BoOQ09QebMd2GwTO4VtUlGtXw4%2FOuTaLJF2Z23gZyf%2BHwgfq0DENctlg3HXODq2wKhkQWwoEuUQwE3egz6gi5rYaPR6FiesCcy090SZ8VAcAIl1HjhocmoBwMJG8QJjzWaO24Km%2BIBNXyAH4YYyEu8yDwf80aUswQE4%2B4hFjA2noybmhowgPvGvgY6ngHFbK6FQ3Vw0vhGon5RvJv5ZjrzsXvYU7VYIReXNM26R0Au9TP3ACW5fHu58E0mIxu8vKCYM14vlUqAA%2FhrBLzXzNMJLFstjDFB0trYRUYNQg97M38Dw1LtW1ZgXr2kX5WO8xUPSIIuQ0j9562WHnl15n1zIbIR5d5EilN4hDwJhldJ9cWr9ngudc5vk31gs3T49w5byhAScxloP9%2FT%2FQ%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:00:59,512 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693329/9c07d7af-f836-43b2-b80c-49fb0a07474b.json?AWSAccessKeyId=ASIAU3FAITLPSFNF5NFY&Signature=reObIHxae6FA%2FGfNtFo4iYNSqdA%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCID4zsOz%2BX7bcommmebuNUwJTxgZsOZZA2Mc4EfVdXP5ZAiEAmfZZlBEGnutekonbRo6muj5s1gDaEJdwIzZdyS7hgjAqjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDDqDvlR36UwKIaziryrjAhwU0EtMqDmT9kNIK0MyLtASKbl1ETmYy8GpSoDph4pFGTCaPLjvPqRd5nZ0j4coQA1K789hJok2DWan5A0nPDCUzE5pAW3uZ57fQaBqVYKxVQ8c5TfcUTGzObNgb1z%2BchpF%2BGMui8pAOeKwsx0c3U0imGGiB7KCWxWLIWD4M78WK4ivXexFyBE0aYlA2o7JaqFUaaY1r40wIVJIAKPSIqmX03EHSqfoNhqPtdbsGaECdZycbPkPmqOn8nArWQlQmMuuek0RF7aIMZR%2B9wX7EtwQdhOzmd5uZKXHWCwfw%2FhK%2BYFxh4AT1z%2B7ku0ZNA%2Bsd2W15cNoaxFx8BalTTtbRvWktEpd2jkp%2FNUA2TB4%2BsBBYrnC%2FlNpqMO4NL0RaGxj45neuPL1%2BpNwIpmolvWL71yI8ESwPiGhRoYg6GXfMwlBL7WGk3DwA3ljrJl0LZmEZCD9%2BdUBhP7s5AXAebOXIbz4f9Aw8PnGvgY6ngGsKnEHiZGK0wnklobaTjOJ7a1UhSTAxdvYVH%2BPYIXtSwUT6gAumbs7GhSCUTnX0CtvXR5479IgM9BSDJnWsJ9DCnNl6CmcHg1ohAy1fqcd18bQ9M8VBi9eu%2F9Fli805KBLuTietOi4WDtY6f9ivKVVPnWVJ0yz7MmGBHC8cpWEBGHvIMyEaKXP9Qo0Se1PsIYfxznJMFSrcUS%2FG8kp7Q%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:00:59,514 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9726566/6ac7fcce-075d-46dc-a8c8-58c967e74097.json?AWSAccessKeyId=ASIAU3FAITLP7CVGPW7T&Signature=HGeWYhQjO2vilONFH6nHMRasFNk%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCIDzI6njr%2B7WvSAaIa5syTx%2Bq82VWX2O%2FU2nHkW1s4aGwAiEAw%2BB27agLpyCi8co3ca57il3%2BAt5No8J1k1y7qEfO3S0qjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDFPd9F%2FoyxXGYi6r2CrjApusXU5HIgNGKVOAaee4S7q2qLCr7E0dxmbVZ93QR9n7D7idGQ%2BYMK2nInkGB8xXVK7TO5%2F%2BHUzyap1bPUFQl9KrqT%2BnDmO6M7o%2BR2lVPZnxiHsTT1Fyrvtf%2Bx5sx0MIAHBlCGj3unbR6JZEJiQpmqjskRk7Vxp2zMK09HdhxIUgzsmxoew7GiRQPlMkTckhmzMQVF1Zn2kMIYaF6YYOZyMVd1ScgorNcNmOvaJH07QTPa8XeLNlOaeKbWyfaMERJp41U5874ILUblYEiNFNFZ9%2FGqlfNkTmVX5giDbtY%2F%2BoOQ09QebMd2GwTO4VtUlGtXw4%2FOuTaLJF2Z23gZyf%2BHwgfq0DENctlg3HXODq2wKhkQWwoEuUQwE3egz6gi5rYaPR6FiesCcy090SZ8VAcAIl1HjhocmoBwMJG8QJjzWaO24Km%2BIBNXyAH4YYyEu8yDwf80aUswQE4%2B4hFjA2noybmhowgPvGvgY6ngHFbK6FQ3Vw0vhGon5RvJv5ZjrzsXvYU7VYIReXNM26R0Au9TP3ACW5fHu58E0mIxu8vKCYM14vlUqAA%2FhrBLzXzNMJLFstjDFB0trYRUYNQg97M38Dw1LtW1ZgXr2kX5WO8xUPSIIuQ0j9562WHnl15n1zIbIR5d5EilN4hDwJhldJ9cWr9ngudc5vk31gs3T49w5byhAScxloP9%2FT%2FQ%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:01:05,796 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693290/53ffc4ba-c499-4993-964d-a9f1daa546ed.json?AWSAccessKeyId=ASIAU3FAITLPQUBGFEQC&Signature=rIs%2F4KlaLqOb6Nmsyb68s1vY0lo%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJIMEYCIQD8JwYvSFFYTWX3nnHrNX0pEE%2BjHW2TduSFhEYESC3Y%2FgIhAIV%2FivODh7Vb7eCWWfr0ooSZBsPNNhmDGsV8kWCf3B9QKo8DCML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMzMzMTk2MDczNjk1IgzVoFEZ51gzeOCPeIEq4wLEbabTORW9VQygM0re8H7HtS%2FS%2B%2FUIe6NPHGnTGhnTxabt3i%2B2UzyW0GRRgUb3%2BzoqsAKCAoxeEmA1hh7K9jlyWpmjOWAM%2BjvQqYPjjV1y7lvLb6hzhRWzcNAHbA6WCilzNkxKxndbPnZp4s65Q0l%2B470LtE%2Fp13MYapWWzPbfmm9IpRWXe3NTtQh7U%2FbIK85l860QVbX5nzsgp%2BGpn6WFj27zBdXN%2FS215PnlTYDlD7cEo1WrmCLrl6SG63mW9%2F%2BCUXpSOWh4xaMDhoEyF85aUzfsKD7RVghS4PKr4GqedCAdAIAfeA78FVMNal1uHTTvtSW7duUqjEF4win%2BNKPPu2GYKtfI9pFxc%2BOVzDM8xqONuvmQqle6Lrm54hoqKafV43FqMi7%2B5LLowo%2BjVZ9WCqmaQowxzwBU%2BLWEiIAS9KysV0v0WHEWbWBizOolmMFQwm35Acy72Gi%2BITcwpYsALnApMK7yxr4GOp0BJOeHPVWX2I%2FyiudXFVDYbmHSm6KP5CfprHvBp13VcSmxcy33TEylKM32O6xW4NyYWkmYX%2BcGcjL1%2Bggny3TiDO7zAA7Jel18CACdSjTVL8kl64s6monZ2Vu9sAbvVodlxDf5ZwugQy75fv03LZO0jqSBJyo2iqaBiV1p2t1SSYbSDsI0QGz14BvQpHhKn0v7HwbdDgzblx3L87ZCIA%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:01:05,810 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9725524/fc549dd2-edad-4b60-94f5-7d5cb7ac032b.json?AWSAccessKeyId=ASIAU3FAITLP2WCF4CJT&Signature=QDVEa1A0Im6aU4gphddO9g5RE%2BQ%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHgaCWV1LXdlc3QtMSJHMEUCIQCMvkYiNK01QkMQMmMoHM39l4TUX2h4JipEqqXj1lHGowIgRmZNUpVnNXiUjhoUIWYNPuAYxuG7NPVl0fy4frFVT2sqjwMIwf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDAiWIQMRNTUCBx50tyrjAr%2B57K0PrBRm2c65UQzjUWAZrVSFNhdFBwQd6p2m6hkbhYtBxJxClsM69dxpoaZwjh5QjgK%2BSc8mAHnuBcHDud%2Fqadu%2FqVl0ZP4FLtNogM9q5G4vI%2FCxvNoIO7Sq6mwUykGln5srS%2BpEcRCR%2F6mFfZaDYc8kizMj5zWQKEf1OKCsmhV6hhiMHlZKEB9JjspTpcOEBzIoYL%2FvvSFgjUkhZSWcjvNAP880SJotUuwUbB2RFJKjpl%2BOD0NlQSGShxM78DIfs%2Bz2MM9LrDy9lXrw6RTpnJ8xkAAVFYMMuUgoPwBDS5uT4WdaKPY6CQUQ213ajjxvgIdvUHQoDs0D9wu2brIcyZLvSSHQP%2FvH18%2FXyJRPH7FBjWkrE5QatdVjWO0uVg%2FO3usSN3XVp%2Baa18Iud2GBS33Z%2FAOcdzf17MXHb9wEDEpJH3Lerfy7w%2FeKglAtvfa0hAxX5v4g6YNwvevyFx%2B3BCkwld3GvgY6ngE3aV9dAnZy9zB%2BhDzUX4P5JlclT2Bq4pc%2Bxb%2Bb6LB9CBdodGsWE4nTRRB0tQb6EqVeQOk8oK3U1FYEbDA43uY0NkPWgNUwSTb72jOinICAEQe7AUfKjZG2AHn%2BU7Ftatt9GXXUkTMtkhBgCNelHzGjIoMfhVMHjAC79M3RJrVAMAdvq5gsoq113YHJh7e2o5bDPIqRSXEjLAMIUSABNQ%3D%3D&Expires=1741799435\n",
      "2025-03-12 18:01:05,921 - hoppe_etl_pipeline - ERROR - SSL-Zertifikatsfehler: HTTPSConnectionPool(host='hm-prod-hda-temporary.s3.amazonaws.com', port=443): Max retries exceeded with url: /fleet-api/timeseries/9693290/53ffc4ba-c499-4993-964d-a9f1daa546ed.json?AWSAccessKeyId=ASIAU3FAITLPQUBGFEQC&Signature=rIs%2F4KlaLqOb6Nmsyb68s1vY0lo%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJIMEYCIQD8JwYvSFFYTWX3nnHrNX0pEE%2BjHW2TduSFhEYESC3Y%2FgIhAIV%2FivODh7Vb7eCWWfr0ooSZBsPNNhmDGsV8kWCf3B9QKo8DCML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMzMzMTk2MDczNjk1IgzVoFEZ51gzeOCPeIEq4wLEbabTORW9VQygM0re8H7HtS%2FS%2B%2FUIe6NPHGnTGhnTxabt3i%2B2UzyW0GRRgUb3%2BzoqsAKCAoxeEmA1hh7K9jlyWpmjOWAM%2BjvQqYPjjV1y7lvLb6hzhRWzcNAHbA6WCilzNkxKxndbPnZp4s65Q0l%2B470LtE%2Fp13MYapWWzPbfmm9IpRWXe3NTtQh7U%2FbIK85l860QVbX5nzsgp%2BGpn6WFj27zBdXN%2FS215PnlTYDlD7cEo1WrmCLrl6SG63mW9%2F%2BCUXpSOWh4xaMDhoEyF85aUzfsKD7RVghS4PKr4GqedCAdAIAfeA78FVMNal1uHTTvtSW7duUqjEF4win%2BNKPPu2GYKtfI9pFxc%2BOVzDM8xqONuvmQqle6Lrm54hoqKafV43FqMi7%2B5LLowo%2BjVZ9WCqmaQowxzwBU%2BLWEiIAS9KysV0v0WHEWbWBizOolmMFQwm35Acy72Gi%2BITcwpYsALnApMK7yxr4GOp0BJOeHPVWX2I%2FyiudXFVDYbmHSm6KP5CfprHvBp13VcSmxcy33TEylKM32O6xW4NyYWkmYX%2BcGcjL1%2Bggny3TiDO7zAA7Jel18CACdSjTVL8kl64s6monZ2Vu9sAbvVodlxDf5ZwugQy75fv03LZO0jqSBJyo2iqaBiV1p2t1SSYbSDsI0QGz14BvQpHhKn0v7HwbdDgzblx3L87ZCIA%3D%3D&Expires=1741799435 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n",
      "2025-03-12 18:01:05,922 - hoppe_etl_pipeline - ERROR - API_ERROR|9693290|timeseries|No response from server for ship 9693290\n",
      "2025-03-12 18:01:05,922 - hoppe_etl_pipeline - ERROR - SSL-Zertifikatsfehler: HTTPSConnectionPool(host='hm-prod-hda-temporary.s3.amazonaws.com', port=443): Max retries exceeded with url: /fleet-api/timeseries/9725524/fc549dd2-edad-4b60-94f5-7d5cb7ac032b.json?AWSAccessKeyId=ASIAU3FAITLP2WCF4CJT&Signature=QDVEa1A0Im6aU4gphddO9g5RE%2BQ%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHgaCWV1LXdlc3QtMSJHMEUCIQCMvkYiNK01QkMQMmMoHM39l4TUX2h4JipEqqXj1lHGowIgRmZNUpVnNXiUjhoUIWYNPuAYxuG7NPVl0fy4frFVT2sqjwMIwf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDAiWIQMRNTUCBx50tyrjAr%2B57K0PrBRm2c65UQzjUWAZrVSFNhdFBwQd6p2m6hkbhYtBxJxClsM69dxpoaZwjh5QjgK%2BSc8mAHnuBcHDud%2Fqadu%2FqVl0ZP4FLtNogM9q5G4vI%2FCxvNoIO7Sq6mwUykGln5srS%2BpEcRCR%2F6mFfZaDYc8kizMj5zWQKEf1OKCsmhV6hhiMHlZKEB9JjspTpcOEBzIoYL%2FvvSFgjUkhZSWcjvNAP880SJotUuwUbB2RFJKjpl%2BOD0NlQSGShxM78DIfs%2Bz2MM9LrDy9lXrw6RTpnJ8xkAAVFYMMuUgoPwBDS5uT4WdaKPY6CQUQ213ajjxvgIdvUHQoDs0D9wu2brIcyZLvSSHQP%2FvH18%2FXyJRPH7FBjWkrE5QatdVjWO0uVg%2FO3usSN3XVp%2Baa18Iud2GBS33Z%2FAOcdzf17MXHb9wEDEpJH3Lerfy7w%2FeKglAtvfa0hAxX5v4g6YNwvevyFx%2B3BCkwld3GvgY6ngE3aV9dAnZy9zB%2BhDzUX4P5JlclT2Bq4pc%2Bxb%2Bb6LB9CBdodGsWE4nTRRB0tQb6EqVeQOk8oK3U1FYEbDA43uY0NkPWgNUwSTb72jOinICAEQe7AUfKjZG2AHn%2BU7Ftatt9GXXUkTMtkhBgCNelHzGjIoMfhVMHjAC79M3RJrVAMAdvq5gsoq113YHJh7e2o5bDPIqRSXEjLAMIUSABNQ%3D%3D&Expires=1741799435 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n",
      "2025-03-12 18:01:05,924 - hoppe_etl_pipeline - ERROR - API_ERROR|9725524|timeseries|No response from server for ship 9725524\n",
      "2025-03-12 18:01:15,644 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9726566/6ac7fcce-075d-46dc-a8c8-58c967e74097.json?AWSAccessKeyId=ASIAU3FAITLP7CVGPW7T&Signature=HGeWYhQjO2vilONFH6nHMRasFNk%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCIDzI6njr%2B7WvSAaIa5syTx%2Bq82VWX2O%2FU2nHkW1s4aGwAiEAw%2BB27agLpyCi8co3ca57il3%2BAt5No8J1k1y7qEfO3S0qjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDFPd9F%2FoyxXGYi6r2CrjApusXU5HIgNGKVOAaee4S7q2qLCr7E0dxmbVZ93QR9n7D7idGQ%2BYMK2nInkGB8xXVK7TO5%2F%2BHUzyap1bPUFQl9KrqT%2BnDmO6M7o%2BR2lVPZnxiHsTT1Fyrvtf%2Bx5sx0MIAHBlCGj3unbR6JZEJiQpmqjskRk7Vxp2zMK09HdhxIUgzsmxoew7GiRQPlMkTckhmzMQVF1Zn2kMIYaF6YYOZyMVd1ScgorNcNmOvaJH07QTPa8XeLNlOaeKbWyfaMERJp41U5874ILUblYEiNFNFZ9%2FGqlfNkTmVX5giDbtY%2F%2BoOQ09QebMd2GwTO4VtUlGtXw4%2FOuTaLJF2Z23gZyf%2BHwgfq0DENctlg3HXODq2wKhkQWwoEuUQwE3egz6gi5rYaPR6FiesCcy090SZ8VAcAIl1HjhocmoBwMJG8QJjzWaO24Km%2BIBNXyAH4YYyEu8yDwf80aUswQE4%2B4hFjA2noybmhowgPvGvgY6ngHFbK6FQ3Vw0vhGon5RvJv5ZjrzsXvYU7VYIReXNM26R0Au9TP3ACW5fHu58E0mIxu8vKCYM14vlUqAA%2FhrBLzXzNMJLFstjDFB0trYRUYNQg97M38Dw1LtW1ZgXr2kX5WO8xUPSIIuQ0j9562WHnl15n1zIbIR5d5EilN4hDwJhldJ9cWr9ngudc5vk31gs3T49w5byhAScxloP9%2FT%2FQ%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:01:15,645 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)'))': /fleet-api/timeseries/9693329/9c07d7af-f836-43b2-b80c-49fb0a07474b.json?AWSAccessKeyId=ASIAU3FAITLPSFNF5NFY&Signature=reObIHxae6FA%2FGfNtFo4iYNSqdA%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCID4zsOz%2BX7bcommmebuNUwJTxgZsOZZA2Mc4EfVdXP5ZAiEAmfZZlBEGnutekonbRo6muj5s1gDaEJdwIzZdyS7hgjAqjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDDqDvlR36UwKIaziryrjAhwU0EtMqDmT9kNIK0MyLtASKbl1ETmYy8GpSoDph4pFGTCaPLjvPqRd5nZ0j4coQA1K789hJok2DWan5A0nPDCUzE5pAW3uZ57fQaBqVYKxVQ8c5TfcUTGzObNgb1z%2BchpF%2BGMui8pAOeKwsx0c3U0imGGiB7KCWxWLIWD4M78WK4ivXexFyBE0aYlA2o7JaqFUaaY1r40wIVJIAKPSIqmX03EHSqfoNhqPtdbsGaECdZycbPkPmqOn8nArWQlQmMuuek0RF7aIMZR%2B9wX7EtwQdhOzmd5uZKXHWCwfw%2FhK%2BYFxh4AT1z%2B7ku0ZNA%2Bsd2W15cNoaxFx8BalTTtbRvWktEpd2jkp%2FNUA2TB4%2BsBBYrnC%2FlNpqMO4NL0RaGxj45neuPL1%2BpNwIpmolvWL71yI8ESwPiGhRoYg6GXfMwlBL7WGk3DwA3ljrJl0LZmEZCD9%2BdUBhP7s5AXAebOXIbz4f9Aw8PnGvgY6ngGsKnEHiZGK0wnklobaTjOJ7a1UhSTAxdvYVH%2BPYIXtSwUT6gAumbs7GhSCUTnX0CtvXR5479IgM9BSDJnWsJ9DCnNl6CmcHg1ohAy1fqcd18bQ9M8VBi9eu%2F9Fli805KBLuTietOi4WDtY6f9ivKVVPnWVJ0yz7MmGBHC8cpWEBGHvIMyEaKXP9Qo0Se1PsIYfxznJMFSrcUS%2FG8kp7Q%3D%3D&Expires=1741799445\n",
      "2025-03-12 18:01:15,776 - hoppe_etl_pipeline - ERROR - SSL-Zertifikatsfehler: HTTPSConnectionPool(host='hm-prod-hda-temporary.s3.amazonaws.com', port=443): Max retries exceeded with url: /fleet-api/timeseries/9693329/9c07d7af-f836-43b2-b80c-49fb0a07474b.json?AWSAccessKeyId=ASIAU3FAITLPSFNF5NFY&Signature=reObIHxae6FA%2FGfNtFo4iYNSqdA%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCID4zsOz%2BX7bcommmebuNUwJTxgZsOZZA2Mc4EfVdXP5ZAiEAmfZZlBEGnutekonbRo6muj5s1gDaEJdwIzZdyS7hgjAqjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDDqDvlR36UwKIaziryrjAhwU0EtMqDmT9kNIK0MyLtASKbl1ETmYy8GpSoDph4pFGTCaPLjvPqRd5nZ0j4coQA1K789hJok2DWan5A0nPDCUzE5pAW3uZ57fQaBqVYKxVQ8c5TfcUTGzObNgb1z%2BchpF%2BGMui8pAOeKwsx0c3U0imGGiB7KCWxWLIWD4M78WK4ivXexFyBE0aYlA2o7JaqFUaaY1r40wIVJIAKPSIqmX03EHSqfoNhqPtdbsGaECdZycbPkPmqOn8nArWQlQmMuuek0RF7aIMZR%2B9wX7EtwQdhOzmd5uZKXHWCwfw%2FhK%2BYFxh4AT1z%2B7ku0ZNA%2Bsd2W15cNoaxFx8BalTTtbRvWktEpd2jkp%2FNUA2TB4%2BsBBYrnC%2FlNpqMO4NL0RaGxj45neuPL1%2BpNwIpmolvWL71yI8ESwPiGhRoYg6GXfMwlBL7WGk3DwA3ljrJl0LZmEZCD9%2BdUBhP7s5AXAebOXIbz4f9Aw8PnGvgY6ngGsKnEHiZGK0wnklobaTjOJ7a1UhSTAxdvYVH%2BPYIXtSwUT6gAumbs7GhSCUTnX0CtvXR5479IgM9BSDJnWsJ9DCnNl6CmcHg1ohAy1fqcd18bQ9M8VBi9eu%2F9Fli805KBLuTietOi4WDtY6f9ivKVVPnWVJ0yz7MmGBHC8cpWEBGHvIMyEaKXP9Qo0Se1PsIYfxznJMFSrcUS%2FG8kp7Q%3D%3D&Expires=1741799445 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n",
      "2025-03-12 18:01:15,777 - hoppe_etl_pipeline - ERROR - SSL-Zertifikatsfehler: HTTPSConnectionPool(host='hm-prod-hda-temporary.s3.amazonaws.com', port=443): Max retries exceeded with url: /fleet-api/timeseries/9726566/6ac7fcce-075d-46dc-a8c8-58c967e74097.json?AWSAccessKeyId=ASIAU3FAITLP7CVGPW7T&Signature=HGeWYhQjO2vilONFH6nHMRasFNk%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEHkaCWV1LXdlc3QtMSJHMEUCIDzI6njr%2B7WvSAaIa5syTx%2Bq82VWX2O%2FU2nHkW1s4aGwAiEAw%2BB27agLpyCi8co3ca57il3%2BAt5No8J1k1y7qEfO3S0qjwMIwv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwzMzMxOTYwNzM2OTUiDFPd9F%2FoyxXGYi6r2CrjApusXU5HIgNGKVOAaee4S7q2qLCr7E0dxmbVZ93QR9n7D7idGQ%2BYMK2nInkGB8xXVK7TO5%2F%2BHUzyap1bPUFQl9KrqT%2BnDmO6M7o%2BR2lVPZnxiHsTT1Fyrvtf%2Bx5sx0MIAHBlCGj3unbR6JZEJiQpmqjskRk7Vxp2zMK09HdhxIUgzsmxoew7GiRQPlMkTckhmzMQVF1Zn2kMIYaF6YYOZyMVd1ScgorNcNmOvaJH07QTPa8XeLNlOaeKbWyfaMERJp41U5874ILUblYEiNFNFZ9%2FGqlfNkTmVX5giDbtY%2F%2BoOQ09QebMd2GwTO4VtUlGtXw4%2FOuTaLJF2Z23gZyf%2BHwgfq0DENctlg3HXODq2wKhkQWwoEuUQwE3egz6gi5rYaPR6FiesCcy090SZ8VAcAIl1HjhocmoBwMJG8QJjzWaO24Km%2BIBNXyAH4YYyEu8yDwf80aUswQE4%2B4hFjA2noybmhowgPvGvgY6ngHFbK6FQ3Vw0vhGon5RvJv5ZjrzsXvYU7VYIReXNM26R0Au9TP3ACW5fHu58E0mIxu8vKCYM14vlUqAA%2FhrBLzXzNMJLFstjDFB0trYRUYNQg97M38Dw1LtW1ZgXr2kX5WO8xUPSIIuQ0j9562WHnl15n1zIbIR5d5EilN4hDwJhldJ9cWr9ngudc5vk31gs3T49w5byhAScxloP9%2FT%2FQ%3D%3D&Expires=1741799445 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n",
      "2025-03-12 18:01:15,777 - hoppe_etl_pipeline - ERROR - API_ERROR|9693329|timeseries|No response from server for ship 9693329\n",
      "2025-03-12 18:01:15,777 - hoppe_etl_pipeline - ERROR - API_ERROR|9726566|timeseries|No response from server for ship 9726566\n",
      "2025-03-12 18:01:15,781 - hoppe_etl_pipeline - INFO - Cleanup of data older than 2024-12-12 completed\n",
      "2025-03-12 18:01:15,783 - hoppe_etl_pipeline - INFO - Cleanup of data older than 2024-12-12 completed\n",
      "2025-03-12 18:01:15,785 - hoppe_etl_pipeline - INFO - Cleanup of data older than 2024-12-12 completed\n",
      "2025-03-12 18:01:15,786 - hoppe_etl_pipeline - INFO - Pipeline run completed with stats: {'success': False, 'ships_total': 14, 'ships_processed': 6, 'ships_failed': 8, 'api_calls_total': 15, 'api_calls_success': 7, 'api_calls_failed': 8, 'errors': ['Empty timeseries data for ship 9400071', 'No response from server for ship 9725524', 'Empty timeseries data for ship 9778399', 'No response from server for ship 9693290', 'Empty timeseries data for ship 9693331', 'No response from server for ship 9693329', 'Empty timeseries data for ship 9395525', 'No response from server for ship 9726566'], 'ship_results': [{'imo': '9400071', 'success': False, 'errors': ['Empty timeseries data for ship 9400071'], 'api_calls': 1, 'api_success': 0}, {'imo': '9725512', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9725524', 'success': False, 'errors': ['No response from server for ship 9725524'], 'api_calls': 1, 'api_success': 0}, {'imo': '9778399', 'success': False, 'errors': ['Empty timeseries data for ship 9778399'], 'api_calls': 1, 'api_success': 0}, {'imo': '9696826', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9693290', 'success': False, 'errors': ['No response from server for ship 9693290'], 'api_calls': 1, 'api_success': 0}, {'imo': '9306184', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9693331', 'success': False, 'errors': ['Empty timeseries data for ship 9693331'], 'api_calls': 1, 'api_success': 0}, {'imo': '9306287', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9306160', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9696838', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9693329', 'success': False, 'errors': ['No response from server for ship 9693329'], 'api_calls': 1, 'api_success': 0}, {'imo': '9395525', 'success': False, 'errors': ['Empty timeseries data for ship 9395525'], 'api_calls': 1, 'api_success': 0}, {'imo': '9726566', 'success': False, 'errors': ['No response from server for ship 9726566'], 'api_calls': 1, 'api_success': 0}]}\n",
      "2025-03-12 18:01:15,787 - hoppe_etl_pipeline - INFO - Pipeline completed in 0:00:42.190910\n",
      "2025-03-12 18:01:15,787 - hoppe_etl_pipeline - INFO - Pipeline run completed at ('2025/03/12/17/00', {'success': False, 'ships_total': 14, 'ships_processed': 6, 'ships_failed': 8, 'api_calls_total': 15, 'api_calls_success': 7, 'api_calls_failed': 8, 'errors': ['Empty timeseries data for ship 9400071', 'No response from server for ship 9725524', 'Empty timeseries data for ship 9778399', 'No response from server for ship 9693290', 'Empty timeseries data for ship 9693331', 'No response from server for ship 9693329', 'Empty timeseries data for ship 9395525', 'No response from server for ship 9726566'], 'ship_results': [{'imo': '9400071', 'success': False, 'errors': ['Empty timeseries data for ship 9400071'], 'api_calls': 1, 'api_success': 0}, {'imo': '9725512', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9725524', 'success': False, 'errors': ['No response from server for ship 9725524'], 'api_calls': 1, 'api_success': 0}, {'imo': '9778399', 'success': False, 'errors': ['Empty timeseries data for ship 9778399'], 'api_calls': 1, 'api_success': 0}, {'imo': '9696826', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9693290', 'success': False, 'errors': ['No response from server for ship 9693290'], 'api_calls': 1, 'api_success': 0}, {'imo': '9306184', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9693331', 'success': False, 'errors': ['Empty timeseries data for ship 9693331'], 'api_calls': 1, 'api_success': 0}, {'imo': '9306287', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9306160', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9696838', 'success': True, 'errors': [], 'api_calls': 1, 'api_success': 1}, {'imo': '9693329', 'success': False, 'errors': ['No response from server for ship 9693329'], 'api_calls': 1, 'api_success': 0}, {'imo': '9395525', 'success': False, 'errors': ['Empty timeseries data for ship 9395525'], 'api_calls': 1, 'api_success': 0}, {'imo': '9726566', 'success': False, 'errors': ['No response from server for ship 9726566'], 'api_calls': 1, 'api_success': 0}]})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and run pipeline\n",
    "try:\n",
    "    pipeline = Pipeline(config, api_key)\n",
    "    run_timestamp = pipeline.run(\"timeseries\")  # \"all\" or \"timeseries\" or \"fleet\"\n",
    "    logger.info(f\"Pipeline run completed at {run_timestamp}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Pipeline run failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the pivoted data\n",
    "exporter = TimeseriesPivotExporter(config)\n",
    "output_path = \"./data/analysis/pivoted_timeseries.parquet\"\n",
    "max_days = None #30  # Process last 30 days, set to None for all available data\n",
    "max_signals = None  # Process all signals, set a number to limit the most frequent ones\n",
    "\n",
    "try:\n",
    "    result_path = exporter.export_pivoted_data(output_path, max_days, max_signals)\n",
    "    if result_path:\n",
    "        logger.info(f\"Successfully exported pivoted data to {result_path}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to export pivoted data\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Export failed with error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If database connection is configured, store to database\n",
    "db_connection_string = os.getenv('MSSQL_CONNECTION_STRING')\n",
    "if db_connection_string:\n",
    "    try:\n",
    "        engine = sa.create_engine(db_connection_string)\n",
    "        logger.info(\"Database connection established\")\n",
    "        \n",
    "        # Process and store data to database\n",
    "        pipeline.process_and_store_to_db(engine, run_timestamp)\n",
    "        logger.info(\"Database integration completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database integration failed: {str(e)}\")\n",
    "else:\n",
    "    logger.warning(\"MSSQL_CONNECTION_STRING not set, skipping database integration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "base_url=os.getenv('HOPPE_BASE_URL', \"https://api.hoppe-sts.com/\")\n",
    "api_key = os.getenv('HOPPE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_url = \"fleet/9306160/event-notifications/latest\"\n",
    "request_url = f\"{base_url}{relative_url}\"\n",
    "response = requests.request(\"GET\", request_url, headers={\"Authorization\": f\"ApiKey {api_key}\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoppe-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
