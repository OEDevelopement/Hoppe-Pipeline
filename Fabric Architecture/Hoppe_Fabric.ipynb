{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    base_url: str = \"https://api.hoppe-sts.com/\"\n",
    "    raw_path: str = \"./data/raw_data\"\n",
    "    transformed_path: str = \"./data/transformed_data\" \n",
    "    metadata_path: str = \"./data/latest\"\n",
    "    silver_tables: str = \"./\"\n",
    "    metadata_tables: str = \"./\"\n",
    "    batch_size: int = 1000\n",
    "    max_workers: int = 8  # Erhöhte Worker für bessere Parallelisierung\n",
    "    days_to_keep: int = 90  # Daten werden für 90 Tage aufbewahrt\n",
    "    history_days: int = 5  # Letzten 5 Tage für Historie laden\n",
    "    logger: logging.Logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2225ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import time\n",
    "\n",
    "class API_Client:\n",
    "\n",
    "    def __init__(self, base_url: str, api_key: str, cofig):\n",
    "        self.config = config\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def get_data(self, relative_url, max_retries=3, backoff_factor=2):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                request_url = f\"{self.base_url}{relative_url}\"\n",
    "                response = requests.request(\"GET\", request_url, headers={\"Authorization\": f\"ApiKey {self.api_key}\"})\n",
    "                self.config.logger.info(f\"INFO - Request for {relative_url} successful\")\n",
    "                return response, response.json()\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                self.config.logger.error(f\"ERROR - SSL-Zertifikatsfehler: {str(e)}\")\n",
    "                return None, None\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = backoff_factor ** attempt\n",
    "                    self.config.logger.warning(f\"Timeout bei API-Anfrage: {str(e)}. Retry {attempt+1}/{max_retries} nach {wait_time}s\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                self.config.logger.error(f\"ERROR - Timeout bei API-Anfrage nach {max_retries} Versuchen: {str(e)}\")\n",
    "                return None, None\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = backoff_factor ** attempt\n",
    "                    self.config.logger.warning(f\"Verbindungsfehler: {str(e)}. Retry {attempt+1}/{max_retries} nach {wait_time}s\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                self.config.logger.error(f\"ERROR - Verbindungsfehler nach {max_retries} Versuchen: {str(e)}\")\n",
    "                return None, None\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.config.logger.error(f\"ERROR - API request failed: {str(e)}\")\n",
    "                if hasattr(e, 'response'):\n",
    "                    return e.response, None\n",
    "                return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a827144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class Data_Processor:\n",
    "    #logger = logging.getLogger(\"Data_Processor\") ???\n",
    "\n",
    "    @staticmethod\n",
    "    def get_imo_numbers(data: List[dict]) -> List[str]:\n",
    "        return [ship['imo'] for ship in data if ship.get('active', True)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_shipdata(shipdata: pl.DataFrame, run_timestamp: str) -> Tuple[pl.DataFrame, Dict[str, pl.DataFrame]]:\n",
    "        shipdata = shipdata.unnest(\"data\")\n",
    "        \n",
    "        # Verschachtelte Tabellen extrahiren\n",
    "        tables = {}\n",
    "        for column, dtype in shipdata.collect_schema().items():\n",
    "            if dtype == pl.List(pl.Struct):\n",
    "                tables[column] = (\n",
    "                    shipdata.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .unnest(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    \n",
    "                )\n",
    "            elif dtype == pl.List:\n",
    "                tables[column] = (\n",
    "                    shipdata.select(\"imo\", column)\n",
    "                    .explode(column)\n",
    "                    .with_columns(\n",
    "                        pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "                    )\n",
    "                    \n",
    "                )\n",
    "\n",
    "        # Schiffsdaten ohne Verschachtelung extrahieren\n",
    "        shipdata = shipdata.select(\n",
    "            pl.exclude([col for col, dtype in shipdata.collect_schema().items() if dtype == pl.List])\n",
    "        ).with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        )\n",
    "\n",
    "        return shipdata, tables\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_signals(signals: pl.DataFrame, run_timestamp: str) -> pl.DataFrame:\n",
    "        if len(signals) == 0:\n",
    "            return signals\n",
    "\n",
    "        # Initiale Transformation    \n",
    "        signals = (\n",
    "            signals.unnest(\"signals\")\n",
    "            .unpivot(index=\"imo\", variable_name=\"signal\")\n",
    "            .unnest(\"value\")\n",
    "        )\n",
    "\n",
    "        # Verbleibende Verschachtelungen plätten\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Struct:\n",
    "                signals = signals.unnest(column)\n",
    "\n",
    "        # Null-Werte\n",
    "        for column, dtype in signals.collect_schema().items():\n",
    "            if dtype == pl.Null:\n",
    "                signals = signals.with_columns(pl.lit(column).cast(pl.String))\n",
    "        \n",
    "        # Das Lade-Datum hinzufügen\n",
    "        signals = signals.with_columns(\n",
    "            pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "        )\n",
    "                \n",
    "        return signals\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def transform_timeseries(timeseries: pl.DataFrame, imo: str, run_timestamp: str) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \n",
    "        if len(timeseries) == 0:\n",
    "            # Hier müssen wir sicherstellen, dass beide zurückgegebenen DataFrames dieselbe Struktur haben\n",
    "            empty_df = pl.DataFrame({\n",
    "                \"imo\": [], \n",
    "                \"signal\": [],\n",
    "                \"signal_timestamp\": [], \n",
    "                \"signal_value\": [], \n",
    "                \"loaddate\": []\n",
    "            })\n",
    "            return empty_df, empty_df.clone()\n",
    "        \n",
    "        # Optimierte Transformation\n",
    "        transformed = (\n",
    "            timeseries.drop(\"timestamp\")\n",
    "            .unpivot(\n",
    "                index=[],\n",
    "                variable_name=\"signal\"\n",
    "            )\n",
    "            .unnest(\"value\")\n",
    "            .unpivot(\n",
    "                index=[\"signal\"],\n",
    "                variable_name=\"signal_timestamp\",\n",
    "                value_name=\"signal_value\"\n",
    "            )\n",
    "            .with_columns([\n",
    "                pl.lit(imo).alias(\"imo\"),\n",
    "                pl.lit(run_timestamp).alias(\"loaddate\")\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Lücken (NULL-Werte) identifizieren\n",
    "        gaps = (\n",
    "            transformed\n",
    "            .filter(pl.col(\"signal_value\").is_null())\n",
    "            .select(\"imo\", \"signal\", \"signal_timestamp\", \"loaddate\")\n",
    "            .with_columns([\n",
    "                pl.col(\"signal_timestamp\").alias(\"gap_start\")\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # NULL-Werte aus dem Hauptdatensatz entfernen\n",
    "        data = transformed.filter(pl.col(\"signal_value\").is_not_null())\n",
    "        \n",
    "        return data, gaps\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_gaps(gaps_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \n",
    "        if len(gaps_df) == 0:\n",
    "            return pl.DataFrame()\n",
    "        \n",
    "        # Stelle sicher, dass gap_start ein Datetime-Objekt ist\n",
    "        if gaps_df[\"gap_start\"].dtype != pl.Datetime:\n",
    "            gaps_df = gaps_df.with_columns(\n",
    "                pl.col(\"gap_start\").cast(pl.Datetime)\n",
    "            )\n",
    "            \n",
    "        result = []\n",
    "        \n",
    "        # Gruppieren nach IMO und Signal, sortiere nach Zeitstempel\n",
    "        for (imo, signal), group in gaps_df.sort(\"gap_start\").group_by([\"imo\", \"signal\"]):\n",
    "            group_df = group.sort(\"gap_start\")\n",
    "            \n",
    "            if len(group_df) <= 1:\n",
    "                # Bei nur einem Eintrag\n",
    "                result.append({\n",
    "                    \"imo\": imo,\n",
    "                    \"signal\": signal,\n",
    "                    \"gap_start\": group_df[\"gap_start\"][0],\n",
    "                    \"gap_end\": group_df[\"gap_start\"][0],\n",
    "                    \"loaddate\": group_df[\"loaddate\"][0]\n",
    "                })\n",
    "                continue\n",
    "                \n",
    "            current_start = group_df[\"gap_start\"][0]\n",
    "            prev_time = current_start\n",
    "            \n",
    "            for i in range(1, len(group_df)):\n",
    "                curr_time = group_df[\"gap_start\"][i]\n",
    "                \n",
    "                # Prüfe, ob mehr als 5 Minuten zwischen zwei Einträgen liegen\n",
    "                max_sec = 5*60\n",
    "                # Hier kommt es zum Fehler, wenn prev_time und curr_time Strings sind\n",
    "                time_diff_seconds = (curr_time - prev_time).total_seconds()\n",
    "                \n",
    "                if time_diff_seconds > max_sec:\n",
    "                    result.append({\n",
    "                        \"imo\": imo,\n",
    "                        \"signal\": signal,\n",
    "                        \"gap_start\": current_start,\n",
    "                        \"gap_end\": prev_time,\n",
    "                        \"loaddate\": group_df[\"loaddate\"][i]\n",
    "                    })\n",
    "                    current_start = curr_time  # Starte neue Lücke\n",
    "                \n",
    "                prev_time = curr_time  # Aktualisiere den vorherigen Zeitstempel\n",
    "            \n",
    "            # Letzte Lücke hinzufügen\n",
    "            result.append({\n",
    "                \"imo\": imo,\n",
    "                \"signal\": signal,\n",
    "                \"gap_start\": current_start,\n",
    "                \"gap_end\": prev_time,\n",
    "                \"loaddate\": group_df[\"loaddate\"][-1]\n",
    "            })\n",
    "        \n",
    "        return pl.DataFrame(result)\n",
    "    \n",
    "    @staticmethod\n",
    "    def enrich_timeseries_with_friendly_names(timeseries_df: pl.DataFrame, signals_df: pl.DataFrame, imo: pl.String) -> pl.DataFrame:\n",
    "\n",
    "        if len(timeseries_df) == 0 or len(signals_df) == 0:\n",
    "            return timeseries_df\n",
    "            \n",
    "        # Extrahiere Signal-Mapping (signal -> friendly_name)\n",
    "        signal_mapping = (\n",
    "            signals_df\n",
    "            .filter(pl.col(\"friendly_name\").is_not_null(), pl.col(\"imo\") == imo)\n",
    "            .select([\"signal\", \"friendly_name\"])\n",
    "            .unique()\n",
    "        )\n",
    "        \n",
    "        # Join mit Timeseries-Daten\n",
    "        return timeseries_df.join(\n",
    "            signal_mapping,\n",
    "            on=\"signal\",\n",
    "            how=\"left\"\n",
    "        ).select([\"imo\", \"signal\", \"friendly_name\", \"signal_timestamp\", \"signal_value\", \"loaddate\"])\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_daily_timeseries_summary(hist_df: pl.DataFrame, daily_df: pl.DataFrame, current_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        combined_df = pl.concat([hist_df, daily_df, current_df])\n",
    "        # summary_df = combined_df.unique(subset=[\"imo\", \"signal_timestamp\", \"friendly_name\"], keep=\"first\").filter(pl.col(\"tag\")==\"new\"| pl.col(\"tag\")==\"today\")\n",
    "        new_df = combined_df.unique(subset=[\"imo\", \"signal_timestamp\", \"friendly_name\"], keep=\"first\").filter(pl.col(\"tag\").is_in([\"new\"]))\n",
    "        summary_df = combined_df.unique(subset=[\"imo\", \"signal_timestamp\", \"friendly_name\"], keep=\"first\").filter(pl.col(\"tag\").is_in([\"new\", \"today\"]))\n",
    "\n",
    "        return summary_df, new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Union, defaultdict\n",
    "import polars as pl\n",
    "\n",
    "class Data_Storage:\n",
    "    \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # storage_options = {\"bearer_token\": notebookutils.credentials.getToken('storage'), \"use_fabric_endpoint\": \"true\"}\n",
    "        \n",
    "    # Schreiben von Files in lokale Ordner    \n",
    "    def write_file(self, data: Union[List, Dict, pl.DataFrame], filename: str, path: str, postfix: str) -> None:\n",
    "        notebookutils.fs.mkdirs(path)\n",
    "        full_path = f\"{path}/{filename}.{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            # Schreibt json files\n",
    "            if postfix == 'json':\n",
    "                with open(full_path, 'w') as f:\n",
    "                    json.dump(data, f)\n",
    "                self.config.logger.info(f\"INFO - Writting to {filename}.json file successful\")\n",
    "\n",
    "            # Schreibt parquet files\n",
    "            elif postfix == 'parquet':\n",
    "\n",
    "                # Check auf richtiges Input-Format\n",
    "                if not isinstance(data, pl.DataFrame):\n",
    "                    if isinstance(data, list) or isinstance(data, dict):\n",
    "                        data = pl.DataFrame(data)\n",
    "                    else:\n",
    "                        raise ValueError(\"Data must be DataFrame, List, or Dict for parquet format\")\n",
    "                    \n",
    "                data.write_parquet(full_path, compression=\"snappy\")\n",
    "                self.config.logger.info(f\"INFO - Writting to {filename}.parquet file successful\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {postfix}\")\n",
    "                \n",
    "            # self.config.logger.info(f\"INFO - Data saved to {full_path}\")\n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to write file {full_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def read_file(self, filename: str, path: str, postfix: str) -> pl.DataFrame:\n",
    "        full_path = f\"{path}/{filename}.{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            if not notebookutils.fs.exists(full_path):\n",
    "                self.config.logger.info(f\"INFO - File {full_path} does not exist\")\n",
    "                return pl.DataFrame()  # Leeres DataFrame zurückgeben\n",
    "            \n",
    "            else:\n",
    "                if postfix == 'json':\n",
    "                    with open(full_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        data = pl.DataFrame(data)\n",
    "                    self.config.logger.info(f\"INFO - Reading from {filename}.json file successfully\")\n",
    "                    return data\n",
    "\n",
    "                elif postfix == 'parquet':\n",
    "                    data = pl.read_parquet(full_path)\n",
    "                    self.config.logger.info(f\"INFO - Reading from {filename}.parquet file successfully\")\n",
    "                    return data\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported format: {postfix}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to read file {full_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def write_table(self, data: pl.DataFrame, tablename: str, path: str, method: str = 'append', key: str = None, key2: str = None, key3: str = None):\n",
    "        # bsp. final_table_path: f'{lh_path}/Tables/hoppe/{tablename}'\n",
    "        # schema_mode=\"overwrite\" or schema_mode=\"merge\". schema_mode=\"overwrite\"\n",
    "\n",
    "        table_path = f'{path}/{tablename}'\n",
    "\n",
    "        try: \n",
    "            if method != 'merge':\n",
    "                data.write_delta(table_path, mode=method)\n",
    "                self.config.logger.info(f\"INFO - Writting to table {tablename} successful\")\n",
    "                print(f\"INFO - Writting to table {tablename} successful\")\n",
    "            else: \n",
    "                if key2 != None and key3 != None:\n",
    "                    data.write_delta(table_path, mode=method,\n",
    "                    delta_merge_options={\n",
    "                        'predicate': f'source.{key} = target.{key} AND source.{key2} = target.{key2} AND source.{key3} = target.{key3}',\n",
    "                        'source_alias': 'source',\n",
    "                        'target_alias': 'target',\n",
    "                    },\n",
    "                    ).when_matched_update_all(\n",
    "                    ).when_not_matched_insert_all(\n",
    "                    ).execute()\n",
    "                    self.config.logger.info(f\"INFO - Merge to table {tablename} successful\")\n",
    "                elif key2 != None:\n",
    "                    data.write_delta(table_path, mode=method,\n",
    "                    delta_merge_options={\n",
    "                        'predicate': f'source.{key} = target.{key} AND source.{key2} = target.{key2}',\n",
    "                        'source_alias': 'source',\n",
    "                        'target_alias': 'target',\n",
    "                    },\n",
    "                    ).when_matched_update_all(\n",
    "                    ).when_not_matched_insert_all(\n",
    "                    ).execute()\n",
    "                    self.config.logger.info(f\"INFO - Merge to table {tablename} successful\")\n",
    "                else:\n",
    "                    data.write_delta(table_path, mode=method,\n",
    "                    delta_merge_options={\n",
    "                        'predicate': f'source.{key} = target.{key}',\n",
    "                        'source_alias': 'source',\n",
    "                        'target_alias': 'target',\n",
    "                    },\n",
    "                    ).when_matched_update_all(\n",
    "                    ).when_not_matched_insert_all(\n",
    "                    ).execute()\n",
    "                    self.config.logger.info(f\"INFO - Merge to table {tablename} successful\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to {method} to table {table_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        \n",
    "\n",
    "    def read_table(tablename: str, path: str) -> pl.DataFrame:\n",
    "        # bsp. final_table_path: f'{lh_path}/Tables/hoppe/{tablename}' \n",
    "\n",
    "        table_path = f'{path}/{tablename}'\n",
    "        try: \n",
    "            df = pl.read_delta(table_path)\n",
    "            print(f\"INFO - Loading table {tablename} successful\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to load {table_path}: {str(e)}\")\n",
    "            return pl.DataFrame\n",
    "            raise\n",
    "        \n",
    "    def find_timeseries_files(self, base_path: str, max_days: int = None, pattern: str = \"Timeseries_*.parquet\") -> defaultdict:\n",
    "        if not notebookutils.fs.exists(base_path) or not notebookutils.fs.ls(full_path):\n",
    "            self.config.logger.error(f\"ERROR - Directory {base_path} does not exist or is no directory\")\n",
    "            return defaultdict(list)\n",
    "\n",
    "        try:\n",
    "            # Optimiertes Dateisystem-Scannen mit einmaliger Tiefensuche\n",
    "            files_by_imo = defaultdict(list)\n",
    "            \n",
    "            # Metadaten für Logging\n",
    "            days_found = set()\n",
    "            months_found = set()\n",
    "            years_found = set()\n",
    "            \n",
    "            # Schnellere Dateisuche mit glob statt rekursivem Durchsuchen\n",
    "            year_dirs = sorted([d for d in base_dir.iterdir() if d.is_dir()], key=lambda x: x.name, reverse=True)\n",
    "            \n",
    "            for year_dir in year_dirs:\n",
    "                years_found.add(year_dir.name)\n",
    "                \n",
    "                month_dirs = sorted([d for d in year_dir.iterdir() if d.is_dir()], key=lambda x: x.name, reverse=True)\n",
    "                for month_dir in month_dirs:\n",
    "                    months_found.add(f\"{year_dir.name}/{month_dir.name}\")\n",
    "                    \n",
    "                    day_dirs = sorted([d for d in month_dir.iterdir() if d.is_dir()], key=lambda x: x.name, reverse=True)\n",
    "                    \n",
    "                    # Begrenzung auf max_days\n",
    "                    if max_days is not None and len(days_found) >= max_days:\n",
    "                        break\n",
    "                        \n",
    "                    for day_dir in day_dirs:\n",
    "                        # Begrenzung auf max_days\n",
    "                        if max_days is not None and len(days_found) >= max_days:\n",
    "                            break\n",
    "                            \n",
    "                        days_found.add(f\"{year_dir.name}/{month_dir.name}/{day_dir.name}\")\n",
    "                        \n",
    "                        # Direktes Sammeln aller Dateien für diesen Tag mit glob\n",
    "                        for file in day_dir.glob(pattern):\n",
    "                            imo = file.stem.split(\"_\")[1]  # Extrahiert <imo> aus \"Timeseries_<imo>.parquet\"\n",
    "                            files_by_imo[imo].append(file)\n",
    "            \n",
    "            self.config.logger.info(f\"INFO - {sum(len(files) for files in files_by_imo.values())} files found: \"\n",
    "                            f\"{len(files_by_imo)} different ships, {len(days_found)} days, \"\n",
    "                            f\"{len(months_found)} months, {len(years_found)} years\")\n",
    "            \n",
    "            return files_by_imo  # Dictionary mit Listen von Dateien nach IMO\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to get historical Data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def find_timeseries_summaries(self, base_path: str,  pattern:str = \"*.parquet\") -> list:\n",
    "        base_dir = Path(base_path)\n",
    "        if not base_dir.exists() or not base_dir.is_dir():\n",
    "            self.config.logger.error(f\"ERROR - Directory {base_path} does not exist or is no directory\")\n",
    "            return defaultdict(list)\n",
    "\n",
    "        # Alles Files mit dem pattern finden, pattern kann Datum begrenzen z.B. 2025*.parquet = alle Dateien aus 2025\n",
    "        try:\n",
    "            \n",
    "            files =  []\n",
    "            files_found = 0\n",
    "\n",
    "            for file in base_dir.rglob(pattern):\n",
    "                files.append(file)\n",
    "                files_found +=1\n",
    "\n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to get historical Data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        self.config.logger.info(f\"INFO - {files_found} summary files found\")\n",
    "    \n",
    "        return files\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# from env.api_client import API_Client\n",
    "# from env.data_storage import Data_Storage\n",
    "# from env.data_processor import Data_Processor\n",
    "# from env.config import Config\n",
    "import polars as pl\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, config: Config, api_key: str, verify_ssl: bool = True):\n",
    "        self.config = config\n",
    "        self.api_client = API_Client(config.base_url, api_key, config)\n",
    "        self.processor = Data_Processor()\n",
    "        self.storage = Data_Storage(config)\n",
    "    \n",
    "    def process_shipdata(self, run_timestamp: str):\n",
    "        try:\n",
    "\n",
    "            self.config.logger.info(\"INFO - Sart processing of ship data\")\n",
    "\n",
    "            # Get Shipdata\n",
    "            response, shipdata = self.api_client.get_data(\"fleet\")\n",
    "\n",
    "            # Get & Store IMO Numbers\n",
    "            imo_numbers = self.processor.get_imo_numbers(shipdata)\n",
    "            self.storage.write_file(\n",
    "                imo_numbers,\n",
    "                'imos',\n",
    "                f\"{self.config.metadata_path}\",\n",
    "                'parquet'\n",
    "            )\n",
    "            self.storage.write_table(\n",
    "                data = pl.DataFrame({\"imo\": imo_numbers}), \n",
    "                tablename = 'imos', \n",
    "                path = f\"{self.config.metadata_tables}\", \n",
    "                method = 'overwrite'\n",
    "                )\n",
    "\n",
    "            self.config.logger.info(f\"INFO - Found {len(imo_numbers)} active ships\")\n",
    "\n",
    "            if not shipdata:\n",
    "                self.config.logger.error(f\"ERROR - No ship data received - {response}\")\n",
    "                raise ValueError(f\"No ship data received - {response}\")\n",
    "            \n",
    "            # Store raw ship data\n",
    "            self.storage.write_file(\n",
    "                    shipdata,\n",
    "                    'ShipData',\n",
    "                    f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                    'json'\n",
    "                )\n",
    "            \n",
    "            # Transform and store ship data\n",
    "            ships_df = pl.DataFrame(shipdata)\n",
    "            if ships_df.columns == ['detail']:\n",
    "                self.config.logger.info(\"Request Error: see json file for details\")\n",
    "            \n",
    "            else:\n",
    "                ships_transformed, tables = self.processor.transform_shipdata(ships_df, run_timestamp)\n",
    "            \n",
    "                self.storage.write_file(\n",
    "                        ships_transformed,\n",
    "                        'ShipData',\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "\n",
    "                self.storage.write_table(\n",
    "                    data = ships_transformed,\n",
    "                    tablename ='ShipData',\n",
    "                    path = f\"{self.config.silver_tables}\",\n",
    "                    method = 'overwrite'\n",
    "                )\n",
    "                    \n",
    "                # Process nested tables\n",
    "                for name, table in tables.items():\n",
    "                        self.storage.write_file(\n",
    "                            table,\n",
    "                            f\"ShipData_{name}\",\n",
    "                            f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                        self.storage.write_table(\n",
    "                            table,\n",
    "                            'ShipData_{name}',\n",
    "                            f\"{self.config.silver_tables}\",\n",
    "                            'overwrite'\n",
    "                        )\n",
    "                        \n",
    "            \n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to process ship data: {str(e)}\")\n",
    "\n",
    "\n",
    "    def process_signals_old(self, run_timestamp: str, imo_numbers:List[str], current_signals_df: pl.DataFrame):\n",
    "        try:\n",
    "            \n",
    "            for imo in imo_numbers:\n",
    "\n",
    "                self.config.logger.info(f\"INFO - Processing signals data for {imo}\")\n",
    "\n",
    "                response, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "\n",
    "                if not signals:\n",
    "                    self.config.logger.error(f\"ERROR - No signals data received - {response}\")\n",
    "                    ValueError(f\"No signals data received - {response}\")\n",
    "\n",
    "                # Store raw signals data\n",
    "                self.storage.write_file(\n",
    "                        signals,\n",
    "                        f'Signals_{imo}',\n",
    "                        f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                        'json'\n",
    "                    )\n",
    "                \n",
    "                # Transform and store signals data\n",
    "                signals_df = pl.DataFrame(signals)\n",
    "                if signals_df.columns == ['detail']:\n",
    "                    self.config.logger.info(\"Request Error: see json file for details\")\n",
    "                else:\n",
    "                    signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "                    self.storage.write_file(\n",
    "                            signals_transformed,\n",
    "                            f'Signals_{imo}',\n",
    "                            f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                            'parquet'\n",
    "                        )\n",
    "                    self.storage.write_table(\n",
    "                            data = signals_transformed,\n",
    "                            tablename = 'Signals',\n",
    "                            path = f\"{self.config.silver_tables}\",\n",
    "                            method = 'merge',\n",
    "                            key = 'signal'\n",
    "                        )\n",
    "                    self.config.logger.info(f\"INFO - Signals data processed for {imo}\")\n",
    "                \n",
    "                    # Update Signals Mapping\n",
    "\n",
    "                    new_signal_mapping = signals_transformed.select([\"imo\", \"signal\", \"friendly_name\"]).unique()\n",
    "                    self.storage.write_table(\n",
    "                            data = signals_transformed,\n",
    "                            tablename = 'signal_mapping',\n",
    "                            path = f\"{self.config.metadata_tables}\",\n",
    "                            method = 'merge',\n",
    "                            key = 'signal',\n",
    "                            key2 = 'signal',\n",
    "                            key3 = 'friendly_name'\n",
    "                        )\n",
    "                    self.config.logger.info(f\"INFO - Signal Mapping updated\")\n",
    "                    # if current_signals_df is None:\n",
    "                        # self.storage.write_file(\n",
    "                            # new_signal_mapping,\n",
    "                            # 'signal_mapping',\n",
    "                            # f\"{self.config.metadata_path}\",\n",
    "                            # 'parquet'\n",
    "                        # )\n",
    "                        # self.config.logger.info(f\"INFO - Signal Mapping updated for the first time\")\n",
    "                    # else:        \n",
    "                        # Add new Signals to existing Signal Mapping\n",
    "                        # \n",
    "                        # updated_signals = pl.concat([current_signals_df, new_signal_mapping]).unique(subset=[\"imo\", \"signal\", \"friendly_name\"], keep=\"first\")\n",
    "                        # \n",
    "                        # self.storage.write_file(\n",
    "                                # updated_signals,\n",
    "                                # \"signal_mapping\",\n",
    "                                # f\"{self.config.metadata_path}\",\n",
    "                                # \"parquet\")\n",
    "                        # self.config.logger.info(f\"INFO - Signal Mapping updated\")    \n",
    "                \n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to process signals data: {str(e)}\")\n",
    "\n",
    "    def process_timeseries(self, run_timestamp: str, imo_numbers: List[str], signal_mapping: pl.DataFrame, current_df: pl.DataFrame, run_start) -> pl.DataFrame:\n",
    "        try:\n",
    "            self.config.logger.info(f\"INFO - Starting parallel processing of timeseries data for {len(imo_numbers)} ships\")\n",
    "            result_df = current_df\n",
    "            \n",
    "            def process_per_imo(imo):\n",
    "                try:\n",
    "                    self.config.logger.info(f\"INFO - Processing timeseries data for {imo}\")\n",
    "                    response, timeseries = self.api_client.get_data(f\"fleet/{imo}/timeseries\")\n",
    "                    \n",
    "                    # Store raw timeseries data\n",
    "                    self.storage.write_file(\n",
    "                        timeseries,\n",
    "                        f'Timeseries_{imo}',\n",
    "                        f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                        'json'\n",
    "                    )\n",
    "                    \n",
    "                    # Transform and store timeseries data\n",
    "                    timeseries_df = pl.DataFrame(timeseries)\n",
    "                    \n",
    "                    if timeseries_df.columns == ['detail']:\n",
    "                        self.config.logger.info(f\"INFO - Request Error for {imo}: see json file for details\")\n",
    "                        return pl.DataFrame()\n",
    "                    \n",
    "                    if not timeseries:\n",
    "                        self.config.logger.error(f\"ERROR - No timeseries data received for {imo} - {response}\")\n",
    "                        timeseries_transformed = pl.DataFrame({\n",
    "                            \"imo\": [], \"signals\":[], \"friendly_name\": [], \"signal_timestamp\": [], \"signal_value\": [], \"loaddate\": []\n",
    "                        })\n",
    "                        gaps = pl.DataFrame()\n",
    "                    else:\n",
    "                        timeseries_transformed, gaps = self.processor.transform_timeseries(timeseries_df, imo, run_timestamp)\n",
    "                    \n",
    "                    # Enrich with friendly names\n",
    "                    timeseries_transformed = self.processor.enrich_timeseries_with_friendly_names(\n",
    "                        timeseries_transformed, signal_mapping, imo\n",
    "                    )\n",
    "                    \n",
    "                    self.storage.write_file(\n",
    "                        timeseries_transformed,\n",
    "                        f\"Timeseries_{imo}\",\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                    \n",
    "                    # Process gaps\n",
    "                    gaps_df = self.processor.process_gaps(pl.DataFrame(gaps))\n",
    "                    self.storage.write_file(\n",
    "                        gaps_df,\n",
    "                        f\"Gaps_{imo}\",\n",
    "                        f\"{self.config.transformed_path}/gaps/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                    \n",
    "                    # self.storage.write_table(\n",
    "                    #     data = gaps_df,\n",
    "                    #     tablename = f\"Gaps\",\n",
    "                    #     path = f\"{self.config.silver_tables}\",\n",
    "                    #     method ='merge',\n",
    "                    #     key = 'imo',\n",
    "                    #     key2 = 'signal',\n",
    "                    #     key3 = 'gap_end'\n",
    "                    # )\n",
    "                    \n",
    "                    return timeseries_transformed\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.config.logger.error(f\"ERROR - Failed to process timeseries for {imo}: {str(e)}\")\n",
    "                    return pl.DataFrame()\n",
    "            \n",
    "            # Process parallelized with ThreadPoolExecutor\n",
    "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                results = list(executor.map(process_per_imo, imo_numbers))\n",
    "            # Alle Ergebnisse kombinieren\n",
    "            valid_results = [df for df in results if not df.is_empty()]\n",
    "            if valid_results:\n",
    "                result_df = pl.concat([result_df] + valid_results)\n",
    "                \n",
    "            return result_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to process timeseries data: {str(e)}\")\n",
    "            return current_df\n",
    "        \n",
    "    def process_signals(self, run_timestamp: str, imo_numbers: List[str], current_signals_df: pl.DataFrame):\n",
    "        try:\n",
    "                      \n",
    "            self.config.logger.info(f\"INFO - Starting parallel processing of signal data for {len(imo_numbers)} ships\")\n",
    "            result_df = current_signals_df\n",
    "\n",
    "            def process_per_imo(imo: str) -> pl.DataFrame:\n",
    "                try:\n",
    "                    self.config.logger.info(f\"INFO - Processing signals data for {imo}\")\n",
    "                    \n",
    "                    response, signals = self.api_client.get_data(f\"fleet/{imo}/signals\")\n",
    "                    \n",
    "                    if not signals:\n",
    "                        self.config.logger.error(f\"ERROR - No signals data received for {imo} - {response}\")\n",
    "                        return pl.DataFrame()\n",
    "                        \n",
    "                    # Store raw signals data\n",
    "                    self.storage.write_file(\n",
    "                        signals,\n",
    "                        f'Signals_{imo}',\n",
    "                        f\"{self.config.raw_path}/{run_timestamp}\",\n",
    "                        'json'\n",
    "                    )\n",
    "                    \n",
    "                    # Transform and store signals data\n",
    "                    signals_df = pl.DataFrame(signals)\n",
    "                    if signals_df.columns == ['detail']:\n",
    "                        self.config.logger.info(f\"INFO - Request Error for {imo}: see json file for details\")\n",
    "                        return pl.DataFrame()\n",
    "                        \n",
    "                    signals_transformed = self.processor.transform_signals(signals_df, run_timestamp)\n",
    "                    \n",
    "                    self.storage.write_file(\n",
    "                        signals_transformed,\n",
    "                        f'Signals_{imo}',\n",
    "                        f\"{self.config.transformed_path}/{run_timestamp}\",\n",
    "                        'parquet'\n",
    "                    )\n",
    "                    self.config.logger.info(f\"INFO - Signals data processed for {imo}\")\n",
    "                    return signals_transformed\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.config.logger.error(f\"ERROR - Failed to process signals for {imo}: {str(e)}\")\n",
    "                    return pl.DataFrame()\n",
    "            \n",
    "            # Process parallelized with ThreadPoolExecutor\n",
    "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                results = list(executor.map(process_per_imo, imo_numbers))\n",
    "            \n",
    "            \n",
    "            self.config.logger.info(\"All Signals processed and written\")\n",
    "            # Alle Ergebnisse kombinieren\n",
    "\n",
    "            valid_subset = [df.select([\"imo\", \"signal\", \"friendly_name\"]).unique() for df in results if not df.is_empty()]\n",
    "\n",
    "            if valid_subset:\n",
    "\n",
    "                updated_signals = pl.concat([result_df] + valid_subset).unique(\n",
    "                        subset=[\"imo\", \"signal\", \"friendly_name\"], \n",
    "                        keep=\"first\"\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                updated_signals = result_df\n",
    "\n",
    "                    \n",
    "            # Speichere das aktualisierte Signal-Mapping\n",
    "            if not updated_signals.is_empty():\n",
    "                self.storage.write_file(\n",
    "                    updated_signals,\n",
    "                    \"signal_mapping\",\n",
    "                    f\"{self.config.metadata_path}\",\n",
    "                    \"parquet\"\n",
    "                )\n",
    "\n",
    "                self.storage.write_table(\n",
    "                    data = updated_signals,\n",
    "                    tablename = 'signal_mapping',\n",
    "                    path = f\"{self.config.metadata_tables}\",\n",
    "                    method = 'overwrite'\n",
    "                )\n",
    "\n",
    "                self.config.logger.info(f\"INFO - Signal Mapping updated with {len(updated_signals)} records\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.config.logger.error(f\"ERROR - Failed to process signals: {str(e)}\")\n",
    "            \n",
    "\n",
    "\n",
    "    def run(self, mode: str = \"all\"):\n",
    "\n",
    "        try: \n",
    "\n",
    "            run_start = datetime.now()\n",
    "            run_end = None\n",
    "            run_timestamp = run_start.strftime('%Y/%m/%d/%H/%M')\n",
    "            summary_filename = f\"{run_start.strftime('%Y%m%d')}\"\n",
    "            self.config.logger.info(f\"INFO - Starting pipeline run at {run_start}\")\n",
    "            cutoff_date_str = (run_start - timedelta(days=self.config.history_days)).strftime('%Y/%m/%d')\n",
    "\n",
    "            # Initialize directories\n",
    "            notebookutils.fs.mkdirs(f\"{self.config.raw_path}/{run_timestamp}\")\n",
    "            notebookutils.fs.mkdirs(f\"{self.config.transformed_path}/{run_timestamp}\")\n",
    "            notebookutils.fs.mkdirs(f\"{self.config.transformed_path}/gaps/{run_timestamp}\")\n",
    "            notebookutils.fs.mkdirs(f\"{self.config.transformed_path}/daily_summary\")\n",
    "\n",
    "            empty_ts_schema = {\"imo\": pl.String, \"signal\": pl.String, \"friendly_name\": pl.String, \"signal_timestamp\": pl.String, \"signal_value\": pl.Float64, \"loaddate\": pl.String}\n",
    "            empty_ts_schema_tags = {\"imo\": pl.String, \"signal\": pl.String, \"friendly_name\": pl.String, \"signal_timestamp\": pl.String, \"signal_value\": pl.Float64, \"loaddate\": pl.String, \"tag\": pl.String}\n",
    "            current_signals_df = self.storage.read_file(\"signal_mapping\", f\"{self.config.metadata_path}\", \"parquet\")\n",
    "\n",
    "            # Read daily and historical data\n",
    "            daily_df = self.storage.read_file(summary_filename, f\"{self.config.transformed_path}/daily_summary\", \"parquet\")\n",
    "            hist_df = self.storage.read_file(\"ref_data\",  f\"{self.config.metadata_path}\", \"parquet\")\n",
    "            if hist_df.is_empty(): \n",
    "                hist_df = pl.DataFrame(schema=empty_ts_schema_tags)\n",
    "            else: # Filter, wo die ersten 10 Zeichen (YYYY/MM/DD) größer als cutoff sind\n",
    "                hist_df = hist_df.filter(pl.col(\"loaddate\").str.slice(0, 10) > cutoff_date_str).with_columns(pl.lit(\"hist\").alias(\"tag\"))\n",
    "                \n",
    "\n",
    "            if daily_df.is_empty(): # means it is a new day\n",
    "\n",
    "                daily_df = pl.DataFrame(schema=empty_ts_schema_tags)\n",
    "                \n",
    "                last_day = (run_start - timedelta(days=1)).strftime('%Y%m%d')\n",
    "                last_day_df = self.storage.read_file(last_day, f\"{self.config.transformed_path}/daily_summary\", \"parquet\")\n",
    "                \n",
    "                if last_day_df.is_empty(): \n",
    "                    last_day_df = pl.DataFrame(schema=empty_ts_schema_tags)\n",
    "                else:\n",
    "                    last_day_df = last_day_df.with_columns(pl.lit(\"hist\").alias(\"tag\"))\n",
    "\n",
    "                hist_df = pl.concat([hist_df, last_day_df])\n",
    "\n",
    "                self.storage.write_file(\n",
    "                    hist_df, \n",
    "                    \"ref_data\", \n",
    "                    f\"{self.config.metadata_path}\", \n",
    "                    \"parquet\")\n",
    "                \n",
    "                self.storage.write_table(\n",
    "                    data = hist_df, \n",
    "                    tablename = \"ref_data\", \n",
    "                    path = f\"{self.config.metadata_tables}\", \n",
    "                    method = \"overwrite\")\n",
    "                \n",
    "            else: \n",
    "                daily_df = daily_df.select([\"imo\", \"signal\", \"friendly_name\", \"signal_timestamp\", \"signal_value\", \"loaddate\"]).with_columns(pl.lit(\"today\").alias(\"tag\"))\n",
    "\n",
    "            current_df = pl.DataFrame(schema=empty_ts_schema)\n",
    "\n",
    "            self.config.logger.info(f\"INFO - Loaded {hist_df.shape[0]} historical records and {daily_df.shape[0]} daily records\")\n",
    "            \n",
    "            self.config.logger.info(f\"INFO - Processing data for mode: {mode}\")\n",
    "\n",
    "            if mode in [\"all\", \"fleet\"]:\n",
    "\n",
    "                # Process Shipdata\n",
    "                self.process_shipdata(run_timestamp)\n",
    "\n",
    "                # Get IMO Numbers and turn them into list\n",
    "                imo_numbers = self.storage.read_file(\n",
    "                    'imos',\n",
    "                    f\"{self.config.metadata_path}\",\n",
    "                    'parquet'\n",
    "                )            \n",
    "                imo_numbers = imo_numbers.to_series(0).to_list()\n",
    "\n",
    "\n",
    "                # Process Signals\n",
    "                self.process_signals(run_timestamp, imo_numbers, current_signals_df)\n",
    "\n",
    "\n",
    "\n",
    "            if mode in [\"all\", \"timeseries\"]:\n",
    "\n",
    "                self.config.logger.info(\"Start to Process Timeseries Data\")\n",
    "                # Get IMO Numbers from file in data\n",
    "                imo_numbers = self.storage.read_file(\n",
    "                    'imos',\n",
    "                    f\"{self.config.metadata_path}\",\n",
    "                    'parquet'\n",
    "                )\n",
    "                imo_numbers = imo_numbers.to_series(0).to_list()\n",
    "\n",
    "                # Get Signal-Mapping\n",
    "                signal_mapping = self.storage.read_file(\n",
    "                    'signal_mapping',\n",
    "                    f\"{self.config.metadata_path}\",\n",
    "                    'parquet'\n",
    "                )                \n",
    "                \n",
    "                # Process Timeseries\n",
    "                current_df = self.process_timeseries(run_timestamp, imo_numbers, signal_mapping, current_df, run_start)\n",
    "                current_df = current_df.with_columns(pl.lit(\"new\").alias(\"tag\"))\n",
    "\n",
    "                # Save Delta\n",
    "                summary_df, new_df = self.processor.update_daily_timeseries_summary(hist_df, daily_df, current_df)\n",
    "                self.storage.write_file(\n",
    "                    summary_df.select([\"imo\", \"signal\", \"friendly_name\", \"signal_timestamp\", \"signal_value\", \"loaddate\"]),\n",
    "                    summary_filename,\n",
    "                    f\"{self.config.transformed_path}/daily_summary\",\n",
    "                    \"parquet\"\n",
    "                )\n",
    "\n",
    "                self.storage.write_table(\n",
    "                    data = summary_df.select([\"imo\", \"signal\", \"friendly_name\", \"signal_timestamp\", \"signal_value\", \"loaddate\"]),\n",
    "                    tablename = 'Timeseries_today',\n",
    "                    path = f\"{self.config.silver_tables}\",\n",
    "                    method = \"overwrite\"\n",
    "                )\n",
    "\n",
    "            \n",
    "            run_end = datetime.now()\n",
    "        \n",
    "        except Exception as e:\n",
    "            run_end = datetime.now()\n",
    "            self.config.logger.error(f\"ERROR - Pipeline run failed at {run_end}: {str(e)}\") \n",
    "            raise\n",
    "\n",
    "        \n",
    "\n",
    "        return run_start, run_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "#from env.config import Config\n",
    "#from env.pipeline import Pipeline\n",
    "\n",
    "\n",
    "api_key = \"YWExNTNiMzEtNmJlZi00ODAwLWJkMzgtYzk4NTdkYTEyZDk0OmNUaGo5T05OMks3YmVpTk1TVmI4ZDVTenR0bm9wY3llMzlGbGZDUzY=\"\n",
    "notebookutils.fs.mount(\"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/9e6baa74-8d1f-4201-9a08-b6fd53c72180\", \"/lakehouse/lh_metadata\")\n",
    "notebookutils.fs.mount(\"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/30df6f99-a299-4ca8-9d9d-ddc2a10b1943\",\"/lakehouse/lh_silver\")\n",
    "notebookutils.fs.mount(\"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/92bb8409-5f37-48a1-af16-827475a5000b\",\"/lakehouse/lh_bronze\")\n",
    "notebookutils.fs.getProperties\n",
    "file_info = notebookutils.fs.ls(\"/synfs/notebook\")\n",
    "base_path = file_info [0].path\n",
    "\n",
    "metadata_base = \"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/9e6baa74-8d1f-4201-9a08-b6fd53c72180\"\n",
    "bronze_base = \"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/92bb8409-5f37-48a1-af16-827475a5000b\"\n",
    "silver_base = \"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/30df6f99-a299-4ca8-9d9d-ddc2a10b1943\"\n",
    "\n",
    "def setup_logging(log_podtfix):\n",
    "    \"\"\"Konfiguriert das Logging\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO\n",
    "        ,format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        ,handlers=[\n",
    "            logging.FileHandler(f\"{base_path}/lakehouse/lh_metadata/Files/hoppe/logs/runs_{log_postfix}.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716bf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "log_postfix = f\"{datetime.now().strftime('%Y%m%d')}\"\n",
    "logger = setup_logging(log_postfix)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "config = Config(\n",
    "    base_url=\"https://api.hoppe-sts.com/\",\n",
    "    raw_path= f'{base_path}/lakehouse/lh_bronze/Files/hoppe',\n",
    "    transformed_path= f'{base_path}/lakehouse/lh_silver/Files/hoppe',\n",
    "    metadata_path = f'{base_path}/lakehouse/lh_metadata/Files/hoppe',\n",
    "    silver_tables = f'{silver_base}/Tables/hoppe',\n",
    "    metadata_tables = f'{metadata_base}/Tables/hoppe',\n",
    "    batch_size = int(\"1000\"),\n",
    "    max_workers=int(\"4\"),\n",
    "    days_to_keep=int(\"90\"),\n",
    "    history_days=int(\"5\"),\n",
    "    logger=logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00225684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run pipeline\n",
    "try:\n",
    "    mode = \"all\" # \"all\" or \"timeseries\" or \"fleet\"\n",
    "    pipeline = Pipeline(config, api_key)\n",
    "    run_start, run_end = pipeline.run(mode)  \n",
    "    logger.info(f\"INFO - Pipeline run completed at {run_end}: total runtime {run_end - run_start}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"ERROR - Pipeline run failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8462ab",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953831dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imo in imo_numbers:\n",
    "    print(imo)\n",
    "    full_path = f\"{config.transformed_path}/2025/04/03/14/50/Signals_{imo}.parquet\"\n",
    "    if notebookutils.fs.exists(full_path):\n",
    "        df = pl.read_parquet(f\"{full_path}\")\n",
    "        #print(df.schema)\n",
    "        print(df.shape)\n",
    "    else:\n",
    "        print(\"No Signals found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d23c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_base = \"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/30df6f99-a299-4ca8-9d9d-ddc2a10b1943\"\n",
    "path = f'{silver_base}/Tables/hoppe/Timeseries'\n",
    "data = pl.read_delta(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db6b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = data.select([\"imo\", \"signal\", \"signal_timestamp\", \"signal_value\", \"loaddate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_base = \"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/9e6baa74-8d1f-4201-9a08-b6fd53c72180\"\n",
    "path = f'{metadata_base}/Tables/hoppe/signal_mapping'\n",
    "signal_mapping = pl.read_delta(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_proc = ts_df.join(\n",
    "        signal_mapping,\n",
    "        on=\"signal\",\n",
    "        how=\"left\"\n",
    "        ).select([\"imo\", \"signal\", \"friendly_name\", \"signal_timestamp\", \"signal_value\", \"loaddate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_proc = ts_df_proc.with_columns(pl.col().cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_base = \"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/30df6f99-a299-4ca8-9d9d-ddc2a10b1943\"\n",
    "table_path = f'{silver_base}/Tables/hoppe/Timeseries'\n",
    "\n",
    "ts_df_proc.write_delta(table_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_ref = ts_df_proc.filter(pl.col().)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf73de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_base = \"abfss://c5691838-bce4-4eff-9d3e-8f3f9ed6d2d3@onelake.dfs.fabric.microsoft.com/30df6f99-a299-4ca8-9d9d-ddc2a10b1943\"\n",
    "table_path = f'{silver_base}/Tables/hoppe/Timeseries_today'\n",
    "\n",
    "ts_df_proc.write_delta(table_path, mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
